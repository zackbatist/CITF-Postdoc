[
  {
    "objectID": "research-protocol.html",
    "href": "research-protocol.html",
    "title": "Research Protocol",
    "section": "",
    "text": "This study investigates the social, technical, administrative and epistemic factors that scaffold data-sharing initiatives in epidemiological research. It takes to heart the notions that data are media that facilitate communication across different research contexts, that data are created with specific intent, and that data are bounded by the social, practical and material circumstances of their creation. In light of these facts, the study approaches data-sharing as a means of reconciling the varied circumstances of datasets’ creation — both among themselves, and in relation to contexts of reuse. It therefore frames data-sharing as efforts to foster a series of collaborative ties beyond a project’s original indended scope.\nData harmonization is one means of data-sharing that draws multiple studies’ recorded observations into a unified formal schema, whose structure is driven by specific objectives and more general underlying suppositions and values. Although these schemas are arrived at through discussion, compromise and consensus-building, with an eye toward practical outcomes afforded by alignment of complementary records, these socially-mediated interactions are generally under-recognized as important factors contributing to data-sharing initiatives’ success relative to their potential impact. The goal of this study is to survey how various factors are prioritized in harmonization activities, the rationales behind these decisions, and the relative efficacy of different approaches to data-sharing.\nThe project’s goal is to survey what factors are being priorized within various data harmonization initiatives, the rationales behind these decisions, and the relative efficacy of these different approaches. More specifically, the project seeks to adress the following research questions:\n\nWhat are the objectives of data-sharing initiatives, how were they established, and what progress has been made to achieve them?\nWhat strategies do data-sharing initiatives employ to ensure they are able to meet their objectives, and how effective are they?\nWhat values underlie these strategies, and can they be linked with effective outcomes?\n\nTo be clear, the intent is not to pit various approaches against each other, but rather to ascertain what actions specific strategies entail, the circumstances in which each is adopted, the value that they bring, and the trade-offs involved. In other words, the goal is to reveal the diverse ways in which data-sharing occurs, and how different approaches impact the outcomes in different ways.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#overview",
    "href": "research-protocol.html#overview",
    "title": "Research Protocol",
    "section": "",
    "text": "This study investigates the social, technical, administrative and epistemic factors that scaffold data-sharing initiatives in epidemiological research. It takes to heart the notions that data are media that facilitate communication across different research contexts, that data are created with specific intent, and that data are bounded by the social, practical and material circumstances of their creation. In light of these facts, the study approaches data-sharing as a means of reconciling the varied circumstances of datasets’ creation — both among themselves, and in relation to contexts of reuse. It therefore frames data-sharing as efforts to foster a series of collaborative ties beyond a project’s original indended scope.\nData harmonization is one means of data-sharing that draws multiple studies’ recorded observations into a unified formal schema, whose structure is driven by specific objectives and more general underlying suppositions and values. Although these schemas are arrived at through discussion, compromise and consensus-building, with an eye toward practical outcomes afforded by alignment of complementary records, these socially-mediated interactions are generally under-recognized as important factors contributing to data-sharing initiatives’ success relative to their potential impact. The goal of this study is to survey how various factors are prioritized in harmonization activities, the rationales behind these decisions, and the relative efficacy of different approaches to data-sharing.\nThe project’s goal is to survey what factors are being priorized within various data harmonization initiatives, the rationales behind these decisions, and the relative efficacy of these different approaches. More specifically, the project seeks to adress the following research questions:\n\nWhat are the objectives of data-sharing initiatives, how were they established, and what progress has been made to achieve them?\nWhat strategies do data-sharing initiatives employ to ensure they are able to meet their objectives, and how effective are they?\nWhat values underlie these strategies, and can they be linked with effective outcomes?\n\nTo be clear, the intent is not to pit various approaches against each other, but rather to ascertain what actions specific strategies entail, the circumstances in which each is adopted, the value that they bring, and the trade-offs involved. In other words, the goal is to reveal the diverse ways in which data-sharing occurs, and how different approaches impact the outcomes in different ways.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#approach",
    "href": "research-protocol.html#approach",
    "title": "Research Protocol",
    "section": "Approach",
    "text": "Approach\nThis study is informed by a set of theoretical and methodological frameworks formed within a more interdisciplinary “science studies” tradition, which contribute to a more sociological outlook on science as cultural practice (cf. Pickering 1992). In practical terms, the study documents the social and collaborative experiences involved in various research practices, which ultimately bind the many ways in which scientists do science.\nThe study will specifically focus on how people contribute to and extract from information commons, which comprise both formal documents and mutually-held and information-laden situated experiences. This involves examining the ways in which participation in disciplinary or even more specialized communities of practice fosters mutual understanding about the potential and limitations pertaining to other people’s data; and how this communally-held knowledge is accessed and re-produced. This approach aligns with the situated cognition methodological framework for examining the improvised, contingent and embodied experiences of human activity, including science (cf. Suchman 2007; Knorr Cetina 2001).\nThe situated cognition framework prioritizes subjects’ outlooks, which are contextualized by their prior experiences, and enables scholars to trace how people make sense of their environments and work with the physical and conceptual tools available to them to resolve immediate challenges. Situated cognition therefore lends itself to investigating rather fluid, open-ended and affect-oriented actions, and is geared towards understanding how actors draw from their prior experiences to navigate unique situations.1\n1 I expand on this in my extended note on efforts to frame the plurality of research experiences as a continuum of practice.Situated cognition is especially salient in explorations of how people who are learning new skills learn how to work in new and possibly unfamiliar ways, and in this sense is closely related to Lave and Wenger’s (1991) theory of situated learning (or ‘communities of practice’ approach), which focuses on how individuals acquire professional skills in relation to their social environments. In such situations, situated cognition enables observers to examine how people align their perspectives as work progresses, and to understand better how people’s general outlooks may have changed under the guidance of more experienced mentors. In other words, situated cognition enables researchers of scientific practices to account for discursive aspects of work, including perceived relationships, distinctions or intersections between practices that professional or research communities deem acceptable and unacceptable, and the cultural or community-driven aspects of decisions that underlie particular actions.\nIn taking on this theoretical framework, the study frames epidemiology as a collective endeavour to derive a coherent understanding of population-level health trends, which involves the use of already established knowledge in the validation of newly formed ideas, and which relies on systems designed to carry information obtained with different chains of inference. These systems have both technical and social elements. The technical elements are the means through which information becomes encoded onto information objects so that they may form the basis for further inference. The social elements constitute a series of norms or expectations that facilitate the delegation of roles and responsibilities among agents who contribute their time, effort and accumulated knowledge to communal goals.\nAs such, in constructing the arguments of this study and in carrying out the interviews that grounds it, the study will rely upon both realist and constructivist viewpoints. In one sense, the study relies on documenting how people actually act, including the longer-term and collaborative implications that their actions may have on other work occurring throughout the continuum of practice. To accomplish this, the study identifies research activities from the perspective of an outside observer. The study also ascribes meanings to things (such as physical or conceptual tools, or objects that captivate subjects’ interests) in ways that conform to the analyst’s own perspective as an investigator of scientific research practices. On the other hand, a constructionist perspective enables the author to consider how individual agents make components of information systems suit their needs to facilitate communication or interoperability among actors who hold different situated perspectives. By listening to participants’ views about the systems with which they engage, including explanations as to why they act in the ways that they do, I am able to trace the assumptions and taken-for-granted behaviours that frame their perspectives. Moreover, these insights are useful for developing a better understanding of how participants identify with particular disciplinary communities and their perception of their roles within broader collective efforts.\nUltimately, this study is about the social order of scientific research, i.e. the frameworks, mindsets or sets of values that humans adopt to carry out their work in specific ways. Human beings rely upon physical and conceptual apparatus to do this work but, in order to understand how they do science in ways that conform to the epistemic mandates of the scientific enterprise, it is necessary to prioritize attention to human intention, drivers and pressures. The study emphasizes the agency of human drivers — as opposed to tools and procedures — since humans are the ones who (a) identify problems that need to be resolved; (b) imagine, project or predict potential outcomes of various kinds of actions that they may select to resolve the challenges; and (c) learn from prior experiences and change their behaviours accordingly.2 By highlighting how pragmatic actions are conducted in relation to broader social and discursive trends and tendencies, the study considers scholarly practices in terms of potential, certainty and desire from the perspectives of practitioners themselves.\n2 Human and non-human agents are considered on equal footing under the Actor-Network Theory (ANT) framework, which has become very popular since its origins in the late 1980s, but which may not be suitable for this approach. See my extended note on this for further details.To this end, the study follows an abductive qualitative data analysis (QDA) methodology to construct theories founded upon empirical evidence, which relates to, but is distinct from, grounded theory. Grounded theory consists of a series of systematic yet flexible guideline for deriving theory from data through continuous and reiterative engagement with evidence (Charmaz 2014: 1). The approach taken for this study draws from what Charmaz (2014: 14-15) calls the “constellation of methods” associated with grounded theory that are helpful for making sense of qualitative data. However, it differs from grounded theory as it is traditionally conceived in that I came to the project with well defined theoretical goals (as described above) and did not make a concerted effort to allow the theory to emerge through the analytical process. Proponents of a more open-ended or improvised approach, as grounded theory was originally applied, argue that researchers should be free to generate theories in accordance with their own creative insights and their intimate engagements with the evidence. We can evaluate the quality of such work in terms of the dialogical commitments between researchers and their subjects, and between researchers and those who read their work (Glaser and Strauss 1967: 230-233). Others view grounded theory more as a means of clarifying and articulating phenomena that lie below the surface of observable social experiences (Strauss and Corbin 1990; Kelle 2005). Proponents of this approach are very concerned with ensuring that concepts, themes and theories are truly represented in and limited by the data, and therefore prioritize adherence to systematic validation criteria to ensure the soundness of their claims.\nAnother view, known as constructivist grounded theory, most resembles the approach taken for this study. It recognizes that it is impossible to initiate a project without already holding ideas regarding the phenomena of interest, and that the ways that one ascribes meanings to the data represent already established mindsets or conceptual frameworks (Charmaz 2014). It encourages reflection on the researcher’s standpoint as they pursue an abductive approach rooted in their own preconceptions (Mills, Bonner, and Francis 2006).\nAll of these approaches rely on a core set of methods of coding and memoing. Coding, which involves defining what data are about in terms that are relevant to the theoretical frameworks that inform the research, entails rendering instances within a text as interpreted abstractions called codes (Charmaz 2014: 43). These methods, which are described in more detail below, are particularly useful for examining the broad assemblage of evidence comprising various kinds of media and spanning multiple case studies. The abstraction of specific instances as conceptual codes enables comparisons across documents that would otherwise prove difficult to compare, due either to the analyst’s own preconceptions (drawn from internalized narratives or biases) that might have framed their attitudes, to disproportionate volumes of evidence that might obscure parallels between case studies, or to difficulties experienced when examining different kinds of documents that call for different lenses or perspectives.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#data",
    "href": "research-protocol.html#data",
    "title": "Research Protocol",
    "section": "Data",
    "text": "Data\nThe study draws from semi-structured interviews with 12-15 individuals from 4-5 cases who lead, support or participate in epidemiological data-sharing initiatives. These individuals include profesional researchers, research trainees and administrative and technical support staff affilited with epidemiological projects that coordinate, support or participate as a member of data-sharing initiatives.\nOnly data-sharing initiatives that partner with the Maelstrom Research project, which facilitates collaborative epidemiological research through rigorous data documentation, harmonization, integration, and co-analysis, will be considered to serve as cases for this study. Maelstrom is a well-established entity in this field and has established a broad network of partner projects to select cases from. Maelstrom serves as a “fixed point” which ensures that the researchers and study participants share a common frame of reference. Maelstrom established generalizable nomenclature and toolsets across all its partner projects, which reduces the overhead of matching different terms and and practices across cases. This is especially valuable in the context of interview-based research, wherein it is crucial to remain focused on obtaining information that is relevant to the themes the project seeks to address during the limited time alloted. Additionally, Maelstrom’s principal investigators and partners have written extensively about the values and practical challenges concerning data-sharing in epidemiology, which provides a rich foundation upon which the analysis may be based (cf. Demir and Murtagh 2013; Fortier et al. 2017; Bergeron et al. 2018; Fortier et al. 2023).3 See the case selection strategy document for further information on how cases are decided upon.\n3 See my extended notes on these works and on additional related studies: zackbatist.info/CITF-Postdoc/notes/maelstrom-readings.The project seeks to interview 3-4 people from each case, including individuals who work in specific roles, such the leaders of data-sharing consortia, support staff, and leaders of contributing projects. Interviews are oriented by the study’s goal to document processes of reconciling different stakeholders’ interests as they converge in the formation of a common data resource. Specifically, interviews will focus on motivations for their initiatives, the challenges they experience, how they envision success and failure, their perceptions of their own roles and the roles of other team members and stakeholders, the values that inform their decisions, how the technological apparatus they set up enables them to realize their goals and values, and ways in which they believe data-sharing could be improved. See the interview protocol for further details on the questions I will ask and how participants’ responses will contribute to the project’s findings, as well as logistical considerations.\nThe number of cases reflects the capacity to draw adequate comparison across unique circumstances while also complementing the meaningful number of individuals who may serve as interview participants. Breadth of perspective is of greater concern than sample size in qualitative research following the constructivist grounded theory methodological framework. Since this study is rooted in case study methods, and the goal is to articulate the series of inter-woven factors that impact how epidemiological researchers coordinate and participate in data-sharing initiatives while explicitly accounting for and drawing from the unique and situational contexts that frame each case, the goal is not to define causal relationships or to derive findings that may be generalized across the whole field of epidemiology. As such, statistical representativeness is not an objective of this research.\nAt the same time, the number of individuals also reflects the practical constraints that this work affords, namely the time-consuming nature of transcribing interviews and conducting qualitative data analysis. My experience leading projects of similar scale will enable me to collect, process and analyze such a comprehensive corpus in the relatively short period in which funding has been allotted. The number of participants therefore represents a careful balance between a meaningful sample size and the amount of work required to collect, process and analyze the data within the project’s one-year timeframe.\nInterviews will be transcribed, and transriptions will be edited to optimize them for use in qualitative data analysis software. Secure and locally-hosted automated speech recognition software may be used to create preliminary transcripts, which will then be manually edited. All data will be collected and curated in full compliance with the ethics protocol and in accordance with the data management plan.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#methods",
    "href": "research-protocol.html#methods",
    "title": "Research Protocol",
    "section": "Methods",
    "text": "Methods\nThe study will implement qualitative data analysis (QDA) methods to highlight collaborative aspects data-sharing in epidemiology, as elicited in the corpus of transcribed interviews. QDA involves encoding the primary sources of evidence in ways that enable a researcher to draw cohesive theoretical accounts or explanations. This is done by tagging segments of a document (such as an interview transcript) using codes, and by embedding open-ended interpretive memos directlly alongside the data. Through these methods, a researcher is able to articulate theories based on empirical evidence that reflect the informants’ diverse experiences.\nCoding — which involves defining what specific elicitations are about in terms that are relevant to the theoretical frameworks that inform the research — entails rendering instances within a text as interpreted abstractions called codes (Charmaz 2014, 43). Codes can exist at various levels of abstraction. For instance, an analyst may apply descriptive codes to characterize literal facets of an instance within a text, and theoretical codes to represent more interpretive concepts that correspond with aspects of particular theoretical frameworks. This project will primarly implement an “open” coding protocol, which entails creating codes on the fly when prompted by encounters with demonstrative instances in the text. As new codes are generated in this manner, they are situated within a code system that affords greater taxonomic structure to encoded observations, thereby facilitating more effective queries. Coding in this manner involves synthesis of concepts that speak to the analyst’s understanding of the phenomena of interest, while forcing the analyst to remain receptive to limits imposed by what is actually contained in the corpus. In other words, coding involves applying a precise language to segments of transcribed interviews that serve to bridge the gap between what participants said and the theoretical frameworks that the analyst applies to explore them as epistemic activities, interfaces and values (cf. Charmaz 2014; Saldaña 2011, 95–98).\nMemoing entails more open-ended exploration and reflection upon latent ideas in order to crystallize them into new avenues to pursue (Charmaz 2014, 72). Constructing memos is a relatively flexible way of engaging with data and serves as fertile ground for honing new ideas. Memoing is especially crucial while articulating sensitizing concepts, which Charmaz (2003, 259) refers to as the “points of departure from which to study the data”. Memoing allows the researcher to take initial notions that lack specification of well-defined attributes, and gradually refine them into more cohesive, definitive concepts (Blumer 1954, 7; Bowen 2006). Exploring the main features, relationships or arrangements that underlie a superficial view of a sensitizing concept through memoing helps the analyst to identify what kinds of things they need to locate in the data in order to gain a full understanding of the phenomena of interest. Memoing is also very important in the process of drawing out more coherent meaning from coded data (cf. Charmaz 2014, 181, 290–93). By creating memos pertaining to the intersections of various codes and drawing comparisons across similarly coded instances, an analyst is able to form more robust and generalizable arguments about the phenomena of interest and relate them to alternative perspectives expressed by others.\nThroughout the analysis, I will follow the approach that Nicolini (2009) and Maryl et al. (2020, para. 30) advocate, who suggest “zooming in to a granular study of particular research activities and operations and zooming out to considering broader sociotechnical and cultural factors.” This involves “magnifying or blowing up the details of practice, switching theoretical lenses, and selective re-positioning so that certain aspects are fore-grounded and others are temporarily sent to the background” (Nicolini 2009, 1412). This approach is useful in the context of this study because the research projects that represent the cases start from different positions but share common practices and tendencies that vary according to those contextual circumstances. It is therefore possible to tactfully switch between those lenses to understand the interplay between circumstances and practical implementations, which vary across cases, which have their own histories, memberships, sets of tools, methods, and social or political circumstances.ships, sets of tools, methods, and social or political circumstances.\nThis work will be performed using computer assisted qualitative data analysis software that enables analysts to retrieve segments of interview transcripts and identify patterned distributions of codes from across the entire corpus. Querying the dataset in this way enables the analyst to articulate elaborated accounts of specific kinds of activities, decisions, values and sentiments that cut across various informants’ perspectives.\nStatistical methods will play a limited role in this study. Basic summary statistics (e.g. cross tabulation) will be used to represent the distribution of codings across individual interviews or ranges of interviews, which will help to identify trends and associations as they pertain to their limited scopes. This will be used to support theory-building but will not be used to infer generalizable causal relationships.\nSee the QDA protocol for further details on the code system and memoing guidelines, as well as specific QDA procedures, and my methodology notes that situate this’s project’s methods in relation to alternative approaches.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#outcomes",
    "href": "research-protocol.html#outcomes",
    "title": "Research Protocol",
    "section": "Outcomes",
    "text": "Outcomes\nThis project will produce insights regarding the practical benefits and challenges involved in epidemiological data-sharing. It will identify how relevant stakeholders actually engage with the systems that scaffold data-sharing initiatives, which may differ from modelled behaviours specified in aspirational plans and procedural documents. In effect, by articulating how these systems succeed or fail to account for their users practical needs and disciplinary values, this study will provide constructive feedback that will inform their further development.\nThe study will produce three peer-reviewed articles. One of these will be published in a journal concerned with scientific practice or research data management (e.g. Computer Supported Cooperative Work; Scientific Data); Another will be published in a journal dedicated to advancing research practices in epidemiology (e.g. Epidemiologic Methods) A third paper will comprise a more practical set of guidelines deriving from the findings, as either an editorial or as a “10 simple rules” style article. I will also present this work at conferences and workshops, as opportunities arise.\nMoreover, this work is intended to be constructive, and it is therefore necessary to ensure that the findings may be put to practical use so as to enhance and improve data-sharing initiatives. I will therefore draft policy briefs and reach out to leading stakeholders at the helm of major data-sharing infrastuctures and policy frameworks (such as the ongoing revisions to federal open science policies) so that the findings may directly inform efforts to improve these systems.\nAdditionally, I will promote this work publicly. This will entail publishing an article in The Conversation, a website with a broad public following and which specializes in showcasing specialized research for the general public. I may also share the findings on podcasts about open science and science policy with broad interdisciplinary appeal. Moreover, owing to my broad pan-disciplinary background, I am plugged into a diverse network of scholars, and my work will reach a very broad audience through active my engagement on social media and posting regular updates on my professional blog.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#ethics",
    "href": "research-protocol.html#ethics",
    "title": "Research Protocol",
    "section": "Ethics",
    "text": "Ethics\nThe study will be conducted according to ethical principles stated in the Declaration of Helsinki (2013). Ethics approval will be obtained before initiating the study. Consent forms will take into consideration the well-being, free-will and respect of the participants, including respect of privacy. The practices undertaken to ensure adherence to these principles are described in the ethics protocol.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#timelime",
    "href": "research-protocol.html#timelime",
    "title": "Research Protocol",
    "section": "Timelime",
    "text": "Timelime\n\n\n\n\n\n\n\nDate\nMilestone\n\n\n\n\n2025/01/14\nFinalize ethics protocol\n\n\n2025/01/21\nFinalize data management plan\n\n\n2025/01/31\nFinalize case selection and invite members to participate\n\n\n2025/02/14\nFinalize interview protocol\n\n\n2025/02/14 — 2025/03/31\nConduct interviews\n\n\n2025/04/01 — 2025/04/30\nPrepare interview transcripts for analysis\n\n\n2025/05/01 — 2025/05/31\nPrepare code system and develop sensitizing concepts\n\n\n2025/06/01 — 2025/09/30\nQualitative coding and memoing\n\n\n2025/10/01 — 2025/11/30\nDraft manuscripts and submit for publication\n\n\n2025/11/01 — 2025/12/31\nWrite accessible and constructive reports",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "posts/weeknotes-2025-W05.html",
    "href": "posts/weeknotes-2025-W05.html",
    "title": "Week notes (2025-W05)",
    "section": "",
    "text": "The first part of my week largely involved reading and taking notes on methodological texts. I focused on case study design and qualitative coding techniques. I plan to continue my methodology readings on coding techniques, memoing, interview methods and systematic note-taking, as well filling in gaps in my understanding of grounded theory and related debates. These readings are especially useful in this planning stage, but also serve to fill time while I wait for my IRB approval.\nOn that note, I finally submitted my ethics application on Thursday. I expect an expedited review based on the low-risk nature of the work. I posted the materials I submitted on the ethics protocol page.\nI had my biweekly meeting with David, and he was very encouraging.\nI met with Isabel Fortier again yesterday and we came up a list of six projects that may serve as potential cases. We will discuss them in greater depth next week.\nI finally sent some feedback on qc.\nNext week I also need to fulfill a few commitments not as related to the postdoc: I need to work on a peer-review I had committed to, continue assemmbling constructive feedback for qc, and continue going through the DOAJ for the diamond.open-archaeo initiative."
  },
  {
    "objectID": "posts/weeknotes-2025-W03.html",
    "href": "posts/weeknotes-2025-W03.html",
    "title": "Week notes (2025-W03)",
    "section": "",
    "text": "I’m trying out a new way to track and communicate my progress on this project. Every week I’ll write a post to track the work I’ve been doing and reflect on my activities. I’ll try to maintain this document throughout the week, tidy it up and post it here on Friday afternoons. However the specific process will probably vary as it grows into the rest of my workflow.\nI’m purposefully trying to not tie this into the personal knowledge management trend. It’s for me and my own purposes, and I don’t want to get bogged down with the unabashed managerial phoniness that belies the PKM phenomenon.\nAnyway, I didn’t take notes on my work this past week, but here’s an overview of what I’ve done from memory:\nContinued to take notes on readings produced by the Maelstrom Project and its partners.\nContinued to investigate and maintain notes on potential cases.\nSet up a placeholder document for notes on methodological concerns.\nMet with David for our bi-weekly check-in (meeting notes are private, at least for now). I was also given access to the lab’s private git server but haven’t really had much of a chance to explore what it’s being used for or devise my own plans to make use of it.\nWorked extensively on the ethics protocol. David and I went back and forth deciding on whether this was necessary, given how the project is based on his grant which already has IRB approval. But it’s better to play it safe than sorry, especially when it’s necessary to obtain informed consent. So to this end, I revised the research protocol and responses to the ethics form, and I also drafted an informed consent document. I’ll share all these things once the whole package is put together (probably in a week or two), but my responses to the ethics form already appears on the ethics protocol page.\nI simplified the way private documents are handled in the quarto project and git respository. May still need to so some fiddling, especially for draft blog posts and notes.\nI started drafting a blog post about the potential use of AI/LLMs in my research. Stay tuned.\nOn a related note, I watched this recent video about the use of LLMs in qualitative data analysis, which did not prompt me to draft the post but which is well-timed, nevertheless.\nI worked a bit more on the data management plan, which prompted me to think more about which QDA software I’ll use. I started filling in a university form to use cloud services provided by MaxQDA, but stumbled upon qualitative-coding (abbreviated as qc), an open source CLI-based QDA system. It represents a very innovative approach to QDA rooted in computational thinking and plain text social science, while also remaining true to the core tenets and purpose of QDA, which make it unique and difficult to design software for. If this is the sort of thing that appeals to you, I highly recommend you read the docs.\nI had a bit of trouble installing it and getting it running, but I met remotely with Chris Proctor, who develops the tool through his work at the Computational Literacies Lab, based in the Department of Learning and Instruction at University at Buffalo (SUNY). He helped me resolve some issues, gave me a guided tour of the system and we just talked about the overall state of qualitative data analysis and its tooling. I don’t really have the capacity right now to post everything he showed me but I will definitely be posting about my experiences tinkering around with qc in the coming weeks.\nSimilarly, I asked on Mastodon about whether there are any tools that might support automatic generation of transcripts that include support for specialized notation. A few linguists and conversation analysis scholars responded with recommendations to use GailBot, and with discussion about the tool’s capabilities and limitations. I requested access to the software but haven’t heard back from the dev team yet. I also created a thread on the whisper github repo, which I now realize it a bit of a naive place to put it, and it hasn’t yet gotten any responses.\nI attended a talk from the epidemiology seminar series, which went wayyyy over my head.\nDid my usual amount of engagement on masotodon, I suppose. And I continued to make new friends in the department too :)"
  },
  {
    "objectID": "posts/2024-12-11-technical-specs.html",
    "href": "posts/2024-12-11-technical-specs.html",
    "title": "Technical specs for this website",
    "section": "",
    "text": "I’m using this website as a way to help organize and share key documents and resources. The research protocols are in flux at this stage in the project’s development, and this will make it easier to distribute up-to-date drafts with partners, while simultaneously enhancing transparency.\nThis post outlines the technical specifications for this website and outlines a roadmap for its further development. It will therefore be continually updated as the site evolves."
  },
  {
    "objectID": "posts/2024-12-11-technical-specs.html#fundamentals",
    "href": "posts/2024-12-11-technical-specs.html#fundamentals",
    "title": "Technical specs for this website",
    "section": "Fundamentals",
    "text": "Fundamentals\nThis website is based on Quarto, a platform for writing and publishing scientific and technical writing. I had used quarto before but without fully understanding it, and now I am starting to see its elegance.\nI had started off using Hugo, but there were too many limitations that Quarto was able to accomodate. You can find an older version of this post reflecting that setup here: #2346852.\nThe site is hosted on GitHub Pages. The repo is located at https://github.com/zackbatist/CITF-Postdoc."
  },
  {
    "objectID": "posts/2024-12-11-technical-specs.html#generating-pdfs",
    "href": "posts/2024-12-11-technical-specs.html#generating-pdfs",
    "title": "Technical specs for this website",
    "section": "Generating PDFs",
    "text": "Generating PDFs\nAs an avid user, one thing I really like about Quarto is the ability to generate PDFs alongside html versions served over the web. I started tinkering with includes but I need to review how Quarto passes info from YAML frontmatter. It is not at all straightforward and I will need to experiment a bit more with this to get the hang of it."
  },
  {
    "objectID": "posts/2024-12-11-technical-specs.html#archiving-and-version-control",
    "href": "posts/2024-12-11-technical-specs.html#archiving-and-version-control",
    "title": "Technical specs for this website",
    "section": "Archiving and Version Control",
    "text": "Archiving and Version Control\nEvery change is tracked using git. I would also like to archive each research protocol in Zenodo once they reach a point of stability. This would ensure that they ca be aassigned DOIs and detailed metadata, which will make them easier to reference.\nHowever, I do not want to rely on Zenodo’s GitHub integration for two reasons: (1) I want this to be as platform-agnostic as possible, and (2) that system relies on GitHub’s release system which operates on the level of the whole repository rather than specific files.\nI might be able to write a custom CI workflow to archive specific files to Zenodo using their API. But, I want to be able to toggle this option, rather than have it occur for every single detected change. Maybe I can accomplish this by pushing the changes that I want to archive to a dedicated branch that the CI workflow is configured to operate on. Or it might be easier to simply do this manually, since I’m not sure I will be using it that often anyway."
  },
  {
    "objectID": "posts/2024-12-09-first-team-meeting.html",
    "href": "posts/2024-12-09-first-team-meeting.html",
    "title": "Reflection on first team meeting",
    "section": "",
    "text": "Last week (2024/12/04) I finally met with David Buckeridge, Tanya Murphy and Aklil Noza in person. The meeting was meant to convey my vision for the project to the whole team, to align perspectives, and to articulate how this will actually work in practice.\nThe gist is that I will be investigating the role of social and cultural factors in data-sharing initiatives such as CITF and other Maelstrom-affiliated projects, and how these relate to, overlap with, or conflict with technical and institutional/administrative factors. To be clear, these are all very important aspects of data-sharing, but we generally recognized that the social and cultural aspects are under-explored relative to their impact.\nWe briefly talked about how we will go about selecting cases, and I emphasized the importance of strategic case selection. This also involves carefully articulating the project’s goals so that the cases will complement them. We agreed that the dataset will likely comprise between 12-15 interviews of around 60-90 minutes in length with representatives from 4-5 cases (one of them being CITF), in addition to representatives of the Maelstrom team. Maelstrom will serve as a “fixed point” that limits the scope of the cases’ breadth and ensures that participants have a common frame of reference. It also potentially allows me to “offload” or “consolidate” reference to technical and administrative aspects of data-sharing through targeted interviews with Maelstrom personnel, instead of dealing with those things with the representatives for each case.\nWe discussed timelines and overlap with Aklil’s work, which will be more concerned with focus groups with CITF databank users. There is definitely overlap with the emphasis of my own work and we will coordinate data collection to enhance the potential for analytical alignment.\nAfter the meeting I chatted with Tanya and Aklil who helped familiarize me with the bigger-picture theoretical discourse and tensions in epidemiology. Much of it seemed familiar since these concerns are common across disciplines, but I still need to read more to concretize my understanding. Tanya recommended I read the “Baby Rothman” which is a condensed version of a very long-lived textbook in this field, among a few other papers she sent me.\nOverall, this meeting got me really excited about this project :)"
  },
  {
    "objectID": "notes/potential-cases.html",
    "href": "notes/potential-cases.html",
    "title": "Potential cases",
    "section": "",
    "text": "A Europe-Canada joint infrastructure and network for enhancing multi-cohort studies in cardiovascular research.\nThe mission is to develop the first integrated data platform for cross-border data sharing and personalized medicine research in cardiology, integrating cohorts from Europe, Canada and beyond.\nThe key objectives are the following:\n\nTo build a centralized platform for cardiovascular data across Europe, Canada and beyond\nTo facilitate big data research in personalized cardiovascular medicine\nTo apply new legal framework for Open Science across continents\nTo establish the largest international research community in cardiology\n\n\n\n\nStarted partership with Maelstrom in 2018\nListed on the Maelstrom website as a current partnership, but the project site is more ambiguous about current activities.\nThe github organization also seems stagnant since 2023 but seems to have producing lots of meaningful software, largely in relation to data transformation and access interfaces.\nhttps://maelstrom-research.org/network/eucanshare\nhttp://www.eucanshare.eu/\nhttps://github.com/euCanSHare\n\n\n\nCould be really useful for understanding how people manage cross-jurisdiction logistics and administration.\nNot so much focusing on the actual logistics, but perceptions of the challenges and the way people going about resolving perceived challenges.\n\n\n\nLed by Karim Lekadir at the University of Barcelona, with 17 partner institutions across the EU and Canada\nCanadian centres seem to actually be omited to NcMaster, plus the RI-MUHC for access to Maelstrom and McGill’s Centre of Genomics and Policy.\nMcMaster seems to be contributing via their lead on the Canadian Alliance for Healthy Hearts and Minds (CAHMM), which is a very large and ongoing cohort study.\n\n\n\n\n\nIntegrating pregnancy and birth cohort studies\n\n\n\nPast partership, initiated in 2016\nhttps://www.maelstrom-research.org/network/reach\nBergeron et al. (2021)\nI wonder what its current status is.\n\n\n\nPapers demonstrated major concern with cataloging infrastructure\n\n\n\n\n\nCanadian Urban Environmental Health Research Consortium\nMore interdisciplinary, includes environmental data\nBased at Dalla Lana School of Public Health at UofT\nhttps://healthydesign.city/ seems like a spinoff service to make the CANUE work more accessible to planners and community stakeholders\n\n\n\nhttps://canue.ca\nPast Maelstrom partner, started in 2017"
  },
  {
    "objectID": "notes/potential-cases.html#top-candidates",
    "href": "notes/potential-cases.html#top-candidates",
    "title": "Potential cases",
    "section": "",
    "text": "A Europe-Canada joint infrastructure and network for enhancing multi-cohort studies in cardiovascular research.\nThe mission is to develop the first integrated data platform for cross-border data sharing and personalized medicine research in cardiology, integrating cohorts from Europe, Canada and beyond.\nThe key objectives are the following:\n\nTo build a centralized platform for cardiovascular data across Europe, Canada and beyond\nTo facilitate big data research in personalized cardiovascular medicine\nTo apply new legal framework for Open Science across continents\nTo establish the largest international research community in cardiology\n\n\n\n\nStarted partership with Maelstrom in 2018\nListed on the Maelstrom website as a current partnership, but the project site is more ambiguous about current activities.\nThe github organization also seems stagnant since 2023 but seems to have producing lots of meaningful software, largely in relation to data transformation and access interfaces.\nhttps://maelstrom-research.org/network/eucanshare\nhttp://www.eucanshare.eu/\nhttps://github.com/euCanSHare\n\n\n\nCould be really useful for understanding how people manage cross-jurisdiction logistics and administration.\nNot so much focusing on the actual logistics, but perceptions of the challenges and the way people going about resolving perceived challenges.\n\n\n\nLed by Karim Lekadir at the University of Barcelona, with 17 partner institutions across the EU and Canada\nCanadian centres seem to actually be omited to NcMaster, plus the RI-MUHC for access to Maelstrom and McGill’s Centre of Genomics and Policy.\nMcMaster seems to be contributing via their lead on the Canadian Alliance for Healthy Hearts and Minds (CAHMM), which is a very large and ongoing cohort study.\n\n\n\n\n\nIntegrating pregnancy and birth cohort studies\n\n\n\nPast partership, initiated in 2016\nhttps://www.maelstrom-research.org/network/reach\nBergeron et al. (2021)\nI wonder what its current status is.\n\n\n\nPapers demonstrated major concern with cataloging infrastructure\n\n\n\n\n\nCanadian Urban Environmental Health Research Consortium\nMore interdisciplinary, includes environmental data\nBased at Dalla Lana School of Public Health at UofT\nhttps://healthydesign.city/ seems like a spinoff service to make the CANUE work more accessible to planners and community stakeholders\n\n\n\nhttps://canue.ca\nPast Maelstrom partner, started in 2017"
  },
  {
    "objectID": "notes/potential-cases.html#possibly-suitable",
    "href": "notes/potential-cases.html#possibly-suitable",
    "title": "Potential cases",
    "section": "Possibly suitable",
    "text": "Possibly suitable\n\nInterConnect\n\nOptimising existing data for new research into diabetes and obesity\nEU + UKRI funded\nStarted partership with Maelstrom in 2014\n\n\n\nCAPACIty\n\nCAnadian PediAtric diabetes ConsortIum (CAPACIty) is a network of 15 childhood diabetes centers from across Canada.\nPartnering with patients/families and health care professionals to jointly design and develop a Canadawide childhood diabetes registry and research platform.\nThe registry will enable us to improve diabetes care and health outcomes for Canadian youth through comparison of diabetes care quality and outcomes between Canadian diabetes centers, quality improvement initiatives, patient-informed research initiatives across Canada, and successful advocacy work.\n\n\n\nUnnamed project\n\nLength of storage of red blood cell units as a risk factor for hospital-acquired infections: an individual patient data meta-analysis\nStarted partership with Maelstrom in 2020\n\n\n\nCould be interesting as a relatively low-scale initiative"
  },
  {
    "objectID": "notes/potential-cases.html#not-suitable",
    "href": "notes/potential-cases.html#not-suitable",
    "title": "Potential cases",
    "section": "Not suitable",
    "text": "Not suitable\n\nCanPath\n\nFormerly called Canadian Partnership for Tomorrow’s Health\nCanada’s largest population health study\n\n\n\nOne of the projects that would be supported by the workshop that was summarized in Doiron, Raina, and Fortier (2013).\nAlso mentioned in Doiron et al. (2017).\n\n\n\nCurrent Maelstrom partner, began in 2014.\nhttps://maelstrom-research.org/network/cptp-1\nhttps://doi.org/10.1503/cmaj.170292\n\n\n\nhttps://canpath.ca/\nThe website seem less concerned with data-drivenness, more focused on the areas that the work will address, as well as participant and community engagement.\n\n\n\nCanadian Longitudinal Study on Aging (CLSA)\n\nOne of the projects that would be supported by the workshop that was summarized in Doiron, Raina, and Fortier (2013).\nAlso mentioned in Doiron et al. (2017).\nSeems related with CanPath\n\n\n\nBegan as a Maelstrom partner in 2017.\nhttps://maelstrom-research.org/study/clsa\nhttps://doi.org/10.1017/s0714980809990055\nhttps://doi.org/10.1093/ije/dyz173\n\n\n\nhttps://www.clsa-elcv.ca/\n\n\nCombines data from multiple cohors, including CARTaGENE.\n\n\nCARTaGENE\n\nhttps://cartagene.qc.ca/en/\nhttps://en.wikipedia.org/wiki/CARTaGENE_biobank\n\n\n\nA long-term cohort study for the investigation of modifiable environmental and lifestyle factors and the genomic determinants of chronic diseases in Quebec, Canada.\nInitially founded by Dr. Claude Laberge and Prof. Bartha Maria Knoppers, CARTaGENE is today under the scientific direction of Dr. Guillaume Lettre, Dr. Simon Gravel and Dr. Vikki Ho.\nA regional cohort member of the Canadian Partnership for Tomorrow’s Health (CanPath)\n\n\n\nCurrent Maelstrom partner, began in 2018.\nhttps://maelstrom-research.org/study/cag\nhttps://doi.org/10.1093/ije/dys160\n\n\n\nHealthy Obesity Project (HOP)\n\nThe ilustrative case from Doiron et al. (2013)\nPrimary paper is van Vliet-Ostaptchouk et al. (2014)\n\n\n\nAvon Longitudinal Study of Parents and Children\n\nBased in the UK\n\n\n\nAthlos: Ageing Trajectories of Health: Longitudinal Opportunities and Synergies\n\nPan-European, but lead is based in Spain\n\n\n\nMindmap: Promoting mental well-being and healthy ageing in cities\n\nPan-European\nSeems very similar to CanPath in terms of scope and governance"
  },
  {
    "objectID": "notes/maelstrom-readings.html",
    "href": "notes/maelstrom-readings.html",
    "title": "Maelstrom reading notes",
    "section": "",
    "text": "Initial overview of data harmonization procedures, using the Healthy Obesity Project (HOP) as an illustrative case.\nOutlines the technical apparatus, especially for DataShield, but also broadly describes the discursive process of arriving at a DataSchema that is both functional and flexible.\n\nThis description is quite broad and abstracy, seems somewhat ideal and aspirational.\n\nDescribes reliance on international standards, such as the International Labour Organization’s International Standard Classification of Occupations.\n\nIt seems like these are used as black boxes that encapsulate a series of tensions which epidemiologists are unconcerned with; in effect, they simplify the need for stretching the collaborative ties even further than they are already extended, they represent matters out of scope for deeper discursive engagement.\n\nIt is notable that they emphasize that it’s easy to set up and use DataShield and Maelstorm toolkits independently of university IT and that it can be run using RStudio installed on a basic laptop.\n\nMaybe look into the historical context (2013) and the evolving role of university IT in software selection.\n\nThe conclusion states that the HOP project was successful in its harmonization efforts, but does not go as far as to state that it produced meaningful findings as a result of harmonization.\n\nI may take some time to find and read studies that used these data to see what’s what.\nThis seems like the main one: https://doi.org/10.1186/1472-6823-14-9, but these other papers may or not not also be relevant:\n\nhttps://doi.org/10.1016/j.smhl.2021.100263\nhttps://doi.org/10.1007/s10654-014-9977-1\nhttps://doi.org/10.1530/EJE-14-0540\nhttps://doi.org/10.1007/S13679-020-00375-0\nhttps://doi.org/10.1093/eurpub/ckac061"
  },
  {
    "objectID": "notes/maelstrom-readings.html#doiron2013",
    "href": "notes/maelstrom-readings.html#doiron2013",
    "title": "Maelstrom reading notes",
    "section": "",
    "text": "Initial overview of data harmonization procedures, using the Healthy Obesity Project (HOP) as an illustrative case.\nOutlines the technical apparatus, especially for DataShield, but also broadly describes the discursive process of arriving at a DataSchema that is both functional and flexible.\n\nThis description is quite broad and abstracy, seems somewhat ideal and aspirational.\n\nDescribes reliance on international standards, such as the International Labour Organization’s International Standard Classification of Occupations.\n\nIt seems like these are used as black boxes that encapsulate a series of tensions which epidemiologists are unconcerned with; in effect, they simplify the need for stretching the collaborative ties even further than they are already extended, they represent matters out of scope for deeper discursive engagement.\n\nIt is notable that they emphasize that it’s easy to set up and use DataShield and Maelstorm toolkits independently of university IT and that it can be run using RStudio installed on a basic laptop.\n\nMaybe look into the historical context (2013) and the evolving role of university IT in software selection.\n\nThe conclusion states that the HOP project was successful in its harmonization efforts, but does not go as far as to state that it produced meaningful findings as a result of harmonization.\n\nI may take some time to find and read studies that used these data to see what’s what.\nThis seems like the main one: https://doi.org/10.1186/1472-6823-14-9, but these other papers may or not not also be relevant:\n\nhttps://doi.org/10.1016/j.smhl.2021.100263\nhttps://doi.org/10.1007/s10654-014-9977-1\nhttps://doi.org/10.1530/EJE-14-0540\nhttps://doi.org/10.1007/S13679-020-00375-0\nhttps://doi.org/10.1093/eurpub/ckac061"
  },
  {
    "objectID": "notes/maelstrom-readings.html#doiron2017",
    "href": "notes/maelstrom-readings.html#doiron2017",
    "title": "Maelstrom reading notes",
    "section": "Doiron et al. (2017)",
    "text": "Doiron et al. (2017)\n\nAn overview of the key software that facilitates data harmonization practices under Maelstrom, also briefly touched upon in Doiron et al. (2013).\nPage 1373 refers to graphical and programmatic interfaces and assumes certain roles and tasks associated with each.\nBriefly describes its use by the Canadian Longitudinal Study on Aging (CLSA), the Canadian Partnership for Tomorrow Project (CPTP) and InterConnect, primarily by describing the range and quantity of data that these systems manage in each case.\n\n\nOpal provides a centralized web-based data management system allowing study coordinators and data managers to securely import/export a variety of data types (e.g. text, nu merical, geolocation, images, videos) and formats (e.g. SPSS, CSV) using a point-and-click interface. Opal then converts, stores and displays these data under a standar dized model.\n\n\nMica is used to create websites and metadata portals for individual epidemiological studies or multi-study consor tia, with a specific focus on supporting observational co hort studies. The Mica application helps data custodians and study or network coordinators to efficiently organize and disseminate information about their studies and net works without significant technical effort."
  },
  {
    "objectID": "notes/maelstrom-readings.html#fortier2010",
    "href": "notes/maelstrom-readings.html#fortier2010",
    "title": "Maelstrom reading notes",
    "section": "Fortier et al. (2010)",
    "text": "Fortier et al. (2010)\n\nA very grandiose paper presenting the grand vision for DataSHaPER, which would eventually become Maelstrom.\n\nLots of co-authors!\n\nInvokes the pan-European EPIC project (European Prospective Investigation into Cancer and Nutrition), which faced numerous data synthesis challenges despite its proactive effort to coordinate work across numerous research centres.\n\n\nTwo complementary approaches may be adopted to support effective data synthesis. The first one principally targets ‘what’ is to be synthesized, whereas the other one focuses on ‘how’ to collect the required information. Thus: (i) core sets of information may be identified to serve as the foundation for a flexible approach to harmonization; or (ii) standard collection devices (questionnaires and stand ard operating procedures) may be suggested as a required basis for collection of information.\n\n\nDataSHaPER is an acronym for DataSchema and Harmonization Platform for Epidemiological Research.\n\n\nIn an ideal world, information would be ‘prospectively harmonized’: emerging studies would make use, where possible, of harmonized questionnaires and standard operating procedures. This enhances the potential for future pooling but entails significant challenges —- ahead of time -— in developing and agree ing to common assessment protocols. However, at the same time, it is important to increase the utility of existing studies by ‘retrospectively harmonizing’ data that have already been collected, to optimize the subset of information that may legitimately be pooled. Here, the quantity and quality of infor mation that can be pooled is limited by the heterogeneity intrinsic to the pre-existing differences in study design and conduct.\n\nCompares prospective and retrospective harmonizatiom, with the former being presented as ideal, and the latter being a pragmatic reconciliation in acknowledgement that the former is essentially impossible to achieve.\n\nDataSHaPER is strikingly similar to OCHRE:\n\nXML-based data structures\nGenesis of a generic and ultimately optional base-level schema that illustrates the kind of data that the data structure may hold in ways that are immediately recognizable to all practitioners (at OCHRE it was associations between contexts and finds)\nSeparate harmonization platform where users can edit and manipulate records and associations between them\n\n\n\nThe question ‘What would constitute the ultimate proof of success or failure of the DataSHaPER approach’ needs to be addressed. Such proof will necessarily accumulate over time, and will involve two fundamental elements: (i) ratification of the basic DataSHaPER approach; and (ii) confirmation of the quality of each individual DataSHaPER as they are developed and/or extended. An important indication of the former would be provided by the widespread use of our tools. However, the ultimate proof of principle will necessarily be based on the generation of replicable scientific findings by researchers using the approach. But, for such evidence to accumulate it will be essential to assure the quality of each individual DataSHaPER. Even if the fundamental approach is sound, its success will depend critically on how individual DataSHaPERs are constructed and used. It seems likely that if consistency and quality are to be assured in the global development of the approach, it will be necessary for new DataSHaPERs to be formally endorsed by a central advisory team."
  },
  {
    "objectID": "notes/maelstrom-readings.html#fortier2011",
    "href": "notes/maelstrom-readings.html#fortier2011",
    "title": "Maelstrom reading notes",
    "section": "Fortier et al. (2011)",
    "text": "Fortier et al. (2011)\nThis paper responds to Hamilton et al. (2011), which presents an effort to devise a standardized nomenclature. The response is basically to advocate for a more flexible approach, rather than a stringent one promoted by Hamilton et al. (2011). It draws extensively from concepts published in the foundational paper by Fortier et al. (2010).\n\nTwo complementary approaches to harmonization may be adopted to support effective data synthesis or comparison across studies. The first approach makes use of identical data collection tools and procedures as a basis for harmoni zation and synthesis. Here we refer to this as the ‘‘stringent’’ approach to harmonization. The second approach is con sidered ‘‘flexible’’ harmonization. Critically, the second ap proach does not demand the use of identical data collection tools and procedures for harmonization and synthesis. Rather, it has to be based on sound methodology to ensure inferential equivalence of the information to be harmonized. Here, standardization is considered equivalent to stringent harmonization. It should, however, be noted that the term standard is occasionally employed to refer to common con cepts or comparable classification schemes but does not necessarily involve the use of identical data collection tools and procedures (12, 13).\n\nThis directly parallels the distinction made in Fortier et al. (2010) between “ideal” prospective and more pragmatic retrospective approaches to data harmonization.\n\nSynthesis of data using a flexible harmonization approach may be either prospective or retrospective. To achieve flexible prospective harmonization, investigators from several studies will agree on a core set of variables (or measures), compatible sets of data collection tools, and standard operating procedures but will allow a certain level of flexibilit in the specific tools and procedures used in each study (16, 17). Retrospective harmonization targets synthesis of information already collected by existing legacy studies (15, 18, 19). As an illustrative example, using retrospective harmonization, researchers will define a core set of variables (e.g., body mass index, global level of physical activity) and, making use of formal pairing rules, assess the potential for each participating study to create each variable (15). The ability to retrospectively harmonize data from existing studies facilitates the rapid generation of new scientifi knowledge.\n\nI wonder why there is no example provided for prospective data harmonization. Is it because it is ideal and not realistic? I’d argue that it is simply what occurs within individual projects."
  },
  {
    "objectID": "notes/maelstrom-readings.html#fortier2017",
    "href": "notes/maelstrom-readings.html#fortier2017",
    "title": "Maelstrom reading notes",
    "section": "Fortier et al. (2017)",
    "text": "Fortier et al. (2017)\nExplicit statement regarding the rationale and presumed benefits of harmonization right in the first paragraph:\n\nThe rationales underpinning such an approach include ensuring: sufficient statistical power; more refined subgroup analysis; increased exposure hetero geneity; enhanced generalizability and a capacity to under take comparison, cross validation or replication across datasets. Integrative agendas also help maximizing the use of available data resources and increase cost-efficiency of research programmes.\n\nSummarized in bullet points:\n\nensuring sufficient statistical power\nmore refined subgroup analysis\nincreased exposure heterogeneity\nenhanced generalizability\na capacity to undertake comparison, cross validation or replication across datasets.\nmaximizing the use of available data resources\nincrease cost-efficiency of research programmes\n\nClearly defines harmonization and its benefits:\n\nEssentially, data harmonization achieves or improves comparability (inferential equivalence) of similar measures collected by separate studies.\n\nAdds an additional argument for retrospective harmonization on top of prior discussion of retrospective/prospective approaches (cf. Fortier et al. (2010); Fortier et al. (2011)):\n\nRepeating identical protocols is not necessarily viewed as providing evidence as strong as that obtained by exploring the same topic but using different designs and measures.\n\nAlso relates retrospective harmonization from systematic meta reviews. In fact, the paper basically responds to calls for more structured guidelines for data harmonization, similar to those that had been produced to support structured metareviews in the years prior to this publication. The authors identify several papers that have done similar guidelines or reports on harmonization practices, which they claim are too broad. Those papers include:\n\nRolland et al. (2015)\n\nhttps://doi.org/10.1093/aje/kwv133\n\nSchaap et al. (2011)\n\nhttps://doi.org/10.1186/1471-2474-12-272\n\nBennett et al. (2011)\n\nhttps://doi.org/10.1002/gepi.20564\n\nHohmann et al. (2012)\n\nThis paper reports the findings of a systematic inquiry made to data harmonization initiatives, whose data comprise responses to a questionnaire. The findings indicate that procedures were more attentively follows during earlier stages, such as when matching and aligning available data with the project’s designated scope. However, procedures were less sound with regards to documenting procedures, validating the results of data processing, and dissemination strategy. There is a notable division between work that occurs before and after people actually begin handling the data, which indicates a tension between aspirational plans and tangible experiences.\n\nRespondents were asked to delineate the specific procedures or steps undertaken to generate the harmonized data requested. Sound procedures were generally described; however, the terminologies, sequence and technical and methodological approaches to these procedures varied considerably. Most of the procedures mentioned were related to defining the research questions, identifying and selecting the participating studies (generally not through a systematic approach), identifying the targeted variables to be generated and processing data into the harmonized variables. These procedures were reported by at least 75% of the respondents. On the other hand, few reported steps related to validation of the harmonized data (N=4; 11.8%), documentation of the harmonization process (N=5; 14.7%) and dissemination of the harmonized data outputs (N=2; 5.9%).\n\nThe paper summarizes some specific “potential pitfalls” reported by respondents to their survey:\n\nensuring timely access to data;\nhandling dissimilar restrictions and procedures related to individual participant data access;\nmanaging diversity across the rules for authorship and recognition of input from study-specific investigators;\nmobilizing sufficient time and resources to conduct the harmonization project;\ngathering information and guidance on harmonization approaches, resources and techniques;\nobtaining comprehensive and coherent information on study-specific designs, standard operating procedures, data collection devices, data format and data content;\nunderstanding content and quality of study-specific data;\ndefining the realistic, but scientifically acceptable, level of heterogeneity (or content equivalence) to be obtained;\ngenerating effective study-specific and harmonized datasets, infrastructures and computing capacities;\nprocessing data under a harmonized format taking into account diversity of: study designs and content, study population, synchronicity of measures (events measured at different point in time or at different intervals when repeated) etc;\nensuring proper documentation of the process and decisions undertaken throughout harmonization to ensure transparency and reproducibility of the harmonized datasets;\nmaintaining long-term capacities supporting dissemination of the harmonized datasets to users.\n\nIt’s not made clear how these responses were distributed among respondents.\nThe authors then identify several absolute essential requirements needed to achieve success:\n\nCollaborative framework: a collaborative environment needs to be implemented to ensure the success of any harmonization project. Investigators involved should be open to sharing information and knowledge, and investing time and resources to ensure the successful implementation of a data-sharing infrastructure and achievement of the harmonization process.\nExpert input: adequate input and oversight by experts should be ensured. Expertise is often necessary in: the scientific domain of interest (to ensure harmonized variables permit addressing the scientific question with minimal bias); data harmonization methods (to support achievement of the harmonization procedures); and ethics and law (to address data access and integration issues).\nValid data input: study-specific data should only be harmonized and integrated if the original data items collected by each study are of acceptable quality.\nValid data output: transparency and rigour should be maintained throughout the harmonization process to ensure validity and reproducibility of the harmonization results and to guarantee quality of data output. The common variables generated necessarily need to be of acceptable quality.\nRigorous documentation: publication of results generated making use of harmonized data must provide the information required to estimate the quality of the process and presence of potential bias. This includes a description of the: criteria used to select studies; process achieved to select and define variables to be harmonized; procedures used to process data; and characteristics of the study-specific and harmonized dataset(s) (e.g. attribute of the populations).\nRespect for stakeholders: all study-specific as well as network-specific ethical and legal components need to be respected. This includes respect of the rights, intellectual property interests and integrity of study participants, investigators and stakeholders.\n\nThe authors describe how they arrived at guidelines following the results of this study:\n\nA consensus approach was used to assemble information about pitfalls faced during the harmonization process, establish guiding principles and develop the guidelines. The iterative process (informed by workshops and case studies) permitted to refine and formalize the guide lines. The only substantive structural change to the initial version proposed was the addition of specific steps relating to the validation, and dissemination and archiving of harmonized outputs. These steps were felt essential to em phasize the critical nature of these particular issues.\n\nThe paper outlines a checklist of stages that data harmonization initiatives need to go through to produce ideal outcomes. For each task, they describe a scenario in which the task can be said to be complete, whhich resembles an ideal outcome. This is described in the paper, summarized in a table, and more comprehensively documented in the supplementary materials.\nAlso worth noting, this paper includes a list of harmonization initiatives that I may consult when selecting cases. I’m not quite sure how useful it will be since the findings don’t really break down the distribution of responses in any detail, but maybe the authors have done this analysis and not published it."
  },
  {
    "objectID": "notes/maelstrom-readings.html#bergeron2018",
    "href": "notes/maelstrom-readings.html#bergeron2018",
    "title": "Maelstrom reading notes",
    "section": "Bergeron et al. (2018)",
    "text": "Bergeron et al. (2018)\nThe authors reference the drive for efficiency as a motivating factor that drives open data:\n\nHowever, many cohort databases remain under-exploited. To address this issue and speed up discovery, it is essential to offer timely access to cohort data and samples.\n\nHowever the paper is actually about the need for better and more publicly accessible documentation about data.\nThe authors state that catalogues exist to promote discoverability of data and samples and to answer the data documentation needs of individual studies.\nThey draw attention to the importance of catalogues in research networks (analyzing data across studies), which establish portals that document “summary statistics on study subjects, such as the number of participants presenting specific characteristics (e.g. diseases or exposures)”.\nThe authors outline several challenges that inhibit or limit the potential value of catalogues:\n\nThe quality of a catalogue directly depends on the quality and comprehensiveness of the study-specific information documented. But, maintaining and providing access to understandable and comprehensive documentation to external users can be challenging for cohort investigators, and require resources not always available, particularly for the very small or long-established studies. In addition, the technical work required to build and maintain a catalogue is particularly demanding. For example, gathering comprehensive and comparable information on study designs necessitates the implementation of rigorous procedures and working in close collaboration with study investigators. Manual classification of variables is also a long and a tedious process prone to human error. Moreover, the information collected needs to be regularly revised to update metadata with new data collections. These challenges, among others, can lead to the creation of catalogues with partial or disparate information across studies, documenting limited subsets of variables (e.g. only information collected at baseline) or including only studies with data dictionaries available in a specific language or format.\n\nBullet point summary:\n\nA catalogue’s quality depends on the quality and comprehensiveness of documentation provided by individual studies\nCohort investigators, i.e. leaders of individual studies, are under-equipped to provide such comprehensive documentation\n\nDo they just need material support? Or also guidance on how to do it, what factors to account for, etc?\n\nTechnical work for building and maintaining a catalogue is demanding\n\nI’m not sure if they example they provide to illustrate these challenges aligns with what I would call “technical work”; they refer to precise and detailed documentation in direct consultation with individual study maintainers, and I suppose the discussion about and documentation of methodological details is technical in that it corresponds with the work that was actually done on the ground, using data collection and processing instruments\n\nClassification of variables is a long and tedious process\n\nWhat makes it long and tedious? This isn’t really specified\nThey recite that this is prone to human error, but I wonder what successful or error-ful (?) outcomes would look like and how they would differ\n\nThe information needs to be regularly revised and updated\n\nThe authors recommendations to resolve these concerns:\n\nHowever, to truly optimize usage of available data and leverage scientific discovery, implementation of high quality metadata catalogues is essential. It is thus important to establish rigorous standard operating procedures when developing a catalogue, obtain sufficient financial support to implement and maintain it over time, and where possible, ensure compatibility with other existing catalogues.\n\nBullet point summary:\n\nestablish rigorous standard operating procedures when developing a catalogue\nobtain sufficient financial support to implement and maintain it over time\nwhere possible, ensure compatibility with other existing catalogues"
  },
  {
    "objectID": "notes/maelstrom-readings.html#bergeron2021",
    "href": "notes/maelstrom-readings.html#bergeron2021",
    "title": "Maelstrom reading notes",
    "section": "Bergeron et al. (2021)",
    "text": "Bergeron et al. (2021)\nIdentifies several registries of relevant cohorts, but notes that they face challenges getting the data together. Namely, issues concerning institutional policies concerning data-sharing, lack of open access to cohort data and to documentation about the data, the data’s complexity which makes it difficult to harmonize across studies, and lack of access to funding, secure data environments, and specialized expertise and resources.\nThe Research Advancement through Cohort Cataloguing and Harmonization (ReACH) initiative was establihed in collaboration with Maelstrom to overcome some of these barriers in the context of Developmental Origins of Health and Disease (DOHaD) research.\nThe authors briefly summarize some projects that rely on ReACH data, and provide a more comprehensive table of ongoing and unpublished work.\nIn the supplementary materials, the authors also include an illustrative example specific tasks, decisisions and actions that one might got through when using ReACH data. It is a broad-level but fairly sober account of how one would navigate the catalogue and engage with collaborators."
  },
  {
    "objectID": "notes/maelstrom-readings.html#wey2024",
    "href": "notes/maelstrom-readings.html#wey2024",
    "title": "Maelstrom reading notes",
    "section": "Wey and Fortier (2024)",
    "text": "Wey and Fortier (2024)\nAn overview of the harmonization procedires applied in CanPath and MINDMAP. Authored by two members of the Maelstrom team, but no one from these initiatives.\nAt first, comes across as another broad-level overview of processes. But, as is elicited in the conslusion, the paper highlights some subtle divergent approaches, some of which are relevant to my project and I picked up here.\nInteresting bit about use of centralized systems or systems that are more conducive to end-users’ individual workflows.\n\nIn both illustrative projects, information about participating studies and data collected was gathered and made available on the central data server. In the CanPath BL-HRFQ, harmonization documentation and processing scripts were held centrally and only updated by Maelstrom Research. In MINDMAP, because multiple groups simultaneously worked on generating harmonization processing scripts for different areas of information, working versions of the DataSchema and processing scripts were held on a GitHub, allowing for better version control and dynamic updating of scripts by multiple remote data harmonizers.\n\nMore about the balance being struck at MINDMAP between institutional and popular tech and documentation platforms:\n\nIn the MINDMAP project, R markdown documents with applied harmonization scripts (Figure 13.1b) and any comments were preserved in the GitHub repository, which is also publicly accessible. Furthermore, summary statistics for MINDMAP variables are only available to approved data users through the secure server, as harmonized datasets are only intended for use within the MINDMAP network. Overviews of the harmonization process and outputs have also been published as open-access, peer-reviewed articles for both projects (Fortier et al. 2019; Wey et al. 2021).\n\nBit about existing collaborative ties making things much easier:\n\nthe prospective coordination and unu sually high ability for the harmonization team (Maelstrom Research) to work directly with study data managers on recently collected and documented study data resulted in high standardization and ability to resolve questions.\n\nData curation as an explicitly creative task, involving impactful decisions. Interesting that this was unexpected enough to warrant inclusion as a key challenge:\n\nIn MINDMAP, these differences were clearly documented in comments about harmonization for each source dataset to allow researchers to decide how to use the harmonized variable. In-depth exploration of the best statistical methods to harmonize these types of measures to maintain integrity of content while minimizing loss of information and methodological bias are important and active areas of research (Griffith et al. 2016; Van den Heuvel and Griffith 2016). The approach taken in this case was to apply simpler harmonization meth ods (i.e. rescaling) and document the transformation, leaving investigators the flexibility to further explore and analyze the harmonized datasets as appropriate for their research questions.\n\nAs with the above excerpt, documentation was considered as a viable mode of resolving or accounting for significant discrepancies:\n\nThe papers describing the harmonization projects attempt to highlight these considerations, for example, providing some comparison of demographics of the harmonized populations against the general populations from which they were drawn (Dummer et al. 2018; Fortier et al. 2019; Wey et al. 2021), listing sources of study-specific heterogeneity in the harmonized datasets to consider (Fortier et al. 2019), and pointing users to individual study documentation where more information on weights to use for analysis should be con sidered (e.g. Wey et al. 2021).\n\nThe bit about use of github was rationalized as a way of facilitating collaboration across insitutional boundaries. They refer to R and RMarkdown being open standards as the main reason, but I wonder if a part of it is that GitHub, being a non-institutional platform, was easier to use from an oboarding perspective:\n\nIn MINDMAP, due to the need for multiple international groups to work simultaneously and flexibly on the harmonization processing and frequently evolving versions of study-specific harmonization scripts, scripts for harmonization processing were written and applied entirely through R markdown in an RStudio interface, and the DataSchema and R markdown versions were maintained and frequently updated in a GitHub repository.\n\nPerhaps ironically, the private company and platform may have been used due to the strength of pan-european collaborative ties that institutions may not be able to keep up with. Whereas in Canada, with centralized project-oriented work, it may be much easier to enforce adoption of centralized tooling. This is just speculation."
  },
  {
    "objectID": "notes/maelstrom-readings.html#doiron2013a",
    "href": "notes/maelstrom-readings.html#doiron2013a",
    "title": "Maelstrom reading notes",
    "section": "Doiron, Raina, and Fortier (2013)",
    "text": "Doiron, Raina, and Fortier (2013)\nThis paper summarizes what was discussed at a workshop bringing together stakeholders who would contribute to two large data harmonization initiatives: the Canadian Longitudinal Study on Aging (CLSA) and the Canadian Partnership for Tomorrow Project (CPTP). It is therefore representative of plans and challenges that were held at an early stage when collaborations were being established.\nThe authors identify series of reasons for linking data, which I summarize here:\n\nMaximizing potential of disparate information resources\n\n\nenriching study datasets with additional data not being collected directly from study par ticipants\noffer vital information on health outcomes of participants\nvalidate self-reported information\n\n\nDrawing maximum value from data produced from public expenditure\n\n\noffers a cost-effective means to maximize the use of existing publicly funded data collections\n\n\nDevelop interdisciplinary collaborative networks\n\n\nby combining a wide range of risk factors, disease endpoints, and relevant socio-economic and biological measurements at a population level, linkage lays the groundwork for multidisciplinary health-research initiatives, which allow the exploration of new hypotheses not foreseeable using independent datasets\n\n\nEstablish long-lasting infrastructure and instill a collaborative culture\n\n\nLast, a coordinated pan-Canadian cohort-to-administrative linked database would establish legacy research infrastructures that will better equip the next generation of researchers across the country\n\nThe authors use the term “data linkage”:\n\nData linkage is “the bringing together from two or more different sources, data that relates to the same individual, family, place or event”. When linking data at the individual level, a common identifier (or a combination of identifiers) such as a personal health number, date of birth, place of residence, or sex, is used to combine data related to the same person but found in separate databases. Data linkage has been used in a number of research fields but is an especially valuable tool for health research given the large amount of relevant information collected by institutions such as governments, hospitals, clinics, health authorities, and research groups that can then be matched to data collected directly from consenting individuals participating in health research.\n\nThis is distinct from harmonization in that it is not meant to combine data with similar scope and schematic structure, but rather to relate information collected under various domains so that they could be more easily queried in tandem. I imagine this as reminiscient of establishing links between tables in a relational database.\nThe authors identify the open-endedness of the linked data as a unique challenge, without elaborating on this point:\n\nCLSA/CPTP-to-AHD linkage also poses unique challenges in that, in contrast to more traditional requests to link data to answer one-off research questions, it aims to establish a rich data repository that will allow investigators to answer a multitude of research questions over time.\n\nThe workshop participants established a 5-point plan:\n\nbuild strong collaborative relationships between stakeholders involved in data sharing (e.g., researchers, data custodians, and privacy commissioners);\nidentify an entity which could provide overall leadership as well as individual “champions” within each province;\nfind adequate and long-term resources and funding;\nclarify data linkage and data-sharing models and develop a common framework within which the data linkage process takes place; and\ndevelop a pilot project making use of a limited number of linked variables from participating provinces\n\nThe second point, about identifying “champions”, is kind of interesting, and I’d like to know more about what qualities these people were expcted to have, their domains of expertise, their collaborative/soft or technical skills, and how this plays into access to funds and general governance structures\nNeed to look at Roos, Menec, and Currie (2004), which they cite in the conclusion, specifically with reference to the aspiration to develop “information rich environments”. Seems like it is a primary source for the background on linked data in Manitoba and Australia."
  },
  {
    "objectID": "notes/maelstrom-readings.html#fortier2023",
    "href": "notes/maelstrom-readings.html#fortier2023",
    "title": "Maelstrom reading notes",
    "section": "Fortier et al. (2023)",
    "text": "Fortier et al. (2023)\nRelates harmonization to the FAIR principles, which has not really been featured much in arguments for harmonization in prior Maelstrom papers. Specifically, this paper frames harmonization as a necessary additional condition that enables FAIR data to be made useful; FAIR is deemed not enough.\n\nIn the following paper, we aim to provide an overview of the logistics and key ele ments to be considered from the inception to the end of collabo rative epidemiologic projects requiring harmonizing existing data.\n\nInteresting acronym/framework for defining research questions:\n\nThe research questions addressed and research plan proposed by harmonization initiatives need to be Feasible, Interesting, Novel, Ethical, and Relevant (FINER). (Cummings, Browner, and Hulley 2013)\n\nTable 1 lists examples of questions that could be addressed to help delineate analytical approach, practical requirements, and operations of a harmonization initiative. These are all questions that look toward or project specific goals, which then inform the strategies through which they may be achieved.\nThe supplementary materials include very detailed information about specific practices and objectives pertaining to the REACH initiative. However, it’s unclear how well this reflects any specific challenges experienced. In other words, I was hoping for something more candid.\nMoreover, this paper is also based on survey responses from 20 harmonization initiatives, but neither the findings resulting from the analysis, nor the data, are referenced or included. Is this the same as the one that informed Fortier et al. (2017)?\nLooming in the background of this paper is the DCC lifecycle model. However they do not cite DCC, or the field of digital curation in general. The DCC lifecycle model has always been presented as a guideline, sort of divorced from practical experience, or at least that’s how I’ve always understood it. Basically, and literally, a model for expected behaviours and interstitial outcomes. I think it would be interesting to explore (a) why the authors perceived need to present a model and (b) how they arrived at the phases and principles that are included in it. Is this tacit or common-sense thinking? Or was this directly informed by my concrete thinking from the field of digital curation?\nI should brush up on more recent work regarding the DCC, and specifically, critiques of it. Through a quick search, a few papers seem directly relevant:\n\nCox and Tam (2018)\nChoudhury, Huang, and Palmer (2020)\nRhee (2024)\n\nI think Cox and Tam (2018) may be especially relevant as a critique of the lifecycle metephor in a general sense. From the abstract, it seems like they identify various downsides pertaining to lifecycle models, specifically that they “mask various aspects of the complexity of research, constructing it as highly purposive, serial, uni-directional and occurring in a somewhat closed system.” I’m not sure how explicit the connection is, but I sense this ties into the general paradigm shift toward greater recognition of curation “in the wild”, as per Dallas (2016)."
  },
  {
    "objectID": "notes/maelstrom-readings.html#gaye2014",
    "href": "notes/maelstrom-readings.html#gaye2014",
    "title": "Maelstrom reading notes",
    "section": "Gaye et al. (2014)",
    "text": "Gaye et al. (2014)\nIntroduces DataShield.\nFrames DataShield as a technical fix to administrative problems:\n\nMany technical and policy measures can be enacted to render data sharing more secure from a governance per spective and less likely to result in loss of intellectual prop erty. For example, data owners might restrict data release to aggregate statistics alone, or may limit the number of variables that individual researchers might access for speci fied purposes. Alternatively, secure analysis centres, such as the ESRC Secure Data Service and SAIL represent major informatics infrastructures that can provide a safe haven for remote or local analysis/linkage of data from selected sources while preventing researchers from down loading the original data themselves. However, to comple ment pre-existing solutions to the important challenges now faced, the DataSHIELD consortium has developed a flexible new way to comprehensively analyse individual level data collected across several studies or sources while keeping the original data strictly secure. As a technology, DataSHIELD uses distributed computing and parallelized analysis to enable full joint analysis of individual-level data from several sources, e.g. research projects or health or administrative data—without the need for those data to move, or even be seen, outside the study where they usually reside. Crucially, because it does not require underpin ning by a major informatics infrastructure and because it is based on non-commercial open source software, it is both locally implementable and very cost effective.\n\nAdds a social/collaborative element to earlier arguments about the challenges inherent of prospective harmonization, highlighting a need for engagement with individual studies (either through direct or peripheral participation) to conduct research that was not initially planned for:\n\nUnfortunately, both [study-level metadata] SLMA and [individual-level metadata] ILMA present significant problems Because SLMA com bines analytical results (e.g. means, odds ratios, regression coefficients) produced ahead of time by the contributing studies, it can be very inflexible: only the pre-planned analyses undertaken by all the studies can be converted into joint results across all studies combined. Any additional analyses must be requested post hoc. This hinders exploratory analysis for example the investigation of sub-groups, or interactions between key variables.\n\nProvides a detailed overview of how DataShield was implemented for HOP (Healthy Obesity Project), including the code used to generate specific figures and analyses. Hoever it does not really describe or reflect upon the processes through which the code was developed.\nThe authors highlight the fact that certain analytical approaches are not possible using DataShield, especially analysis that visualize individual data points. It’s unclear how they enforce this, or whether it’s an implicit limitation based on the data that DataShield participants provide.\n\nBecause in DataSHIELD potentially disclosive com mands are not allowed, some analyses that are possible in standard R are not enabled. In essence, there are two classes of limitation on potential DataSHIELD functional ity: (i) absolute limitations which require an analysis that can only be undertaken by enabling one of the functional ities (e.g. visualizing individual data points) that is explicitly blocked as a fundamental element of the DataSHIELD philosophy. For example, this would be the case for a standard scatter plot. Such limitations can never be circumvented and so alternatives (e.g. contour and heat map plots) are enabled which convey similar information but without disclosing individual data points; (ii) current limitations which are functions or models that we believe are implementable but we have not, as yet, under taken or completed the development work required. As examples, these latter include generalized linear mixed model (including multi-level modelling) and Cox regression.\n\nThe authors list numerous other limitations and challenges. Some have to do with what kinds of data DataShield can handle (something about horizontal and vertical that I do not yet fully understand). Other challenges include the need for data to be harmonized, and having to deal with governance concerns.\nNotably, the first challenge mentioned seems to contradict the statement earlier on (and made by Doiron et al. (2013)) that this is relatively easy to set up. The authors acknowledge the fact that coding for analysis using DataShield has a steep learning curve and requires some pre-planning to enable results from satellite computers to be properly combined. Their mitigation is to black-box these concerns by implementing simpler client-side functions that mask the more complex behaviours (and presumably translate error messages in ways that users can understand and act to resolve!).\n\nDespite its potential utility, implementation of DataSHIELD involves significant challenges. First, although set-up is fundamentally straightforward, application involves a relatively steep learning curve because the command structure is complex: it demands specification of the analysis to be undertaken, the studies to use and how to combine the results. In mitigation, most complex serverside functions are now called using simpler client-side functions and we are working on a menu-driven implementation.\n\nAlso interesting that they note how there may be unanticipated problems, either accidental or malicious, and their way of mitigating against this is to log all commands:\n\nFifth, despite the care taken to set up DataSHIELD so that it works properly and is non-disclosive, it is possible that unanticipated prob lems (accidental or malicious) may arise. In order to iden tify, describe and rectify any errors or loopholes that emerge and in order to identify deliberate miscreants, all commands issued on the client server and enacted on each data server are permanently logged.\n\nThis is even more interesting in light of their continuous reference to “data.care”, which they do not address in depth, but which seems to have been a scandal involving unauthorized release of personal health data used in research.\nThe authors add an additional caveat concerning the need to ensure that the data are cleaned in advance.\n\nBut, to be pragmatic, many of the routinely collected healthcare and administra tive databases will have to undergo substantial evolution before their quality and consistency are such that they can directly be used in high-quality research without exten sive preparatory work. By its very nature, such preparation—which typically includes data cleaning and data harmonization—cannot usually be undertaken in DataSHIELD, because it involves investigating discrepan cies and/or extreme results in individual data subjects: the precise functionality that DataSHIELD is designed to block. Such work must therefore be undertaken ahead of time by the data generators themselves—and this is de manding of time, resources and expertise that — at present — many administrative data providers may well be unwilling and/or unable to provide. That said, if the widespread us ability of such data is viewed as being of high priority, the required resources could be forthcoming.\n\nThis corresponds with another limitation identified earlier, namely with regards to identifying duplicate individual records across jurisdictional boundaries (which involves assumptions regarding nationality and identify – one of those weird myths that programmers can’t seem to let go!):\n\nSo far DataSHIELD has been applied in settings where individual participants in different studies are from different countries or from different regions so it is unlikely that any one person will appear in more than one source. However, going forward, that cannot al ways be assumed. We have therefore been consider ing approaches to identify and correct this problem based on probabilistic record linkage. In the genetic setting 48 the BioPIN provides an alternative solution. Ongoing work is required.\n\nNote the last line of the prior block quote regarding data cleaning:\n\nThat said, if the widespread us ability of such data is viewed as being of high priority, the required resources could be forthcoming.\n\nThis seems like a thread worth tugging at!"
  },
  {
    "objectID": "notes/maelstrom-readings.html#wolfson2010",
    "href": "notes/maelstrom-readings.html#wolfson2010",
    "title": "Maelstrom reading notes",
    "section": "Wolfson et al. (2010)",
    "text": "Wolfson et al. (2010)\nx"
  },
  {
    "objectID": "interview-protocol.html",
    "href": "interview-protocol.html",
    "title": "Interview Protocol",
    "section": "",
    "text": "Interviews may be held either in-person or through online video conference. All interviews will be held in quiet and comfortable environments, such as office spaces or conference rooms.\nI will record all in-person interviews using a SONY ICD-UX560 audio recorder to capture audio in the lossless 16 bit 44.1 kHz Linear PCM wav format, with additional audio filters to enhance playback during transcription, if necessary.\nI will also record in-person interview sessions using a GoPro Hero 4 Silver action camera, depending on participants willingness to be video recorded. Based on prior interviews with scientists about their research experiences, I found that interviewees like to show me, rather than merely tell me, about what they are working on and the means through which they engage with information systems. The camera may be leveraged to record spontaneous video records of these demonstrations and provide me with an additional rich data source for further analysis. Moreover, the camera provides an additional backup audio recording in case of data loss on the primary recording device.\nRemote interviews will be recorded using the video conferencing software’s built-in recording tools. Participants will be instructed to disable their microphones or video cameras prior to initiating recording if they have opted to not be recorded through these media. The researcher will record all media locally and refrain from using any cloud services to store or modify the records which the video conference software may provide.\nI will also record handwritten notes comprising descriptive accounts of activities and interactions when recording devices are switched off, as well as preliminary interpretations of observed behaviours and notes on things I plan to follow up on at a later time.",
    "crumbs": [
      "Interview Protocol"
    ]
  },
  {
    "objectID": "interview-protocol.html#interview-records",
    "href": "interview-protocol.html#interview-records",
    "title": "Interview Protocol",
    "section": "",
    "text": "Interviews may be held either in-person or through online video conference. All interviews will be held in quiet and comfortable environments, such as office spaces or conference rooms.\nI will record all in-person interviews using a SONY ICD-UX560 audio recorder to capture audio in the lossless 16 bit 44.1 kHz Linear PCM wav format, with additional audio filters to enhance playback during transcription, if necessary.\nI will also record in-person interview sessions using a GoPro Hero 4 Silver action camera, depending on participants willingness to be video recorded. Based on prior interviews with scientists about their research experiences, I found that interviewees like to show me, rather than merely tell me, about what they are working on and the means through which they engage with information systems. The camera may be leveraged to record spontaneous video records of these demonstrations and provide me with an additional rich data source for further analysis. Moreover, the camera provides an additional backup audio recording in case of data loss on the primary recording device.\nRemote interviews will be recorded using the video conferencing software’s built-in recording tools. Participants will be instructed to disable their microphones or video cameras prior to initiating recording if they have opted to not be recorded through these media. The researcher will record all media locally and refrain from using any cloud services to store or modify the records which the video conference software may provide.\nI will also record handwritten notes comprising descriptive accounts of activities and interactions when recording devices are switched off, as well as preliminary interpretations of observed behaviours and notes on things I plan to follow up on at a later time.",
    "crumbs": [
      "Interview Protocol"
    ]
  },
  {
    "objectID": "interview-protocol.html#interview-guide",
    "href": "interview-protocol.html#interview-guide",
    "title": "Interview Protocol",
    "section": "Interview guide",
    "text": "Interview guide\nInterviews are oriented by my goal to document values and attitudes concerning data harmonization efforts, as elicited by research participants in their responses. Participants will be asked to reflect on:\n\nthe motivations for their initiatives;\nthe challenges they experience;\nhow they envision success and failure;\ntheir perceptions of their own roles and the roles of other team members and stakeholders;\nthe values that inform their decisions;\nhow the technological apparatus they set up enables them to realize their goals and values; and\nways in which they believe data-sharing could be improved.\n\nTo this end, each interview will proceed following a strategic order:\n\n1. Participants’ goals and perspectives\nFollow a life-history method to better understand participants’ professional backgrounds and their roles within their projects. The goal is to obtain information about their paths, not the rehearsed origin story.\n\nTo start, can you please tell me a little bit about your background?\nWhat is the project, and what is your role?\nHow did you find yourself in this role?\nHow has your previous experience prepared you for your role?\n\n\n\n2. Projects’ missions, purposes, motivations\nThis section is about the project in general, including its purpose, scope and value. Information about practices and procedures will be sought in a subsequent phase of the interview.\n\nWhat are the project’s goals?\nWhat makes the project unique?\nWhat is the project doing that no other similar project is doing?\nDo you consider this project as similar to any other initiatives?\nWhat are they, and in what ways are they simiar or different?\n\n\n\nWhat are the expected outcomes?\nHave you achieved these goals and outcomes?\nIf not Are you on track to achieving them?\nWhat are some challenges that the project experienced, and how have you worked to overcome them?\n\n\n\n3. Practices, procedures, relationships\nThis section asks about specific actions and interactions that the participant engages in.\n\nRoles and relationships\n\nWhat does your role entail?\nCan you provide a couple examples of things that you recently did in this capacity?\n\n\n\nWho else do you frequently rely on, and what are their roles?\nCan you describe what they do, and perhaps give a few examples drawn from their recent work?\n\nThe interview might proceed in different ways depending on their initial responses. Here are some questions I might ask, corresponding with the participants’ role and area of expertise.\n\n\nMaintaining the project community\n\nPlease briefly describe the process through which you obtain new partners or users.\nCan you please recall a recent example?\n\n\n\nHow well do you know each partner?\nDid you know them before their involvement?\n\n\n\nWould you describe the project as a tight knit community, or more open-ended?\n\n\n\nHow do you communicate with partners and contributors?\nWhat kinds of media or platforms do you use, and are they targeted for specific purposes? i.e. email, newsletters, social media, skype, personal communication at conferences\n\n\n\nAre there particular people in each project who you communicate with more frequently than others?\nWho are they, and why are these the people who you connect with?\n\n\n\nWhat do you consider your role or responsibility vis-a-vis the development/growth of this community?\nHow do you foster the community’s development and growth?\nDo you consider these efforts to be effective?\n\n\n\nDoes your role as someone who leads a data harmonization initiative differentiate you from other epidemiologists?\nHow has your relationship with other epidemiologists changed after initiating this project and taking on this role?\n\n\n\nReflections on data’s value\n\nHow has the data been used or analyzed?\nDo you track how the data is used?\nIs this tracking formal or informal?\n\n\n\nWhat patterns or trends are apparent based on this tracking?\nIn your view, has the data been used in productive ways?\nIn what ways are people either maximizing or not fully utilizing the data’s full potential?\n\n\n\nCan you tell me about any unexpected or alternative uses of the data?\nWhat made them significant to you?\n\n\n\nWhich skills and competencies do you think researchers need to possess in order to be able to make proper use of the data in their work?\n\n\n\nBased on your experience, what are the main obstacles for the effective and widespread adoption of these skills?\nWhat are some positive factors, or drivers, that can make that prospect more tangible?\n\n\n\nData ownership\n\nWho has rights (in the legal sense or informally) over the information contained in the system, or in related documents and datasets?\nCan you tell me about any conflicts or tensions that emerged relating to expressions of propriety or ownership over data?\n\n\n\nCollecting data\n\nDo projects collect data with future harmonization in mind?\nIf so, how does this affect data collection procedures, and does this play a role in subsequent decision-making?\n\n\n\nCurating data\n\nPlease describe the overall process of getting data into the system and then working with the data.\n\n\n\nPlease tell me about any unexpected or problematic cases that made working with data particularly challenging.\nWhat made these cases unique or challenging?\nHow did you resolve them or work towards a solution or viable outcome?\n\n\n\nAccessing data\n\nDo you consider the system easy to access?\nCan you identify some challenges that pose as barriers to access?\n\n\n\nWho has access to data?\nHow are decisions regarding access rights made?\nCan you tell me about any unnaceptable practices regarding accessing and sharing data?\n\n\n\nUsing data\n\nIf you engage with the data with specific questions in mind, how do these questions emerge?\nWhat role does the data play in shaping the questions and analytical approach?\n\n\n\nIs the system amenable to exploratory or serendipitous modes of discovery?\nPlease tell me about specific examples where you engaged with the data in this way.\n\n\n\nWhat features does the system have to view or export data?\nHow easy is it to view, export or visualize data the data?\nDo you use the tools that are designed to export of visualize data, or do you prefer to use your own tooling?\nWhat are the reasons behind this preference?\n\n\n\nDocumentation\n\nHow is the system documented?\nWho is responsible for creating documentation?\nCan you please tell me about a great example of documentation in your project?\n\n\n\nOverall, do you consider your project’s documentation to be helpful?\nWhy or why not?\n\n\n\nIn your opinion, does the documentation accurately reflect the true nature of the documented data or work practices?\nAre specific things more accurately documented than others?\nPlease tell me why you think some things are more accurately or less accurately documented.\n\n\n\nCan you recall any instances when documentation was updated?\nWhat prompted these updates?\n\n\n\nDo people ever get in touch to ask questions about specific aspects of the data or data curation procedures?\nWhat kinds of questions do they ask?\nWhat kinds of responses are given?\n\n\n\nRelationships with Maelstrom\n\nCan you please concisely describe the role of Maelstrom as part of your project’s overall initiative?\n\n\n\nWhat are the origins of your project’s relationship with Maelstrom?\nHow has this relationship changed over time?\n\n\n\nDoes your project present any unique challenges or require any special attention?\nIf so, please tell me about some unique cases or examples that demonstrate this unique relationship.\n\n\n\nDo you believe that Maelstrom is meeting your project’s needs and enabling it to achieve its goals?\nIn what ways is Maelstrom either satisfying or failing to meet your project’s expectations or needs?\nHow would you change the current system to better suit your project’s needs more effectively?\n\n\n\nDo you engage with Maelstrom’s other partners?\nIf so, what is the nature of these relationships?",
    "crumbs": [
      "Interview Protocol"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "data-sharing\nXXXX.\n\ndata-harmonization\nXXXX.\n\ndata-integration\nXXXX.\n\ncollaboration\nXXXX.\n\ndata-sharing initiative\nXXXX.\nCorresponds with the term “harmonization initiative” in Fortier et al. (2017).\ncatalogue\nXXXX.\nBergeron et al. (2018), Bergeron et al. (2021)\n\n\n\n\nReferences\n\nBergeron, Julie, Dany Doiron, Yannick Marcon, Vincent Ferretti, and Isabel Fortier. 2018. “Fostering Population-Based Cohort Data Discovery: The Maelstrom Research Cataloguing Toolkit.” PLOS ONE 13 (7): e0200926. https://doi.org/10.1371/journal.pone.0200926.\n\n\nBergeron, Julie, Rachel Massicotte, Stephanie Atkinson, Alan Bocking, William Fraser, Isabel Fortier, and the ReACH member cohorts’ principal investigators. 2021. “Cohort Profile: Research Advancement Through Cohort Cataloguing and Harmonization (ReACH).” International Journal of Epidemiology 50 (2): 396–97. https://doi.org/10.1093/ije/dyaa207.\n\n\nFortier, Isabel, Parminder Raina, Edwin R Van den Heuvel, Lauren E Griffith, Camille Craig, Matilda Saliba, Dany Doiron, et al. 2017. “Maelstrom Research Guidelines for Rigorous Retrospective Data Harmonization.” International Journal of Epidemiology 46 (1): 103–5. https://doi.org/10.1093/ije/dyw075.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "data-management.html",
    "href": "data-management.html",
    "title": "Data Management Plan",
    "section": "",
    "text": "Note\n\n\n\nThis document is still a work in progress.\nThis document describes the data that this research will produce and the procedures for curating data throughout the project.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "data-management.html#data-collection",
    "href": "data-management.html#data-collection",
    "title": "Data Management Plan",
    "section": "Data collection",
    "text": "Data collection\nInterviews generate audio, video and textual records. I primarily rely on a SONY ICD-UX560 audio recorder, which contains 4GB of internal storage supplemented with a 32GB microSD card.1 I record using the lossless 16bit 44.1 kHz Linear PCM wav format.\n1 See https://weloty.com/sony-icd-ux560-review and https://weloty.com/using-the-sony-icd-ux560-the-4-how-tos for more information on this audio recording device.With participants’ consent, I also record video using a GoPro Hero 4 Silver action camera equipped with a 64GB microSD card.\nI maintain typed notes before, during and after each interview, which I may edit and re-organize to enhance clarity.\nImmediately after each interview, I copy the data off the recording devices and onto a dedicated project drive. I organize and rename files using a semantic naming scheme and then mirror all files onto physical and a cloud-based backup drives.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "data-management.html#processing-data",
    "href": "data-management.html#processing-data",
    "title": "Data Management Plan",
    "section": "Processing data",
    "text": "Processing data\nI use FFmpeg and Audacity to cut, concatenate, and clean the audio and video files if necessary. I generate transcripts of audio recordings following the transcription protocol.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "data-management.html#storage-and-backups",
    "href": "data-management.html#storage-and-backups",
    "title": "Data Management Plan",
    "section": "Storage and backups",
    "text": "Storage and backups\nI keep all data on a dedicated portable solid state drive which serves as a working directory for all research activities. I mirror the contents of this drive onto an identical secondary backup drive and onto cloud storage administered by the CITF using rsync.\nI maintain this website where I share documentation that supports this project and reflect on the work as it progresses. It is hosted using GitHub Pages and is backed up using Dropbox, however no sensitive research data will pass through these services.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "data-management.html#publishing-and-archiving",
    "href": "data-management.html#publishing-and-archiving",
    "title": "Data Management Plan",
    "section": "Publishing and archiving",
    "text": "Publishing and archiving\nAfter my research is complete, I will make a concerted effort to document all the data I will have collected and generated and deposit them in a digital repository operated by a dedicated team of digital archivists certified to curate and preserve research data in perpetuity.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "case-selection.html",
    "href": "case-selection.html",
    "title": "Case Selection",
    "section": "",
    "text": "Note\n\n\n\nThis document is still a work in progress.",
    "crumbs": [
      "Case Selection"
    ]
  },
  {
    "objectID": "case-selection.html#key-factors",
    "href": "case-selection.html#key-factors",
    "title": "Case Selection",
    "section": "Key Factors",
    "text": "Key Factors\nTo reiterate, this project investigates the social and collaborative apparatus that scaffold data-sharing initiatives in epidemiology. Through analysis of data obtained through interviews with various relevant stakeholders attached to data-sharing initiatives, the project will ascertain the actions taken and challenges experienced to mediate the varied motivations, needs and values of those involved. In effect, the project aims to articulate the collaborative commitments that govern the constitution and maintenance of epidemiological information commons, and to relate these to technological, administrative and epistemic factors.\nIn other words, I aim to make certain under-appreciated social and collaborative commitments that underlie data-sharing initiatives more visible and to draw greater attention to certain sensibilities, attitudes, and apprehensions that are relevant to contemporary discourse on the nature of epidemiological data and ongoing development of information infrastructures designed to support data integration and re-use.\nHere I outline some key factors that will guide the selection of cases so as to ensure that the project meaningfully addressses its goals.\n\n1. Longevity\nInitiatives that have existed for different durations of time will have different capacity to reflect on their practices. Younger projects will not have had as much of a chance to produce any research outcomes, but may be valuable sources for insight on expectations. More established projects will be able to reflect on unexpected challenges they may have experienced.\nIt will be good to have at least one younger project representing an initiative still “in flux”, one or two “legacy” projects (no longer active), and one or two at intermediate stages (extracting data for meaningful analysis, expanding the initiative’s scope, etc).\n\n\n2. Community composition\nThe size and composition of the community, degree of familiarity among its members, and the mechanisms through which connections are managed constitute additional important factors to consider. Communication and decision-making may take different forms when teams are either smaller and locally-concentrated or larger and dispersed. Decision-making may also be significantly impacted by diffferent governance models and degrees of community participation. It would be interesting to identify how leaders are differentiated from other participants, norms and expectations for getting involved in leadership positions, and considerations that are made when making decisions that impact the community.\n\n\n3. Support structures\nData-sharing may be supported by diverse funding models or tech stacks to support the work, which may significantly impact how the work progresses. Comparing sources of support for data-sharing will help me to explore how data-sharing is either integrated into or supplemented as a distinct outgrowth of “normal” science.\nSpecifically, it will be interesting to compare the extent to which projects are left to cobble together their own data-sharing infrastucture, and how this impacts attitudes and norms regarding the curation and nature of research data. I wonder whether lack of government support fosters creative, entrepeneurial, experimental or community-led models, how funding is provided to supporting the development of collaborative research networks, and how these feed back into norms and attitudes regarding the independence of individual research projects and the formation of collectively-maintained information commons.1\n1 There is some precedent for this in the social sciences and humanities, which are fields that open science policies and infrastructures are not really designed to handle. This marginizaliation had contributed to experimentation with community-based governance models (as per the Radical Open Access Collective) and broader community involvement in policy decisions concerning how the rich diversity of social science and humanities data should be curated.I expect a tendency for cases to be supported by limited-term, federally-funded grants, though it might be worth exploring how supplementary funding provided by non-government agencies, including private firms (through MITACS, for instance) and philanthropic organizations (such as the Gates Foundation) impact the work. I would therefore like to included cases funded through these kinds of initiatives in this project.\n\n\n4. Disciplinary trends\nData-sharing is undoubtably impacted by attitudes concerning the nature of data and their roles in scientific knowledge production, and it is therefore necessary to account for different perspectives. Although I am still somewhat unfamiliar with the diversity of thought on such matters in epidemiology, I intuit that much of the open science movement is driven by rather positivist attitude. I would like to include cases that take on alternative approaches to science.\n\n\n5. Historical or contextual factors\nScience is beholden to political trends, which impact ability to obtain funding and collaborate accross borders (e.g. Brexit’s impact on trans-European funding, including initiatives to attract and retain talent). Moreover, certain events, such as the Covid-19 pandemic, trigger responses in the scientific community. Even if these events are not the focus of the research, they must still be accounted for due to their presumed impacts.\n\n\n6. Kinds of data\nThe nature of the data will surely impact how they are shared. In epidemiology specifically, there are ethical limitations on sharing precise patient records. This may be especially salient in studies focusing in health in Indiginous populations, which may involve additional consideration in contexts of data-sharing.2 Moreover, controls on data collection procedures, including limited or controlled scope or decisions to account for specific factors (such as race, which is prevalent in American datasets but largely ignored elsewhere) may significantly impact what can be done with them when integrated at scale.\n2 The Data Governance and Management Toolkit for Self-Governing Indigenous Governments https://indigenousdatatoolkit.ca may be helpful for exploring these concerns, but I am still looking for epidemiologically-oriented resources on such matters.",
    "crumbs": [
      "Case Selection"
    ]
  },
  {
    "objectID": "case-selection.html#selecting-cases",
    "href": "case-selection.html#selecting-cases",
    "title": "Case Selection",
    "section": "Selecting Cases",
    "text": "Selecting Cases\nSince a significant aspect of this work is to compare different approaches to data-sharing that have not yet been systematically articulated, it will be necessary to loosely define the parameters through which each case will be initially characterized. I will rely on structured consultations with the research community to make sense of the data-sharing landscape and select cases accordingly. By consulting with key stakeholders, I will arrive at a consensus about which cases are worth approaching while documenting the rationale behind these selections.\nThe consultation process is meant to ensure that case selection adheres to community will and reasoning, while also ensuring that cases are logistically feasible. I will therefore ask for input from leading members of epidemioligical data-sharing initiatives who are familiar with the goals of the this project, and who are involved with the Maelstrom Project which establishes logistical boundaries around the scope of the project.\n\nFixed cases\nMaelstrom will serve as a “fixed point” that limits the scope of the cases’ breadth, while also ensuring that participants (and myself) have a common frame of reference. Moreover, the practices and values that support Maelstrom’s operations have already been documented to a certain extent by its leaders (cf. Doiron et al. 2017; Fortier et al. 2017; Fortier et al. 2023; Bergeron et al. 2018), by its partners (cf. Doiron et al. 2013; Wey et al. 2021; Bergeron et al. 2021) and by scholars of scientific practice (cf. M. J. Murtagh et al. 2012; Demir and Murtagh 2013; Madeleine J. Murtagh et al. 2016; Tacconelli et al. 2022; Gedeborg et al. 2023). This prior work will serve as valuable resources supporting this project.\nAdditionally, the fact that all cases interact with Maelstrom for their technical infrastructure will greatly simplify the interviews by reducing the “overhead” of having to learn or be told about the technical systems, which may distract from the primary themes I seek to address during interviews.\nCITF will also serve as a fixed case. This is partly for logistical reqasons, since the grant is meant to support the CITF Databank, and this project will align with concurrent research on user experiences pertaining to CITF specifically. At the same time, CITF is relevant to the project’s objectives in its own right, and will contribute meaningful insight in comparison with other cases.",
    "crumbs": [
      "Case Selection"
    ]
  },
  {
    "objectID": "case-selection.html#logistical-constraints-and-sources-of-bias",
    "href": "case-selection.html#logistical-constraints-and-sources-of-bias",
    "title": "Case Selection",
    "section": "Logistical Constraints and Sources of Bias",
    "text": "Logistical Constraints and Sources of Bias\nAfter identifying potential cases, I will reach out to project leaders to invite them to participate. I will prepare a document outlining this project’s objectives and the roles that cases will play in the work. I will also set up a meeting prior to them deciding whether they would like to participate so I can ascertain whether they understand the project and to help determine who may serve as people who can sit for interviews (I expect to hold 12-15 interviews ranging between 60-90 minutes in duration).\nI may prioritize local connections, which provide favourable conditions for holding interviews (i.e., people are more willing to show things that can not be conveyed through a screen, and the pre- and post-interview phases provide meaningful insight). This may introduce bias in that I may obtain more in-depth and nuanced information from local initiatives than those occurring abroad. This can be mitigated by travelling to conduct interviews in person, however the costs of travel may introduce their own biases favouring cases that are easier to reach.",
    "crumbs": [
      "Case Selection"
    ]
  },
  {
    "objectID": "context.html",
    "href": "context.html",
    "title": "Context",
    "section": "",
    "text": "I am Zack Batist — a postdoctoral researcher at McGill University, in the School of Global and Public Health’s Department of Epidemiology, Biostatistics and Occupuational Health. I’m working with David Buckeridge, who leads the Covid-19 Immunity Task Force (CITF) Databank, to investigate data sharing in epidemiological research — with an emphasis on the practical and situated experiences involved in data sharing.\nThe CITF is a “data harmonization” initiative, which entails coordinating a systematic effort to align the information contained in datasets collected by distributed teams of epidemiologists. These efforts to integrate the records collected during various discrete studies are motivated by a desire to establish larger integrated datasets bearing greater statistical power and that facilitate comparison across cohorts. However, epidemiologists must reckon with the diversity of minor variations in data collection procedures, as well as ethico-legal concerns relating to the sharing of individual health records pertaining to human research subjects across numerous institutional and regional jurisdictions.\nAs a scholar of scientific practice, with a primary interest in data-sharing and the formation of information commons, data harmonization represents a fascinating mechanism through which scientists derive technical, administrative, social and epistemic frameworks to enhance the value of their collective endeavours in response to disciplinary needs, warrants, desires and expectations. This study therefore articulates the motivations for doing data harmonization, identifies how value is ascertained, and describes the strategies employed to achieve the desired goals — including perceived and actual challenges, setbacks, opportunities, realizations, and lessons learned.\nThis relates to my previous work that (a) explores tensions that arise when attempting to establish information commons in archaeology, specifically relating to inability to cope with a superficial perception of data’s stability and an intuitive understanding of their situated nature; and that (b) investigates how the open science movement attempts (and fails) to reshape practices relating to data sharing, integration and reuse. I continue in my approach that frames data-sharing — whether it occurs in relatively “closed” curcumstances between close colleagues, or as mediated by open data platforms among strangers — as comprising a series of collaborative commitments that govern who may contribute to and obtain value from the information commons, and in what ways.",
    "crumbs": [
      "Context"
    ]
  },
  {
    "objectID": "ethics-protocol.html",
    "href": "ethics-protocol.html",
    "title": "Ethics Protocol",
    "section": "",
    "text": "Project Title: Articulating epidemiological data harmonization initiatives as practical and collaborative experiences\nSubmitted Materials: zackbatist.info/CITF-Postdoc/irb-docs.pdf\nPrincipal Investigator: Zachary Batist\nProtocol Number: 25-01-057\nSubmitted: 2025-01-30\nApproved:",
    "crumbs": [
      "Ethics Protocol"
    ]
  },
  {
    "objectID": "ethics-protocol.html#recruitment-and-consent",
    "href": "ethics-protocol.html#recruitment-and-consent",
    "title": "Ethics Protocol",
    "section": "Recruitment and consent",
    "text": "Recruitment and consent\nWill this study involve recruitment of human study participants?\n\nYes\nNo\n\nHow are potential study participants identified and/or recruited to the study? Explain how potential participants are identified or introduced to the study, and who will recruit participants. Will the investigator/s require any special permissions or access to the target population e.g. clinic access, patient registries or records, mailing lists, community access?\nThrough consultation with key community stakeholders, the principal investigator will devise a list of prospective projects to serve as cases.1 The principal investigator will then write to the leaders of these projects inviting them to participate in the study. These invitations to project leaders will explain the project’s purpose and scope, and will encourage the recipient to reply with any questions or concerns they may have. If they accept the invitation, the principal investigator will then work with project leaders to devise a list of individuals who may serve as interview candidates based on their roles in the project. The principal investigator will be clear with project leaders that they should not pressure those who work for them to participate in the study, and that individuals’ participation should be treated as separate from their regular duties; if project leaders cannot or will not abide by this condition, their project will be rejected as a prospective case. The principal investigator will then write to the recommended individuals to introduce the study and its objectives and to invite them to participate as research subjects. If these individuals express interest in participating in the study, the principal investigator will schedule a time to sit for an interview. Some interviews may be conducted remotely using internet-based video conferencing software, depending on participants’ availability.\n1 See the case selection protocol for further details.Describe the consent process. If alternate processes for seeking consent are planned (e.g. verbal, online, waiver), please provide a rationale and outline the procedure of obtaining and documenting consent and/or assent, where applicable.\nOnce individuals express their interest in participating, participants will provided with an informed consent document that outlines in more detail the goals of the study, the roles of the participant, how they will be recorded, how data pertaining to them will be retained, and the potential risks and benefits pertaining to their involvement. This document will also describe how participants’ personally identifiable information will be managed and used. Participants will be asked to read and sign the document in order to obtain written informed consent. For interviews that will be held remotely using internet-based video conferencing software, participants will asked to send their signed informed consent documents in PDF format to the principal investigator. At the start of each interview the researcher will reiterate participants’ rights and ask them to orally reaffirm their consent before proceeding.\nIs there a relationship between the study participants and the person obtaining consent and/or the principal investigator/s?\n\nYes\nNo\n\nIf yes, please explain the nature of the relationship, and outline the steps that will be taken to avoid the perception of undue influence.\nOne project that serves as a case in this research is the Covid-19 Immunity Task Force (CITF), which the principal investigator currently serve as postdoctoral researcher. Some of the participants will therefore be his colleagues. The interviews will remain structured and limited in scope, and will not touch on matters relating to other aspects of their work. Moreover, prior to and throughout their involvement as research participants, frank and open discussion will be encouraged regarding collective expectations and to articulate the boundaries between participants’ relationships with the principal investigator as colleagues and as research subjects.\nThe principal investigator will consult with David Buckeridge, who leads the CITF, as one key community stakeholder to help devise a shortlist of projects that may serve as prospective cases.",
    "crumbs": [
      "Ethics Protocol"
    ]
  },
  {
    "objectID": "ethics-protocol.html#risk-benefit-assessment",
    "href": "ethics-protocol.html#risk-benefit-assessment",
    "title": "Ethics Protocol",
    "section": "Risk-benefit assessment",
    "text": "Risk-benefit assessment\nDescribe the foreseeable risks to study participants. What risks are attributable to the research, including cumulative risks? Which risks are participants normally exposed to in the course of their clinical care or in their daily activities as they relate to the research questions/objectives?\nParticipation in this study does not involve any physical, psychological or legal risks. However, the principal investigator will be asking participants to share detailed information about their work practices and work relationships, and public association with their responses may potentially disrupt or complicate their professional reputations. To mitigate against this potential harm, the principal investigator will give participants the option to render their responses confidential.\nWhat procedures are in place to monitor and assess participant safety for the duration of the study?\nPrior to each interview, and as part of the procedure for obtaining informed consent, participants will be asked about whether they want to render their responses confidential. Immediately after each interview, participants will be given an additional opportunity to reflect on their responses, and will be prompted to either confirm or alter their decision regarding whether or not to maintain confidentiality. Furthermore, for participants who have not requested that their responses be treated as confidential immediately before and after the interview, a follow-up email will be sent one week after the interview to reiterate the option to render their responses confidential.\nDescribe the potential benefits of the study for: (1) the study participants; (2) the population under investigation, and (3) the field of research.\nThis study contributes to the development of better epidemiological data-sharing infrastructures by articulating social, collaborative and discursive aspects of data harmonization, and how these factors relate to, overlap with or conflict with technical, institutional and epistemic factors. By explicitly framing data harmonization as a social and collaborative activity, we may devise more effective data-sharing infrastructures that better support the contextualization of data and enhance their value in contexts of data reuse. This work therefore poses new ways to document how epidemiologists mobilize distributed records in the constitution of synthetic knowledge and helps develop practical solutions that enable greater reflexivity. Additionally, this study may directly benefit participants by framing the experiences they address during interviews in ways that they might not have otherwise considered, thereby encouraging greater reflexivity in their own work.",
    "crumbs": [
      "Ethics Protocol"
    ]
  },
  {
    "objectID": "ethics-protocol.html#privacy-and-confidentiality",
    "href": "ethics-protocol.html#privacy-and-confidentiality",
    "title": "Ethics Protocol",
    "section": "Privacy and confidentiality",
    "text": "Privacy and confidentiality\nPlease describe the measures in place for meeting confidentiality obligations. How is information and data safeguarded for the full cycle of the study: i.e. during its collection, use, dissemination, retention, and/or disposal?\nThe specific circumstances that frame each case are significant factors that will shape the findings, and the study will benefit from participants’ consent to associate their identities with their interview responses. However, they may choose to render their interview responses confidential while maintaining their role a research participant. Participants may change their decision regarding whether or not to associate their identities with their interview responses up to one week after the interview, at which point the principal investigator will begin transcribing and analyzing the records pertaining to the interview. Participants will be reminded about this option immediately after the interview and one week following the interview via email.\nThe study engages with a relatively small community, and there is minimal social risk that others may be able to determine the identities of those whose research practices and professional relationships are being documented, even if their responses are rendered confidential. To address this issue, if any single participant from a case decides to render their responses confidential, the responses of all participants pertaining to that case will be rendered confidential as well, and the identify of the project that serves as the case will be obfuscated too.\nIn situations whereby a participant decides to render their responses confidential, or has their responses rendered confidential due to another member of their case deciding to do so, only the principal investigator will have access to records containing un-obfuscated information that may identify them. These un-obfuscated records, which may include audio and video records of interview sessions, as well as unedited transcripts and textual notes containing information that may reveal the participants’ identities, will be kept in secure and encrypted media, and destroyed within five years of concluding the study, which provides sufficient time to revisit the data and produce additional research outputs. However, edited transcripts scrubbed of all information that may identify research participants may be kept, published and archived. If participants consent to maintaining association between their responses and their identities, un-obfuscated records and transcripts may be kept, published and archived.\nThe study is committed to adhering to fundamental data security practices, including those specified in McGill University’s Cloud Directive which regulates the curation of sensitive research data. Physical records will be kept in a locked drawer in secure workspaces, either at McGill University’s School of Public and Global Health or at the principal researcher’s home office. Digital records will be stored on encrypted and password-protected drives and on secure servers approved or managed by McGill University under the Cloud Directive.2\n2 Refer to the data management plan for further details on how information pertaining to this project will be collected, curated and shared.Recordings of remote interviews conducted using internet-based video conferencing software will be made using the software’s built-in recording tools. Only video conferencing software approved by the Cloud Directive will be used. Participants will be instructed to disable their microphones or video cameras prior to initiating recording if they have opted to not be recorded through these media. The researcher will record all media locally and refrain from using any cloud services to store or modify the records which the video conference software may provide.\nIf a contracted cloud/storage service provider or online survey tool is used, provide information on the service provider’s security and privacy policy, location of its servers, data ownership, and what happens to the stored data after the contract is terminated. For more information, please consult the University’s directive.\nThe study uses file-sharing software hosted by the Covid-19 Immunity Task Force at McGill University’s School of Public and Global Health to backup all files maintained for this study. These backups will include files containing information that might reveal participants’ identities. The software used to manage these backups is managed by McGill University and has been approved for storing sensitive research data by the Cloud Directive.\nThe study may use the secure GitLab instance hosted by the surveillance lab within the Clinical and Health Informatics Research Group at McGill University to store and track changes to sensitive research data. This software is managed by McGill University and has been approved for storing sensitive research data by the Cloud Directive.\nThe study maintains a website where the principal investigator shares documentation that supports the study and reflects on the work as it progresses. This is hosted using GitHub Pages and is backed up using Dropbox. No sensitive research data will pass through these services.\nRecordings of remote interviews conducted using internet-based video conferencing software will be made using the software’s built-in recording tools. Only video confering software approved by the Cloud Directive will be used. Participants will be instructed to disable their microphones or video cameras prior to initiating recording if they have opted to not be recorded through these media. The researcher will record all media locally and refrain from using any cloud services to store or modify the records which the video conference software may provide.\nPlease explain any reasonable and foreseeable disclosure requirements (e.g. disclosure to third parties such as government agencies or departments, community partners in research, personnel from an agency that monitors research, research sponsor, the REB/IRB, or regulatory agencies).\nNo disclosure requirements are foreseen.\nIf there are plans for retaining participant and/or study data for future use, please describe the context for its use, requirements for potentially re-contacting study participants and consent, and how the data will be stored and maintained for the long term.\nResearch data will be published in compliance with ethical standards for sharing open social science research data. Records that contain personally-identifying information pertaining to participants who have requested that their responses be rendered confidential and to those who have had their responses rendered confidential due to another member of their case deciding to do so will not be published.\nThe database containing codings, memos and trends deriving from qualitative data analysis will be published only after being scrubbed of all personally-identifying information pertaining to participants who have requested that their responses be rendered confidential and to those who have had their responses rendered confidential due to another member of their case deciding to do so.\nThe principal investigator may follow up with the leaders of the data-sharing initiatives that serve as cases for this project to share the results with them and to present them with constructive feedback deriving from the study’s findings. The principal investigator may also invite select participants to collaborate on a position paper advocating for reforms based on the project’s findings.\nSecondary use of data studies: if the study involves data linkage, please describe the data that will be linked and the likelihood that identifiable information will be created through the linkage.\nThis project does not rely on data deriving from other studies. The data may be reused in related work being undertaken under the same grant and by those who access the openly accessible data after they are published.",
    "crumbs": [
      "Ethics Protocol"
    ]
  },
  {
    "objectID": "ethics-protocol.html#managing-conflicts-of-interest",
    "href": "ethics-protocol.html#managing-conflicts-of-interest",
    "title": "Ethics Protocol",
    "section": "Managing conflicts of interest",
    "text": "Managing conflicts of interest\nConflicts of interest do not imply wrong-doing. It is the responsibility of the investigator to determine if any conflicts apply to any person/s involved in the design and/or conduct of the research study or any member of their immediate family. Disclose all contracts and any conflicts of interest (real, perceived, or potential) relating to this research project. Conflict of interest may also arise with regard to the disclosure of personal health information.\n\nNot applicable. There are no conflicts of interest to disclose.\nYes, there are conflicts of interest to disclose.\n\nIf yes, please describe the conflicts of interest (real, potential, and perceived), and the procedures for managing declared conflicts. Not applicable.",
    "crumbs": [
      "Ethics Protocol"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CITF-Postdoc",
    "section": "",
    "text": "This website serves as a hub for my postdoctoral research at McGill University’s Covid-19 Immunity Task Force Databank.\nThe project is concerned with articulating social, collaborative and discursive aspects of epidemiological data-sharing initiatives, and how they relate to, overlap with or conflict with technical, institutional and epistemic factors.\nThis website hosts a series of preparatory protocols that structure the project, as well as notes about key concepts and reflections on the progress of work. Please keep in mind that this is a continually evolving site and its contents may change as the project goes on. All content is hosted and tracked at github.com/zackbatist/CITF-Postdoc.\nHere’s an overview of what’s on this site:\nContext: My motivations for doing this work and the circumstances that surround the establishment of the project.\nResearch Protocol: Outlines the project’s overall vision and contextualizes it in relation to specific objectives.\nCase Selection: Articulates the parameters that inform how cases are selected.\nEthics Protocol: Specifies ethical considerations, including risks of harm and strategies for mitigating them.\nInterview Protocol: The questions I will be asking research participants, including the rationale for asking them.\nData Management: Procedures that circumscribe collection, management and curation of research data.\nQDA Protocol: The code system, memoing guidelines, and specific QDA procedures.\nGlossary: A series of key terms and their definitions, with reference to the literature and expanded notes about their meanings.\nNotes: Some semi-structured ideas that situate my work in relation to extant literature.\nBlog: Updates and reflections on key events, or general thoughts I wish to share.\nGitHub: A link to the GitHub repository where this website’s files are hosted.\nBib: A biblatex file containing a continually-updated list of sources cited in all documents hosted on this website.\nRSS: RSS feed you can use to subscribe to the blog.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Modified\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJan 23, 2025\n\n\nPotential cases\n\n\ncases, brainstorming\n\n\n\n\nJan 29, 2025\n\n\nMethodology notes\n\n\nreading, general thoughts\n\n\n\n\nJan 28, 2025\n\n\nMaelstrom reading notes\n\n\nreading\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Notes"
    ]
  },
  {
    "objectID": "notes/methodology-notes.html",
    "href": "notes/methodology-notes.html",
    "title": "Methodology notes",
    "section": "",
    "text": "This document is an overview of methodological topics and concerns. It is a place where I think through and justify my methodological decisions, and identify the methods and procedures through which I implement them."
  },
  {
    "objectID": "notes/methodology-notes.html#significant-concepts-and-frameworks",
    "href": "notes/methodology-notes.html#significant-concepts-and-frameworks",
    "title": "Methodology notes",
    "section": "Significant Concepts and Frameworks",
    "text": "Significant Concepts and Frameworks\n\nCase studies\nThese notes describe the features, affordances and limitations of case study research, and articules factors correspoding with variable kinds of case studies. Much of these notes derive from my readings of Stake (2006), which is a more practical guide, and additional theoretical concerns compiled by Charles C. Ragin and Becker (1992) provide additional insight.\nIn case-study research, cases represent discrete instances of a phenomenon that inform the researcher about it. The cases are not the subjects of inquiry, and instead represent unique sets of circumstances that frame or contextualize the phenomenon of interest (Stake 2006: 4-7).\nCases usually share common reference to the overall research themes, but exhibit variations that enable a researcher to capture different outlooks or perspectives on matters of common concern. Drawing from multiple cases thus enables comprehensive coverage of a broad topic that no single case may cover on its own (Stake 2006: 23). In other words, cases are contexts that ascribe particular local flavours to the activities I trace, and which I must consider to account fully for the range of motivations, circumstances and affordances that back decisions to perform activities and to implement them in specific ways.\nMoreover, the power of case study research derives from identifying consistencies that relate cases to each other, while simultaneously highlighting how their unique and distinguishing facets contribute to their representativeness of the underlying phenomon. Case study research therefore plays on the tensions that challenge relationships among cases and the phenomenon that they are being called upon to represent (C. C. Ragin 1999: 1139-1140).\nStake (2006: 4-6) uses the term “quintain” to describe the group, category or phenomenon that bind together a collection of cases. A quintain is an object, phenomenon or condition to be studied – “a target, not a bull’s eye” (Stake 2006: 6). “The quintain is the arena or holding company or umbrella for the cases we will study” (Stake 2006: 6). The quintain is the starting point for multi-case research.\nDraws from the term for a jousting target: https://en.wikipedia.org/wiki/Quintain_(jousting)\n\nMulticase research starts with the quintain. To understand it better, we study some of its single cases — its sites or manifestations. But it is the quintain we seek to understand. We study what is similar and different about the cases in order to understand the quintain better. (Stake 2006: 6)\n\n\nWhen the purpose of a case is to go beyond the case, we call it an “instrumental” case study When the main and enduring interest is in the case itself, we call it “intrinsic” case study (Stake 1988). With multicase study and its strong interest in the quintain, the interest in the cases will be primarily instrumental. (Stake 2006: 8)\n\nIt should be noted that case study research limits my ability to define causal relationships or to derive findings that may be generalized across the whole field of epidemiology. This being said, case study research allows me to articulate the series of inter-woven factors that impact how epidedemiological researchers coordinate and participate in data-sharing initiatives, while explicitly accounting for and drawing from the unique and situational contexts that frame each case.\nAccording to Yin (2014: 16), “a case study is an empirical inquiry that investigates a contemporary phenomenon (the”case”) in depth and within its real-world context, especially when the boundaries between phenomenon and context may not be clearly evident.”\nHe goes on to document some features of a case study: “A casr study inquiry copes with the technically distinctive situation in which there will be many more variables of interest than data points, and as one result relies on multiple sources of evidence, with data needing to converge in a triangulating fashion, and as another result benefits from the prior development of theoretical propositions to guide data collection and analysis.” (Yin 2014: 17)\nYin (2014) is more oriented toward what he refers to as a realist perspective, which he pits against relativist and interpretivist perspectives (used interchangably, it seems), and which I might refer to as constructivist. He characterizes relativist perspectives as “acknowledging multiple realities having multiple meanings, with findings that are observer dependent”. His prioriting of a realist approach corresponds with the analysis by Yazan (2015), who compared Yin with Stake and Merriam. According to Yazan (2015: 137), Yin evades making statements about his epistemic commitments, and is characterized as post-positivist.\nAbbott’s (2004: 22) characaterization of Small-N comparison is very reminiscient of Stake’s (2006) account of the case-quintain dialectic:\n\nSmall-N comparison attempts to combine the advantages of single-case analysis with those of multicase analysis, at the same time trying to avoid the disadantages of each. On the one hand, it retains much information about each case. On the other, it compares the different cases to test arguments in ways that are impossible with a single case. By making these detailed comparisons, it tries to avoid the standard critcism of single-case analysis — that one can’t generalize from a single case — as well as the standard criticism of multicase analysis — that it oversimplifies and changes the meaning of variables by removing them from their context.\n\n\nResearch Design in case study research\nYin (2014) is very concerned with research design in case study research He posits that, in a colloquial sense, “a research design is a logical plan for getting from here to there, where here may be defined as the initial set of questions to be answered, and there is some set of conclusions (answers) about these questions.” (Yin 2014: 28)\nYin distinguishes between a research design and a work plan. A research design deals with a logical problem, whereas a work plan deals with a logistical problem. Seems reminiscient of Brian Cantwell Smith’s distinction between skeletons and outlines.\nYin lists five components of a research design:\n\nA case study’s questions;\nits propositions, if any;\nits unit(s) of analysis;\nthe logic linking the data to the propositions; and\nthe criteria for interpreting the findings.\n\nInterestingly, I have been instinctively following these steps, and am currently hovering somewhere between components 3 and 4, while dipping back to 2 once in a while too.\nThe problem of defining the unit of analysis os salient to me right now. According to Yin (2014: 32), the unit of analysis may change as the project progresses, depending on initial misconceptions (he uses the example of a unit of analysis changing from neighbourhoods to small groups, as contextualized by the socio-geographical entity of the neighbourhood, which is laden with issues of class, race, etc). In my own situation, the unit of analysis may hover between the harmonization initiative, the people, activities or infrastructures that make it work.\nIn the section on criteria for interpreting the findings, Yin emphasizes the role of rival theories, which is akin to a concern with falsifiability as a means of validating truth claims, and which betrays his positivist leanings. This may be compared with Stake’s emphasis on triangulation, which is more concerned with internal cohesiveness. Similarly, Yin cites Corbin and Strauss regarding the role of theory or theoretical propositions in research design, which similarly reveals a concern with rigorous upfront planning and strict adherence to research design as a key aspect of deriving valid findings.\nRegarding generalizability, Yin (2014: 40-41) states that “Rather than thinking about your case as a sample, you should think of it as the opportunity to shed empirical light about some theoretical concepts or principles, not unlike the motive of a laboratory investigator in conceiving of and then conducting a new experiment.” He goes on to state that case studies tend to strive for analytic generalizations that go beyond the specific case that has been studied, and which apply to other concrete situations rather than just abstract theory building.\n\n\nCase selection\nStake recommends between 4-10 cases.\nThere are three main criteria for selecting cases, according to Stake (2006): 23:\n\nIs the case relevant to the quintain?\nDo the cases provide diversity across contexts?\nDo the cases provide good opportunities to learn about complexity and contexts?\n\n\nFor qualitative fieldwork, we will usually draw a purposeive sample of cases. a sample tailored to our study; this will build in variety and create opportunities for intensive study. (Stake 2006: 24)\n\n\n\nSome practical guidance\nStake (2006: 18-22) provides a detailed and realistic overview of common challenges involved in collaborative qualitative research. This could be handy in future work when planning a multicase project involving multiple researchers.\nStake (2006: 29-33) provides guidance on how to plan and conduct interviews in multicase research, including a series of helpful prompts and questions to ask yourself while designing the interview. One thing that stands out is his recommendation that an interview should be more about the interviewee than about the case. It’s necessary to find out about the interviewee to understand their interpretations, but what they reveal about the quintain is more important.\nOn page 34, Stake (2006) also provides some practical tips for documenting and storing data, after Huberman et al. (1994).\nSee Stake (2006) Chapter 5 for a step-by-step overview of a multicase study analysis. The rest of the volume after that includes three very detailed examples from his own work.\n\n\nTriangulation\nTriangulation is a process of gaining assurance. Also sometimes called crystallization.\n“Each important finding needs to have at least three (often more) confirmations and assurances that key meanings are not being overlooked.” (Stake 2006: 33) Triangulation is a process of repetitous data gathering and critical review of what is being said. (Stake 2006: 34)\nWhat needs triangulation? (Stake 2006: 35-36)\n\nIf the description is trivial or beyond question, there is no need to triangulate.\nIf the description is relevant and debatable, there is much need to triangulate.\nIf the data are critical to a main assertion, there is much need to triangulate.\nIf the data are evidence for a controversial finding, there is much need to triangulate.\nIf a statement is clearly a speaker’s interpretation, there is little need to triangulate the quotation but not its content.\n\nStake (2006: 37) cites Denzin (1989) who highlighted several kinds of triangulation, leading to a few advisories:\n\nFind ways to use multiple rather than single observers of the same thing.\nUse second and third perspectives, i.e. the views of teachers, student and parents.\nUse more than one research method on the same thing, i.e. document review and interview.\nCheck carefully to decide how much the total description warrants generalization.\n\nDo your conclusions generalize across other times or places?\nDo your conclusions about the aggregate generalize to individuals?\nDo findings of the interaction among individuals in one group pertain to other groups?\nDo findings of the aggregate of these people generalized to a population?\n\n\n\n\nCross-Case Analysis Procedure\nStake (2006: Chapter 3) lays out a procedure for deriving synthetic findings from data collected across cases. He frames this in terms of a dialectic between cases and quintains. He identifies three tracks (Stake 2006: 46):\n\nTrack 1: Maintains the case findings and the situationality.\nTrack 2: Merges similar findings, maintaining a little of the situationality.\nTrack 3: The most quanitative track, shifts the focus from findings to factors.\n\nAccording to Stake, case reports should be created independently and then brought together by a single individual when working in a collaborative project. In keeping with the case-quintain dialectic, this integration must involve strategically putting the cases aside and bringing them back in to identify convergences and divergences, similarities and differences, normalitities and discrepancies among them.\nThere is some detailed discussion about different kinds of statements, i.e. themes, findings, factors and assertions, but I find this a bit too much detail for me to get at at this point in mymethodological planning. In general though, Stake documents a process whereby an analyst navigates back and forth between the general and the situational, presenting tentativr statements that are shored up, modified or discarded through testing compatability of the evidence across cases.\n\n\nReporting the findings\nStake (2006: Chapter 4) includes a chapter on procedures for reporting the findings, and I may return to this later on once I need to initiative this phase of work. It addresses concerns about how to articulate comparisons, concerns about generalization, and how to handle advocacy based on findings.\n\n\n\nGrounded theory\nThis study follows an abductive qualitative data analysis framework to construct theories founded upon empirical evidence, which relates to, but is distinct from, grounded theory. Grounded theory consists of a series of systematic yet flexible guidelines for deriving theory from data through continuous and reiterative engagement with evidence (Charmaz 2014). My approach draws from what Charmaz (2014: 14-15) calls the “constellation of methods” associated with grounded theory that are helpful for making sense of qualitative data. However, it differs from grounded theory as it is traditionally conceived in that I come to the project with well-defined theoretical goals and did not make a concerted effort to allow the theory to emerge through the analytical process.\nProponents of a more open-ended or improvised approach, as grounded theory was originally applied, argue that researchers should be free to generate theories in accordance with their own creative insights and their intimate engagements with the evidence. We can evaluate the quality of such work in terms of the dialogical commitments between researchers and their subjects, and between researchers and those who read their work (Glaser and Strauss 1967: 230-233).\nOthers view grounded theory more as a means of clarifying and articulating phenomena that lie below the surface of observable social experiences (Strauss and Corbin 1990; Kelle 2005). Proponents of this approach are very concerned with ensuring that concepts, themes and theories are truly represented in and limited by the data, and therefore prioritize adherence to systematic validation criteria to ensure the soundness of their claims.\nAnother view, known as constructivist grounded theory, most resembles my own approach. It recognizes that it is impossible to initiate a project without already holding ideas regarding the phenomena of interest, and that the ways that one ascribes meanings to the data represent already established mindsets or conceptual frameworks (Charmaz 2014). It encourages reflection on the researcher’s standpoint as they pursue an abductive approach rooted in their own preconceptions (Mills, Bonner, and Francis 2006)."
  },
  {
    "objectID": "notes/methodology-notes.html#data-collection",
    "href": "notes/methodology-notes.html#data-collection",
    "title": "Methodology notes",
    "section": "Data Collection",
    "text": "Data Collection\n\nInterviews\n\nstructured, semi-structured\nlunk to more detailed transcription protocol\n\n\nTranscribing\nThis section describes how I transcibe interviews and accounts for the decisions to encode certain things and not others. It goes on to explains the procedures for transcribing spoken dialog into textual formats, including the notation applied to encode idiosyncratic elements of conversational speech.\n\nTranscript notation\nDerived from the transcription protocol applied for the E-CURATORS project.\n\n\nCleaning audio\nTo clean the audio:\n\nI select a clip that is representative of a single source of background noise, and then filter that wavelength throughout the entire audio file.\nAfter selecting the clip, go to Effect &gt;&gt; Noise Reduction and select Get Noise Profile, then press OK.\nClose the noise reduction menu, select the entire range of audio using the keyboard shortcut Command + A.\nThen go back to the noise reduction window (Effect &gt;&gt; Noise Reduction) to apply the filter based on the noise profile identified for the noisy clip.\nExport the modified audio file to the working directory (File &gt;&gt; Export &gt;&gt; Export as .WAV).\nUse ffmpeg to replace the dirty audio track with the clean one:\n\n  ffmpeg -i dirty.mp4 -i clean.wav -c:v copy -map 0:v:0 -map 1:a:0 clean.mp4\n\n\n\n\nField notes\nx\n\n\nRecording video\n\naffordances"
  },
  {
    "objectID": "notes/methodology-notes.html#qda",
    "href": "notes/methodology-notes.html#qda",
    "title": "Methodology notes",
    "section": "QDA",
    "text": "QDA\nMy QDA processes are most influenced by Kathy Charmaz and Johnny Saldaña, as well as the practical experiences instilled during my PhD and while working on E-CURATORS.\n\nCoding\nThese notes are largely derived from my reading of Saldaña (2016), provides a practical overview of what coding entails and specific methods and techniques.\nCoding as component of knowledge construction:\n\nCoding is an intermediate step, “the”critical link” between data collection and their explanation or meaning” (Charmaz (2001), as quoted in Saldaña (2016): 4)\n“coding is usually a mixture of data [summation] and data complication … breaking the data apart in analytically relevant ways in order to ead toward further questions about the data” (Coffey and Atkinson (1996): 29-31, as quoted and edited by Saldaña (2016): 9)\n\nThis relates to the paired notions of decodng when we reflect on a passage to decipher its core meaning, and encoding when we determine its appropriate code and label it (Saldaña 2016: 5).\n\nCoding “generates the bones of your analysis. … [I]ntegration will assemble those bones into a working skeleton” (Charmaz (2014): 113, quoted in Saldaña (2016): 9)\nTo codify is to arrange things in a systematic order, to make something part of a system or classification, to categorize\n\nWhat I sometimes refer to as arranging the code tree\nWhat Saldaña (2016) refers to as categories, I tend to refer to as stubs\n\nCategories are arranged into themes or concepts, which in turn lead to assertions or theories\n\nPre-coding techniques: - Data layout - Separation between lines or paragraphs may hold significant meaning - Putting interviewer words in square brackets or capital letters - Semantic markup - Bold, italics, underline, highlight - Meant to identify “codable moments” worthy of attention (Boyatzis (1998), as referenced in Saldaña (2016): 20) - Relates to Saldaña (2016): 22’s prompt: “what strikes you?” - Preliminary jottings - Tri-column exercise with the text on the left, first impression or preliminary code in the middle, and code on the right, after Liamputtong and Ezzy (2005): 270-273."
  },
  {
    "objectID": "notes/methodology-notes.html#analysis",
    "href": "notes/methodology-notes.html#analysis",
    "title": "Methodology notes",
    "section": "Analysis",
    "text": "Analysis\n\nStatistical methods\n\ncrosstab"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJan 31, 2025\n\n\nWeek notes (2025-W05)\n\n\nweek notes\n\n\n\n\nJan 25, 2025\n\n\nWeek notes (2025-W04)\n\n\nweek notes\n\n\n\n\nJan 24, 2025\n\n\nOn the role of AI in my research\n\n\nAI / LLM, Methods, QDA\n\n\n\n\nJan 18, 2025\n\n\nWeek notes (2025-W03)\n\n\nweek notes\n\n\n\n\nDec 18, 2024\n\n\nTechnical specs for this website\n\n\nwebsite\n\n\n\n\nDec 9, 2024\n\n\nReflection on first team meeting\n\n\nmeeting notes, general thoughts\n\n\n\n\nDec 9, 2024\n\n\nHello World!\n\n\nintroduction, website\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "posts/2024-12-09-hello-world.html",
    "href": "posts/2024-12-09-hello-world.html",
    "title": "Hello World!",
    "section": "",
    "text": "Welcome to the website for my CITF Postdoc! This will serve as a hub for documenting and sharing my work. I decided to do this as a way of managing and sharing always-updated drafts of research protocols with my supervisor and team members, but it is also generally useful for keeping my thoughts organized. I will also use this blog section to write my thoughts as the project progresses."
  },
  {
    "objectID": "posts/2025-01-24-ai-in-my-work.html",
    "href": "posts/2025-01-24-ai-in-my-work.html",
    "title": "On the role of AI in my research",
    "section": "",
    "text": "AI is upon is, and although I would probably be ok if it wasn’t around, I have been (and still am, to a certain extent) tempted to use it in my research. So here I’m gonna articulate some of my thoughts on AI. This isn’t written to convince anyone, or even to convince myself. Just to lay out all my thoughts and take stock of my preconceptions, disapointments, hopes and desires, etc.\nAlso, I’m gonna use AI, LLM and whatever other brand names and marketing buzzwords interchangably here. Draw whatever conclusions you want about that.\nI see AI as being potentially useful in a few productive activities I regularly engage in:\n\nTranscribing spoken words into written text\nTranscription is a significant component of processing interview data, and this can be extremely slow work. It’s a lot easier to edit a transcript produced through a computer algorithm rather than start from scratch. I used trint, otter and other similar tools before all the AI hype, and more recently I’ve been using whisper to transcribe voice notes that I record while I’m waiting for the bus or drifting off to sleep. I’m not really sure how they’re much different, to be honest. Is AI just a rebrand of natural language processing in these contexts? Either way, I will most certainly be using some automatic transcrion tool in my research.\nSummarizing, breaking down and simplifying complex bundles of ideas\nI do a lot of reading, and it can be hard to get through everything on my list. I therefore make lots of compromises and refrain from reading some things because I just can’t make enough time to get through everything. I imagine that AI can help summarize some key points across a whole corpus of articles on my to-read pile, and I may try it out once I have time to figure out the right tooling for the job. However, I do gain a lot of value from the process of reading. Specifically, as a scholar of scientific practice, I’m interested in the language and rhetoric authors use to describe and situate their methods and findings, and I’m not sure if automatic summary tools can capture and communicate this nuance in ways that I want.\nGenerating code snippets for data processing and visualization\nThis is arguably the most productive potential application I can imagine. Specifically, I’m thinking about using this to generate R code that processes and visualizies data according to imagined outcomes. This is directly relevant to a project I’m working on where I’ve already finished the workflows for scraping and processing the data, I have the questions I want to ask of it, but I don’t have the practical know-how to generate the code that will allow me to address them. ggplot is just so dense to me, and stitching together code snippets from stack exchange is a major pain in the ass that produces a horrible abomination of code that would not pass the muster of any rigorous code review. What’s more, those queries to search stack exchange are already half-formed AI prompts! At least an AI would generate some harmony in the code, and I might learn something by having a tidy and consistent template.\n\nI’m more ambivalent and critical about using AI in these contexts where it’s been really hyped:\n\nAny form of writing, including generating emails and abstracts\nFor me, writing is a creative process and a way of unerstanding. It’s a mechanism through which I come to learn about something. The experience of drafting and revising a document is crucial to my research process. This is especially important for honing my position as a scholar at the intersection of various disciplinary communities, who have distinct language and modes of communication.\nQuerying for truth claims\nTo be clear, the idea that knowledge can be total, absolute and disembodied is deeply flawed, and the popular reception of AI as a neutral observer and reporter of nature makes me sad. That being said, I’m still ambivalent about the potential for specialized, home-grown LLMs as means of parsing, sorting through and obtaining greater value from under-used resources. There are patterns in even the messiest and least formal documents we create, and even if we can’t draw information from these documents, LLMs may be useful to help us reflect on the circumstances of their creation. I keep thinking about Shawn Graham’s twitter bots in this context (which were not based on AI, but whatever), which attempted to spit out segments of artificial reports and fieldwork drama, which real archaeologists often related and resonded to. These responses were interesting to me, often expressed as collective fascination, titilation or disgust, and reminiscient of the apprehension one might experience when hearing your own voice played back while standing at the opposite end of a long hallway. Reacting to distortions of your own experience from very different perspectives can be a really powerful reflexive exercise.\nAs a brainstorming tool, or as a rubber duck\nI’ve heard about people using AI chatbots as agents to bounce their ideas off of. Kind of like eliza, but for productive work. While I think it’s intriguing, I don’t know where I’d start. Also, drawing up the prompt and figuring out how to ask the right questions may already be enough to get the ideas flowing. I think I already do this in some ways by drafting little ephemeral notes, usually directed toward a specific person or imaginary audience while anticipating their feedback. It also somehow seems like a perverse way to de-socialize work, and in a world where students and postdocs feel increasingly isolated, I’d much rather solicit and provide feedback among peers. This has been the foundation of some of my most solid friendships and professional partnerships, and should be encouraged.\n\nI also have some previously-unstated opinions in relation to some common critiques of AI:\n\nProcess versus product\nAI seems to be really good at devising formulaic outputs. That is, it’s good at getting things to look like things whose shapes are already well-defined. This can be valuable in various use cases, like writing emails according to a template or translating texts between languages. I could imagine it being really helpful for those who are coming into a field where certain skills are taken for granted, such as learning how to write “proper” academic emails as a student who is not fluent in english. Imagine being up against a deadline for a job application, while also being knee-deep in unpaid work to get your name out there; an LLM could be a godsend. So I don’t discount easy outputs as inherently bad. A standard output for one is a week-long struggle for another, so I think this distinction between product and process is a false and misleading dichotomy.\nBad instructions\nSometimes I find it really hard to believe that people could earnestly follow whatever an AI tells them. But I think we’re getting to the point of urban mythmaking, similar to the older wariness about following your GPS into a lake. There’s a story behind every warning sign, even if it’s a projection of what you think might happen if you disregard it.\n“Intelligence”\nOne weird thing about AI branding is the smushing together of some unified idea of what constitutes “intelligence”. We’ve already been through this with “smart” gadgets, which have always just been ways to capture consumer products under a platforms proprietary injected plastic molds and information protocols. AI is literally just a way to sell you a new version of the smart gadget you threw out last year.\nTruthiness, i.e., AI’s ability to sound authoritative while also making false claims\nI cringe at any retort to a screenshot of AI giving a wrong definition of a thing. Accuracy of responses should come secondary to critique of the notion that all forms of knowledge can be presented in terms of absolute, disembodied and universally truths. For example, when people ridicule AI’s inability to identify the capitols of various nation states, I see missed opportunities to challenge the value of any answer that anyone might provide. True subversion would be to reject or re-frame the question and the simplicity with which it is addressed.\nOne another related note, I see a lot of weird parallels between myths about truth claims made by AI and by practitioners of qualitative data analysis (QDA) — and, as a qualitative researcher, this is obviously a bit unsettling. Specifically, in both QDA and AI, there is no actual attempt to make absolute truth claims, but the focus is rather on attempting to identify and draw out meaningful elements of elicitations in a corpus, and to trace patterns between them. In my current opinion, the key difference lies in positionality. Any QDA researcher who laim that their cases are representative of all experiences will be laughed out of the room. Meanwhile, AI is lauded for the claims made by their creators that it can derive unambiguous and concrete knowledge from inherently situated and biased data sources. Humility is key while contributing to collective knowledge bases, and AI risks changing the dynamic away from deriving greater value from constructive discourse and toward a system where the loudest voice in the room wins.\nClimate change\nAI uses a lot of energy, and is therefore said to be wasteful. However I think there are certain wasteful components of AI. For instance, generative models that spit out a full sentence to wrap around the answer to a question don’t have to do all that extra work. Also, not everyone is reliant on fossil fuels, and the critique that AI is necessarily bad for the environment is laden with a thick American accent (as is the case with so many of the loudest opinions on the internet).\nThat being said, there are enormous problems with resource allocation in AI, and I’m not trying to dismiss all concerns. I see these concerns as relating to the distribution of power and wealth in society at large, and AI is one aspect of this. Sometimes I wonder if comparisons can be made between using AI in selective research contexts and eating a burger or a banana, which each have their own environmental costs. But thinking in this way is a bit of a trap.\n\nI also see that rhetoric, including anxieties about AI, differs in the various communities I participate in:\n\nIn digital-x, where x = {archaeology | humanities | librarianship | whatever}\nThere’s a lot of experimentation going on. Honestly, I don’t know much about it and I tend to scroll past any discussion about AI applications in archaeology that appears in my feed. Part of me sees it as a passing trend, but it could be better framed as a wild frontier, as is the case with many other things in digital archaeology. People are still in the process of taming the landscape, to make it better suit their needs, and maybe I’ll join in once the settlement is established. But I’m not personally motivated by the dynamism of the current landscape, at least in this particular domain.\nEpidemiology, biostats, public health\nI’m still too new in this community to really make sense of this yet. I’ll continue to watch and learn and listen.\nBroader social science and humanities, as well as libraries, archives and museums\nCritique tends to follow broader, more abstract, and more common-sense lines of thought. In my view, much of this does not really account for the material problems and imperfections in which the social sciences and humanities operate. AI is a lifeline for many people in an overworked, overburdened, under-resourced and hyper-competitive environment, and tut-tutting around how other people use AI sometimes comes across as tone-deaf and disrespectful. Some criticisms of AI being used in real, practical circumstances make me second guess critics’ supposed commitments to improving the social experience of research. The fundamental problem is inequitable access to financial and material resources, and AI’s prevalence is a major symptom of, or — depending on your perspective — resolution to that. People’s who recognize this have no choice but to post broader and more abstract criticisms, which come across as somewhat hollow when disconnected from real and tangible experiences.\nSenior faculty\nProbably the most ambivalent of all communities are senior faculty, who want AI to be useful and will test the waters without fully committing. Which is fine and very prudent, and honestly I identify most with this perspective, despite my position as a lowly postdoc.\nGrad students\nI engage with many grad students. I share my workspace with grad students and encounter them constantly in my day to day neighbourhood forays, where I overhear and sometimes participate in conversations about AI. In my new work environment (Epidemiology, Biostatistics and Occupational Health), the grad students who I engage with have a relatively positive perception of AI. They seem to find greater value in the ability to automate complex processes, using it as a black box of sorts, with predictable and abstracted inputs and outputs, which they see as especially helpful for coding. Outside of this space I’m encountering way more diversity of thought on AI, and I’m not quite sure how to group these viewpoints to structure a proper reaction. I think this in fact contributes to the multitude of perspectives, since no one really cares that much one way or the other to really have a strong opinion (though I sense an overwhelming dissatisfaction when it comes to AI in consumer contexts; this post is largely about productive uses of AI in research and pedagogy).\nI was also told about students learning RStats by just having AI generate their code. The person who pointed this out to me related this to the growing misconception that to learn stats you first need to learn how to code. This in turn relates to the sense that to learn how to do RStats, you just need to memorize a series of steps and copy the text from the slides into the IDE. So, in the end, AI reveals the inadequacy of the teaching mechanisms for programming and stats classes, similarly to how AI has revealed the inadequacy of essay-writing as a pedagogical technique.\nOn the other hand, some students are concerned about dulling their skills, or even not being able to take advantage of opportunities to learn new skills, due to the temptation to automate these tasks. Some upper-year PhD students are glad that they were trained in the fundamentals prior to the AI hype wave. This makes me wonder how students are determining what skills they think they need to know how to do on their own and what is worth running through an LLM. Does it basically operate as a bullshit sensor, where you can smell from a distance that the work is just gonna be tedium and irrelevant? Or is it more out of practical necessity, where you’re stretched so thin that you simply have to rely on these tools to achieve anything meaningful, almost as a mechanism for salvaging one’s work from the claws of austerity? In either case, this points to PhD programs’ inadequacy to match students’ needs and desires, and overwhelming amount of administravia or (seemingly) irrelevant work that students are made to do, which get in the way of their true interests.\n\nMaybe I’ll have more to share some other time."
  },
  {
    "objectID": "posts/weeknotes-2025-W04.html",
    "href": "posts/weeknotes-2025-W04.html",
    "title": "Week notes (2025-W04)",
    "section": "",
    "text": "This week was a bit slower than last. I spent much of it finalizing the content for my IRB application, and the rest preparing for a meeting with a key stakeholder relating to my research.\nThe IRB application is more or less done, just waiting on David and the department head to sign off. It was a major opportunity to re-organize my research protocol and related documents. I shuffled some things over into various placeholder sections in my methodology notes, and pushed a revised research protocol to the website.\nYesterday I posted about on the role of AI in my research. It’s mainly meant to lay out my current state of thinking on AI. I’m not fixed to those ideas, and I think there is much more nuance than I do justice to in that post, but putting it on the page helped me consolidate and put aside some scrambled opinions.\nAfter playing around with qc on Sunday, I started to assemble some feedback. I may post a github issue later this week, once I’ve had a chance to consolidate and edit my thoughts.\nI participated in the weekly CITF logistics update, after which I met with Aklil to discuss the overall strategy for her project and strategize on how we might form the focus groups for that work. We’re gonna meet more regularly, just to share some updates on our respective projects which have a lot in common.\nOn Thursday I met with Isabel Fortier, with the intention of discussing data harmonozation initiatives that might serve as potential cases. It was a bit of a tough meeting. During the first 45 minutes I struggled to communicate the purpose of my work, but I think by the end we reached a greater understanding of what this work will entail and the unique perspective it will bring. One surprising outcome that I still need to think through is Isabel’s suggestion that I slow down a bit, immerse myself more in the world of data harmonization. While she is absolutely right that I’ve been rushing through this first month, I do feel pressure to get the project going and to start accumulating data — I felt a similar pressure when starting my PhD, too. So I need to put my eagerness aside so that the data are relevant and of good enough quality. Isabel offered to schedule regular meetings with me, and even to have me work in the Maelstrom office once a week, and I’m extremely grateful for her support! Plus, I’ll get to have lunch with my mom who works in the same hospital complex, one building over :)"
  },
  {
    "objectID": "qda-protocol.html",
    "href": "qda-protocol.html",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "",
    "text": "This document outlines the analytical tools, methods and techniques.",
    "crumbs": [
      "QDA Protocol"
    ]
  }
]