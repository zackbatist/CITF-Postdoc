[
  {
    "objectID": "research-protocol.html",
    "href": "research-protocol.html",
    "title": "Research Protocol",
    "section": "",
    "text": "This study investigates the social, technical, administrative and epistemic factors that scaffold data-sharing initiatives in epidemiological research. It takes to heart the notions that data are media that facilitate communication across different research contexts, that data are created with specific intent, and that data are bounded by the social, practical and material circumstances of their creation. In light of these facts, the study approaches data-sharing as a means of reconciling the varied circumstances of datasets’ creation — both among themselves, and in relation to contexts of reuse. It therefore frames data-sharing as efforts to foster a series of collaborative ties beyond a project’s original indended scope.\nThe project’s goal is to survey what factors are being prioritized by a prominent data-sharing initiative, the rationales behind these decisions, and the relative efficacy of these approaches. More specifically, the project seeks to adress the following research questions:\n\nWhat are the objectives of data-sharing initiatives, how were they established, and what progress has been made to achieve them?\nWhat strategies do data-sharing initiatives employ to ensure they are able to meet their objectives, and how effective are they?\nWhat values underlie these strategies, and can they be linked with effective outcomes, such as the production of harmonized datasets and research deriving therefrom?\n\nThe intent is to ascertain what actions specific strategies entail, the circumstances in which each is adopted, the value that they bring, and the trade-offs involved. In other words, the study will articulate the collaborative experiences that underlie data-sharing activities as a series of converging situated perspectives.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#overview",
    "href": "research-protocol.html#overview",
    "title": "Research Protocol",
    "section": "",
    "text": "This study investigates the social, technical, administrative and epistemic factors that scaffold data-sharing initiatives in epidemiological research. It takes to heart the notions that data are media that facilitate communication across different research contexts, that data are created with specific intent, and that data are bounded by the social, practical and material circumstances of their creation. In light of these facts, the study approaches data-sharing as a means of reconciling the varied circumstances of datasets’ creation — both among themselves, and in relation to contexts of reuse. It therefore frames data-sharing as efforts to foster a series of collaborative ties beyond a project’s original indended scope.\nThe project’s goal is to survey what factors are being prioritized by a prominent data-sharing initiative, the rationales behind these decisions, and the relative efficacy of these approaches. More specifically, the project seeks to adress the following research questions:\n\nWhat are the objectives of data-sharing initiatives, how were they established, and what progress has been made to achieve them?\nWhat strategies do data-sharing initiatives employ to ensure they are able to meet their objectives, and how effective are they?\nWhat values underlie these strategies, and can they be linked with effective outcomes, such as the production of harmonized datasets and research deriving therefrom?\n\nThe intent is to ascertain what actions specific strategies entail, the circumstances in which each is adopted, the value that they bring, and the trade-offs involved. In other words, the study will articulate the collaborative experiences that underlie data-sharing activities as a series of converging situated perspectives.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#approach",
    "href": "research-protocol.html#approach",
    "title": "Research Protocol",
    "section": "Approach",
    "text": "Approach\nThis study is informed by a set of theoretical and methodological frameworks formed within a more interdisciplinary “science studies” tradition, which contribute to a more sociological outlook on science as cultural practice (cf. Pickering 1992). In practical terms, the study documents the social and collaborative experiences involved in various research practices, which ultimately bind the many ways in which scientists do science.\nThe study will specifically focus on how people contribute to and extract from information commons, which comprise both formal documents and mutually-held and information-laden situated experiences. This involves examining the ways in which participation in disciplinary or even more specialized communities of practice fosters mutual understanding about the potential and limitations pertaining to other people’s data; and how this communally-held knowledge is accessed and re-produced. This approach aligns with the situated cognition methodological framework for examining the improvised, contingent and embodied experiences of human activity, including science (cf. Suchman 2007; Knorr Cetina 2001).\nThe situated cognition framework prioritizes subjects’ outlooks, which are contextualized by their prior experiences, and enables scholars to trace how people make sense of their environments and work with the physical and conceptual tools available to them to resolve immediate challenges. Situated cognition therefore lends itself to investigating rather fluid, open-ended and affect-oriented actions, and is geared towards understanding how actors draw from their prior experiences to navigate unique situations.1\n1 I expand on this in an extended note on efforts to frame the plurality of research experiences as a continuum of practice.Situated cognition is especially salient in explorations of how people who are learning new skills learn how to work in new and possibly unfamiliar ways, and in this sense is closely related to Lave and Wenger’s (1991) theory of situated learning (or ‘communities of practice’ approach), which focuses on how individuals acquire professional skills in relation to their social environments. In such situations, situated cognition enables observers to examine how people align their perspectives as work progresses, and to understand better how people’s general outlooks may have changed under the guidance of more experienced mentors. In other words, situated cognition enables researchers of scientific practices to account for discursive aspects of work, including perceived relationships, distinctions or intersections between practices that professional or research communities deem acceptable and unacceptable, and the cultural or community-driven aspects of decisions that underlie particular actions.\nIn taking on this theoretical framework, the study frames epidemiology as a collective endeavour to derive a coherent understanding of population-level health trends, which involves the use of already established knowledge in the validation of newly formed ideas, and which relies on systems designed to carry information obtained with different chains of inference. These systems have both technical and social elements. The technical elements are the means through which information becomes encoded onto information objects so that they may form the basis for further inference. The social elements constitute a series of norms or expectations that facilitate the delegation of roles and responsibilities among agents who contribute their time, effort and accumulated knowledge to communal goals.\nAs such, in constructing the arguments of this study and in carrying out the interviews that ground it, the study will rely upon both realist and constructivist viewpoints. In one sense, the study relies on documenting how people actually act, including the longer-term and collaborative implications that their actions may have on other work occurring throughout the continuum of practice. To accomplish this, the study identifies research activities from the perspective of an outside observer. The study also ascribes meanings to things (such as physical or conceptual tools, or objects that captivate subjects’ interests) in ways that conform to the analyst’s own perspective as an investigator of scientific research practices. On the other hand, a constructionist perspective enables the author to consider how individual agents make components of information systems suit their needs to facilitate communication or interoperability among actors who hold different situated perspectives. By listening to participants’ views about the systems with which they engage, including explanations as to why they act in the ways that they do, I am able to trace the assumptions and taken-for-granted behaviours that frame their perspectives. Moreover, these insights are useful for developing a better understanding of how participants identify with particular disciplinary communities and their perception of their roles within broader collective efforts.\nUltimately, this study is about the social order of scientific research, i.e. the frameworks, mindsets or sets of values that humans adopt to carry out their work in specific ways. Human beings rely upon physical and conceptual apparatus to do this work but, in order to understand how they do science in ways that conform to the epistemic mandates of the scientific enterprise, it is necessary to prioritize attention to human intention, drivers and pressures. The study emphasizes the agency of human drivers — as opposed to tools and procedures — since humans are the ones who (a) identify problems that need to be resolved; (b) imagine, project or predict potential outcomes of various kinds of actions that they may select to resolve the challenges; and (c) learn from prior experiences and change their behaviours accordingly.2 By highlighting how pragmatic actions are conducted in relation to broader social and discursive trends and tendencies, the study considers scholarly practices in terms of potential, certainty and desire from the perspectives of practitioners themselves.\n2 Human and non-human agents are considered on equal footing under the Actor-Network Theory (ANT) framework, which has become very popular since its origins in the late 1980s, but which may not be suitable for this approach. See my extended note on this for further details.To this end, the study follows an abductive qualitative data analysis (QDA) methodology to construct theories founded upon empirical evidence, which relates to, but is distinct from, grounded theory. Grounded theory consists of a series of systematic yet flexible guideline for deriving theory from data through continuous and reiterative engagement with evidence (Charmaz 2014: 1). The approach taken for this study draws from what Charmaz (2014: 14-15) calls the “constellation of methods” associated with grounded theory that are helpful for making sense of qualitative data. However, it differs from grounded theory as it is traditionally conceived in that I came to the project with well defined theoretical goals (as described above) and did not make a concerted effort to allow the theory to emerge through the analytical process. Proponents of a more open-ended or improvised approach, as grounded theory was originally applied, argue that researchers should be free to generate theories in accordance with their own creative insights and their intimate engagements with the evidence. We can evaluate the quality of such work in terms of the dialogical commitments between researchers and their subjects, and between researchers and those who read their work (Glaser and Strauss 1967: 230-233). Others view grounded theory more as a means of clarifying and articulating phenomena that lie below the surface of observable social experiences (Strauss and Corbin 1990; Kelle 2005). Proponents of this approach are very concerned with ensuring that concepts, themes and theories are truly represented in and limited by the data, and therefore prioritize adherence to systematic validation criteria to ensure the soundness of their claims.\nTwo related views, known as constructivist grounded theory and situational analysis, most resemble the approach taken for this study (Charmaz 2014; Clarke, Friese, and Washburn 2018). Both of these methodological frameworks recognize that it is impossible to initiate a project without already holding ideas regarding the phenomena of interest, and that the ways that one ascribes meanings to the data represent already established mindsets or conceptual frameworks. Moreover, they emphasize that knowledge outcomes are co-constituted through partnerships between researchers and study participants, and therefore encourage reflection on the researcher’s standpoint as they pursue an abductive approach rooted in their own preconceptions (Mills, Bonner, and Francis 2006).\nThese approaches rely on a core set of methods of coding and memoing. Coding, which involves defining what data are about in terms that are relevant to the theoretical frameworks that inform the research, entails rendering instances within a text as interpreted abstractions called codes (Charmaz 2014: 43). These methods, which are described in more detail below, are particularly useful for examining the broad assemblage of evidence comprising various kinds of media and spanning multiple participants’ perspectives. The abstraction of specific instances as conceptual codes enables comparisons across documents that would otherwise prove difficult to compare, due either to the analyst’s own preconceptions (drawn from internalized narratives or biases) that might have framed their attitudes, to disproportionate volumes of evidence that might obscure parallels between data sources, or to difficulties experienced when examining different kinds of documents that call for different lenses or perspectives.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#data",
    "href": "research-protocol.html#data",
    "title": "Research Protocol",
    "section": "Data",
    "text": "Data\nThe study draws from semi-structured interviews with 10-15 individuals from a single case: the Covid-19 Immunity Task Force (CITF) Databank. The CITF was an initiative whose mandate was to catalyze, support, fund and harmonize knowledge on SARS-CoV-2 immunity for federal, provincial, and territorial decision-makers in their efforts to protect Canadians and minimize the impact of the COVID-19 pandemic. The CITF Databank provides continued support to the research community by centralizing research data provided by CITF-funded studies and by making the data accessible for extended research.\nThis study draws from interviews with individuals who lead, support or participate in the CITF Databank’s operations, including profesional researchers, research trainees and administrative and technical support staff. Interviews are oriented by the study’s goal to document processes of reconciling different stakeholders’ interests as they converge in the formation of a common data resource. Specifically, interviews focus on participant’s motives, the challenges they experience, how they envision success and failure, their perceptions of their own roles and the roles of other team members and stakeholders, the values that inform their decisions, how the technological apparatus they set up enables them to realize their goals and values, and ways in which they believe the work could be improved. See the interview protocol for further details on the questions I ask and how participants’ responses contribute to the project’s findings, as well as logistical considerations.\nThe case was selected partially out of convenience, but this does not discount the prospective analytical value that it affords. As a community-led initiative, the CITF Databank presents an opportunity to investigate how epidemiologists balance the values deriving from their own epistemic culture with the challenges of coordinating collective efforts; it has established an explicit governance structure, which provides me with terms and concepts to examine through a critical lens; it comprises people working in a multidude of roles, many of whom are locally available; and it is a venue where multiple relevant epistemic conflicts and challenges manifest themselves, which are rich sources from which a qualitative researcher may ascertain competing and complementary value regimes.\nMoreover, the case provides adequate breadth of perspective, which is of greater concern than sample size under the constructivist grounded theory and situational analysis methodological frameworks. The goal is to articulate the series of inter-woven factors that impact how epidemiological researchers coordinate and participate in data-sharing initiatives while explicitly accounting for and drawing from the unique and situational contexts that circumscribe their perspectives, not to define causal relationships or to derive findings that may be generalized across the whole field of epidemiology. As such, statistical representativeness is not an objective of this research.\nThe estimated number of individuals to be interviewed reflects the practical constraints that this work affords, namely the time-consuming nature of transcribing interviews and conducting qualitative data analysis. My experience leading projects of similar scale will enable me to collect, process and analyze such a comprehensive corpus in the relatively short period in which funding has been allotted. The number of participants therefore represents a careful balance between a meaningful sample size and the amount of work required to collect, process and analyze the data within the project’s one-year timeframe.\nInterviews are transcribed, and transcripts are edited for optimal application using qualitative data analysis software. Secure and locally-hosted automated speech recognition software are used to create preliminary transcripts, which are then be manually edited. All data are collected and curated in full compliance with the ethics protocol and in accordance with the data management plan.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#methods",
    "href": "research-protocol.html#methods",
    "title": "Research Protocol",
    "section": "Methods",
    "text": "Methods\nThe study implements qualitative data analysis (QDA) methods to highlight collaborative aspects data-sharing in epidemiology, as elicited in the corpus of transcribed interviews. QDA involves encoding the primary sources of evidence in ways that enable a researcher to draw cohesive theoretical accounts or explanations. This is done by tagging segments of a document (such as an interview transcript) using codes, and by embedding open-ended interpretive memos directlly alongside the data. Through these methods, a researcher is able to articulate theories based on empirical evidence that reflect the informants’ diverse experiences.\nCoding — which involves defining what specific elicitations are about in terms that are relevant to the theoretical frameworks that inform the research — entails rendering instances within a text as interpreted abstractions called codes (Charmaz 2014, 43). Codes can exist at various levels of abstraction. For instance, an analyst may apply descriptive codes to characterize literal facets of an instance within a text, and theoretical codes to represent more interpretive concepts that correspond with aspects of particular theoretical frameworks. This project primarly implements an “open” coding protocol, which entails creating codes on the fly when prompted by encounters with demonstrative instances in the text. As new codes are generated in this manner, they are situated within a code system that affords greater taxonomic structure to encoded observations, thereby facilitating more effective queries. Coding in this manner involves synthesis of concepts that speak to the analyst’s understanding of the phenomena of interest, while forcing the analyst to remain receptive to limits imposed by what is actually contained in the corpus. In other words, coding involves applying a precise language to segments of transcribed interviews that serve to bridge the gap between what participants said and the theoretical frameworks that the analyst applies to explore them as epistemic activities, interfaces and values (cf. Charmaz 2014; Saldaña 2011, 95–98).\nMemoing entails more open-ended exploration and reflection upon latent ideas in order to crystallize them into new avenues to pursue (Charmaz 2014, 72). Constructing memos is a relatively flexible way of engaging with data and serves as fertile ground for honing new ideas. Memoing is especially crucial while articulating sensitizing concepts, which Charmaz (2003, 259) refers to as the “points of departure from which to study the data”. Memoing allows the researcher to take initial notions that lack specification of well-defined attributes, and gradually refine them into more cohesive, definitive concepts (Blumer 1954, 7; Bowen 2006). Exploring the main features, relationships or arrangements that underlie a superficial view of a sensitizing concept through memoing helps the analyst to identify what kinds of things they need to locate in the data in order to gain a full understanding of the phenomena of interest. Memoing is also very important in the process of drawing out more coherent meaning from coded data (cf. Charmaz 2014, 181, 290–93). By creating memos pertaining to the intersections of various codes and drawing comparisons across similarly coded instances, an analyst is able to form more robust and generalizable arguments about the phenomena of interest and relate them to alternative perspectives expressed by others.\nThroughout the analysis, I follow the approach that Nicolini (2009) and Maryl et al. (2020, para. 30) advocate, who suggest “zooming in to a granular study of particular research activities and operations and zooming out to considering broader sociotechnical and cultural factors.” This involves “magnifying or blowing up the details of practice, switching theoretical lenses, and selective re-positioning so that certain aspects are fore-grounded and others are temporarily sent to the background” (Nicolini 2009, 1412). This approach leverages the fact that participants’ actions and interactions are informed by the broader communities of practice, which imbues them with professional norms, expectations and value regimes. When asking about their particular work experiences in the context of the case, I also ask them to relate their own unique circumstances the field as a whole, thereby enhancing the potential for this study to speak of some generalized phenomenon.\nThis work is performed using computer assisted qualitative data analysis software that enables analysts to retrieve segments of interview transcripts and identify patterned distributions of codes from across the entire corpus. Querying the dataset in this way enables the analyst to articulate elaborated accounts of specific kinds of activities, decisions, values and sentiments that cut across various informants’ perspectives.\nStatistical methods play a limited role in this study. Basic summary statistics (e.g. cross tabulation) may be used to represent the distribution of codings across individual interviews or ranges of interviews, which may help to identify trends and associations as they pertain to their limited scopes. This may be used to support theory-building but may not be used to infer generalizable causal relationships.\nSee the QDA protocol for further details on the code system and memoing guidelines, as well as specific QDA procedures, and my methodology notes that situate this’s project’s methods in relation to alternative approaches.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#outcomes",
    "href": "research-protocol.html#outcomes",
    "title": "Research Protocol",
    "section": "Outcomes",
    "text": "Outcomes\nThis project will produce insights regarding the practical benefits and challenges involved in epidemiological data-sharing. It will identify how relevant stakeholders actually engage with the systems that scaffold data-sharing initiatives, which may differ from modelled behaviours specified in aspirational plans and procedural documents. In effect, by articulating how these systems succeed or fail to account for their users practical needs and disciplinary values, this study will provide constructive feedback that will inform their further development.\nThe study will produce three peer-reviewed articles. One of these will be published in a journal concerned with scientific practice or research data management (e.g. Computer Supported Cooperative Work; Scientific Data); Another will be published in a journal dedicated to advancing research practices in epidemiology (e.g. Epidemiologic Methods) A third paper will comprise a more practical set of guidelines deriving from the findings, as either an editorial or as a “10 simple rules” style article. I will also present this work at conferences and workshops, as opportunities arise.\nMoreover, this work is intended to be constructive, and it is therefore necessary to ensure that the findings may be put to practical use so as to enhance and improve data-sharing initiatives. I will therefore draft policy briefs and reach out to leading stakeholders at the helm of major data-sharing infrastuctures and policy frameworks (such as the ongoing revisions to federal open science policies) so that the findings may directly inform efforts to improve these systems.\nAdditionally, I will promote this work publicly. This will entail publishing an article in The Conversation, a website with a broad public following and which specializes in showcasing specialized research for the general public. I may also share the findings on podcasts about open science and science policy with broad interdisciplinary appeal. Moreover, owing to my broad pan-disciplinary background, I am plugged into a diverse network of scholars, and my work will reach a very broad audience through active my engagement on social media and posting regular updates on my professional blog.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "research-protocol.html#ethics",
    "href": "research-protocol.html#ethics",
    "title": "Research Protocol",
    "section": "Ethics",
    "text": "Ethics\nThe study will be conducted according to ethical principles stated in the Declaration of Helsinki (2013). Ethics approval will be obtained before initiating the study. Consent forms will take into consideration the well-being, free-will and respect of the participants, including respect of privacy. The practices undertaken to ensure adherence to these principles are described in the ethics protocol.",
    "crumbs": [
      "Research Protocol"
    ]
  },
  {
    "objectID": "posts/weeknotes-2025-W25.html",
    "href": "posts/weeknotes-2025-W25.html",
    "title": "Week notes (2025-W25)",
    "section": "",
    "text": "Scheduled interviews\nFinalized some interview guides\nRevisions to a paper deriving from my dissertation"
  },
  {
    "objectID": "posts/weeknotes-2025-W23.html",
    "href": "posts/weeknotes-2025-W23.html",
    "title": "Week notes (2025-W23)",
    "section": "",
    "text": "Revising ethics amendment\nWorked on data governance paper\nBackground reading\nSome other archaeology-related things"
  },
  {
    "objectID": "posts/weeknotes-2025-W21.html",
    "href": "posts/weeknotes-2025-W21.html",
    "title": "Week notes (2025-W21)",
    "section": "",
    "text": "Still waiting for approval on the ethics ammendment\nPresented for the surveillance lab and had a great discussion\nWorked on interview planning and memo-writing\nRead about other Covid-19 data-sharing and collaborative initiatives"
  },
  {
    "objectID": "posts/weeknotes-2025-W19.html",
    "href": "posts/weeknotes-2025-W19.html",
    "title": "Week notes (2025-W19)",
    "section": "",
    "text": "Worked on the case file and interview guides\nWrote a memo about the pivot\nPosted on my personal blog"
  },
  {
    "objectID": "posts/weeknotes-2025-W17.html",
    "href": "posts/weeknotes-2025-W17.html",
    "title": "Week notes (2025-W17)",
    "section": "",
    "text": "Started revising my research protocol for a potential pivot\nParticipated in the Surveillance Lab journal club"
  },
  {
    "objectID": "posts/weeknotes-2025-W15.html",
    "href": "posts/weeknotes-2025-W15.html",
    "title": "Week notes (2025-W15)",
    "section": "",
    "text": "Did my first interview, it went really well! We scheduled a follow-up for next month. I finished transcribing and processing the interview. I also started doing some open / initial coding and memoing.\nI made a significant update to the QDA protocol and data management plan.\nI developed another case file and made more concrete progress on planning subsequent interviews. I also obtained informed consent from additional potential participants.\nOver the weekend and outside work hours I made significant progress on a revisions to a paper deriving from my thesis. I will be ready to submit the revisions this week."
  },
  {
    "objectID": "posts/weeknotes-2025-W13.html",
    "href": "posts/weeknotes-2025-W13.html",
    "title": "Week notes (2025-W13)",
    "section": "",
    "text": "Confirmed date and time for my first interview\nWorked on the interview guide\nContinued reading about the harmonization software ecosystem, and about cataloguing\nContinued reading about situational analysis and qualitative research methods in general\nStarted working on my MCHI presentation\nMade significant progress on the data governance paper\nWrote up some notes based on a rich discussion with colleagues\nCreated a private repo / submodule for my private files after nearly losing that data\nPCI-archaeo successfully recruited a recommender\nWorked on the fuzzy-concrete revisions over the weekend\nHad my biweekly update with David\nParticipated in the Surveillance Lab journal club"
  },
  {
    "objectID": "posts/weeknotes-2025-W11.html",
    "href": "posts/weeknotes-2025-W11.html",
    "title": "Week notes (2025-W11)",
    "section": "",
    "text": "Updated David on my progress\nFinally settled on situational analysis as my methodological framework\n\nContinuing to take more notes on key readings\n\nStarted producing situational maps and memos\nStarted looking into ways of mining the CITF papers and research summaries (as briefly discussed during the weekly update)\n\n\n\nWorked a bit on the “locationg creative agency” paper during my off-time\nInitiated and discussed a community-driven survey of AI applications in archaeology\n\nIt’s now on the backburner due to lack of time and motivation from the SSLA members, and becoming aware of similar initiatives being undertaken by others\n\nWorked on ReACH data governance paper a bit\nFinishing some translation work during my off-time\nMigrating my personal/professional blog from wordpress to a quarto-based system\n\nAlso looking through some ancient posts (circa 2011-2014) and reformatting those to be migrated too"
  },
  {
    "objectID": "posts/weeknotes-2025-W09.html",
    "href": "posts/weeknotes-2025-W09.html",
    "title": "Week notes (2025-W09)",
    "section": "",
    "text": "This week I continued to work on my methodology notes, specifically going through the coding methods. I also spent some time updating an article from my dissertation that was rejected from a journal again, and I’m trying to see how it can be published elsewhere (including as a preprint on my own website).\nI experimented with the mechanism for keeping private data contained in a git submodule stored on a secure server, and fiddled with the quarto configuration too. I updated the tech specs blog post with some notes.\nI also started developing more comprehensive case files, outlining and reflecting on the first case and how I will approach it.\nBeing a bit ill this week, in combination with the paper rejection and another grant application, made it a bit of a slog. It will be better next week, right?\nOn the bright side, I started a collaboration with one of Isabel’s former students, trying to revive the momentum on an abandoned paper. We really clicked and I foresee some good work coming out of this in the future, maybe even beyond this one initiaive."
  },
  {
    "objectID": "posts/weeknotes-2025-W07.html",
    "href": "posts/weeknotes-2025-W07.html",
    "title": "Week notes (2025-W07)",
    "section": "",
    "text": "This week I continued my methodological readings. Today I got some requests for minor modifications to my REB protocol, which will be rather painless to implement.\nI also attended the Public Health day, a student conference where grad students presented their practicum research. I really enjoyed engaging with the students at the poster session, I actually learned quite a lot.\nNext week I’ll try to round off my methodology readings in anticipation of my REB application being approved by the end of this month."
  },
  {
    "objectID": "posts/weeknotes-2025-W05.html",
    "href": "posts/weeknotes-2025-W05.html",
    "title": "Week notes (2025-W05)",
    "section": "",
    "text": "The first part of my week largely involved reading and taking notes on methodological texts. I focused on case study design and qualitative coding techniques. I plan to continue my methodology readings on coding techniques, memoing, interview methods and systematic note-taking, as well filling in gaps in my understanding of grounded theory and related debates. These readings are especially useful in this planning stage, but also serve to fill time while I wait for my IRB approval.\nOn that note, I finally submitted my ethics application on Thursday. I expect an expedited review based on the low-risk nature of the work. I posted the materials I submitted on the ethics protocol page.\nI had my biweekly meeting with David, and he was very encouraging.\nI met with Isabel Fortier again yesterday and we came up a list of six projects that may serve as potential cases. We will discuss them in greater depth next week.\nI finally sent some feedback on qc.\nNext week I also need to fulfill a few commitments not as related to the postdoc: I need to work on a peer-review I had committed to, continue assemmbling constructive feedback for qc, and continue going through the DOAJ for the diamond.open-archaeo initiative."
  },
  {
    "objectID": "posts/weeknotes-2025-W03.html",
    "href": "posts/weeknotes-2025-W03.html",
    "title": "Week notes (2025-W03)",
    "section": "",
    "text": "I’m trying out a new way to track and communicate my progress on this project. Every week I’ll write a post to track the work I’ve been doing and reflect on my activities. I’ll try to maintain this document throughout the week, tidy it up and post it here on Friday afternoons. However the specific process will probably vary as it grows into the rest of my workflow.\nI’m purposefully trying to not tie this into the personal knowledge management trend. It’s for me and my own purposes, and I don’t want to get bogged down with the unabashed managerial phoniness that belies the PKM phenomenon.\nAnyway, I didn’t take notes on my work this past week, but here’s an overview of what I’ve done from memory:\nContinued to take notes on readings produced by the Maelstrom Project and its partners.\nContinued to investigate and maintain notes on potential cases.\nSet up a placeholder document for notes on methodological concerns.\nMet with David for our bi-weekly check-in (meeting notes are private, at least for now). I was also given access to the lab’s private git server but haven’t really had much of a chance to explore what it’s being used for or devise my own plans to make use of it.\nWorked extensively on the ethics protocol. David and I went back and forth deciding on whether this was necessary, given how the project is based on his grant which already has IRB approval. But it’s better to play it safe than sorry, especially when it’s necessary to obtain informed consent. So to this end, I revised the research protocol and responses to the ethics form, and I also drafted an informed consent document. I’ll share all these things once the whole package is put together (probably in a week or two), but my responses to the ethics form already appears on the ethics protocol page.\nI simplified the way private documents are handled in the quarto project and git respository. May still need to so some fiddling, especially for draft blog posts and notes.\nI started drafting a blog post about the potential use of AI/LLMs in my research. Stay tuned.\nOn a related note, I watched this recent video about the use of LLMs in qualitative data analysis, which did not prompt me to draft the post but which is well-timed, nevertheless.\nI worked a bit more on the data management plan, which prompted me to think more about which QDA software I’ll use. I started filling in a university form to use cloud services provided by MaxQDA, but stumbled upon qualitative-coding (abbreviated as qc), an open source CLI-based QDA system. It represents a very innovative approach to QDA rooted in computational thinking and plain text social science, while also remaining true to the core tenets and purpose of QDA, which make it unique and difficult to design software for. If this is the sort of thing that appeals to you, I highly recommend you read the docs.\nI had a bit of trouble installing it and getting it running, but I met remotely with Chris Proctor, who develops the tool through his work at the Computational Literacies Lab, based in the Department of Learning and Instruction at University at Buffalo (SUNY). He helped me resolve some issues, gave me a guided tour of the system and we just talked about the overall state of qualitative data analysis and its tooling. I don’t really have the capacity right now to post everything he showed me but I will definitely be posting about my experiences tinkering around with qc in the coming weeks.\nSimilarly, I asked on Mastodon about whether there are any tools that might support automatic generation of transcripts that include support for specialized notation. A few linguists and conversation analysis scholars responded with recommendations to use GailBot, and with discussion about the tool’s capabilities and limitations. I requested access to the software but haven’t heard back from the dev team yet. I also created a thread on the whisper github repo, which I now realize it a bit of a naive place to put it, and it hasn’t yet gotten any responses.\nI attended a talk from the epidemiology seminar series, which went wayyyy over my head.\nDid my usual amount of engagement on masotodon, I suppose. And I continued to make new friends in the department too :)"
  },
  {
    "objectID": "posts/2025-01-24-ai-in-my-work.html",
    "href": "posts/2025-01-24-ai-in-my-work.html",
    "title": "On the role of AI in my research",
    "section": "",
    "text": "AI is upon is, and although I would probably be ok if it wasn’t around, I have been (and still am, to a certain extent) tempted to use it in my research. So here I’m gonna articulate some of my thoughts on AI. This isn’t written to convince anyone, or even to convince myself. Just to lay out all my thoughts and take stock of my preconceptions, disapointments, hopes and desires, etc.\nAlso, I’m gonna use AI, LLM and whatever other brand names and marketing buzzwords interchangably here. Draw whatever conclusions you want about that.\nI see AI as being potentially useful in a few productive activities I regularly engage in:\n\nTranscribing spoken words into written text\nTranscription is a significant component of processing interview data, and this can be extremely slow work. It’s a lot easier to edit a transcript produced through a computer algorithm rather than start from scratch. I used trint, otter and other similar tools before all the AI hype, and more recently I’ve been using whisper to transcribe voice notes that I record while I’m waiting for the bus or drifting off to sleep. I’m not really sure how they’re much different, to be honest. Is AI just a rebrand of natural language processing in these contexts? Either way, I will most certainly be using some automatic transcrion tool in my research.\nSummarizing, breaking down and simplifying complex bundles of ideas\nI do a lot of reading, and it can be hard to get through everything on my list. I therefore make lots of compromises and refrain from reading some things because I just can’t make enough time to get through everything. I imagine that AI can help summarize some key points across a whole corpus of articles on my to-read pile, and I may try it out once I have time to figure out the right tooling for the job. However, I do gain a lot of value from the process of reading. Specifically, as a scholar of scientific practice, I’m interested in the language and rhetoric authors use to describe and situate their methods and findings, and I’m not sure if automatic summary tools can capture and communicate this nuance in ways that I want.\nGenerating code snippets for data processing and visualization\nThis is arguably the most productive potential application I can imagine. Specifically, I’m thinking about using this to generate R code that processes and visualizies data according to imagined outcomes. This is directly relevant to a project I’m working on where I’ve already finished the workflows for scraping and processing the data, I have the questions I want to ask of it, but I don’t have the practical know-how to generate the code that will allow me to address them. ggplot is just so dense to me, and stitching together code snippets from stack exchange is a major pain in the ass that produces a horrible abomination of code that would not pass the muster of any rigorous code review. What’s more, those queries to search stack exchange are already half-formed AI prompts! At least an AI would generate some harmony in the code, and I might learn something by having a tidy and consistent template.\n\nI’m more ambivalent and critical about using AI in these contexts where it’s been really hyped:\n\nAny form of writing, including generating emails and abstracts\nFor me, writing is a creative process and a way of unerstanding. It’s a mechanism through which I come to learn about something. The experience of drafting and revising a document is crucial to my research process. This is especially important for honing my position as a scholar at the intersection of various disciplinary communities, who have distinct language and modes of communication.\nQuerying for truth claims\nTo be clear, the idea that knowledge can be total, absolute and disembodied is deeply flawed, and the popular reception of AI as a neutral observer and reporter of nature makes me sad. That being said, I’m still ambivalent about the potential for specialized, home-grown LLMs as means of parsing, sorting through and obtaining greater value from under-used resources. There are patterns in even the messiest and least formal documents we create, and even if we can’t draw information from these documents, LLMs may be useful to help us reflect on the circumstances of their creation. I keep thinking about Shawn Graham’s twitter bots in this context (which were not based on AI, but whatever), which attempted to spit out segments of artificial reports and fieldwork drama, which real archaeologists often related and resonded to. These responses were interesting to me, often expressed as collective fascination, titilation or disgust, and reminiscient of the apprehension one might experience when hearing your own voice played back while standing at the opposite end of a long hallway. Reacting to distortions of your own experience from very different perspectives can be a really powerful reflexive exercise.\nAs a brainstorming tool, or as a rubber duck\nI’ve heard about people using AI chatbots as agents to bounce their ideas off of. Kind of like eliza, but for productive work. While I think it’s intriguing, I don’t know where I’d start. Also, drawing up the prompt and figuring out how to ask the right questions may already be enough to get the ideas flowing. I think I already do this in some ways by drafting little ephemeral notes, usually directed toward a specific person or imaginary audience while anticipating their feedback. It also somehow seems like a perverse way to de-socialize work, and in a world where students and postdocs feel increasingly isolated, I’d much rather solicit and provide feedback among peers. This has been the foundation of some of my most solid friendships and professional partnerships, and should be encouraged.\n\nI also have some previously-unstated opinions in relation to some common critiques of AI:\n\nProcess versus product\nAI seems to be really good at devising formulaic outputs. That is, it’s good at getting things to look like things whose shapes are already well-defined. This can be valuable in various use cases, like writing emails according to a template or translating texts between languages. I could imagine it being really helpful for those who are coming into a field where certain skills are taken for granted, such as learning how to write “proper” academic emails as a student who is not fluent in english. Imagine being up against a deadline for a job application, while also being knee-deep in unpaid work to get your name out there; an LLM could be a godsend. So I don’t discount easy outputs as inherently bad. A standard output for one is a week-long struggle for another, so I think this distinction between product and process is a false and misleading dichotomy.\nBad instructions\nSometimes I find it really hard to believe that people could earnestly follow whatever an AI tells them. But I think we’re getting to the point of urban mythmaking, similar to the older wariness about following your GPS into a lake. There’s a story behind every warning sign, even if it’s a projection of what you think might happen if you disregard it.\n“Intelligence”\nOne weird thing about AI branding is the smushing together of some unified idea of what constitutes “intelligence”. We’ve already been through this with “smart” gadgets, which have always just been ways to capture consumer products under a platforms proprietary injected plastic molds and information protocols. AI is literally just a way to sell you a new version of the smart gadget you threw out last year.\nTruthiness, i.e., AI’s ability to sound authoritative while also making false claims\nI cringe at any retort to a screenshot of AI giving a wrong definition of a thing. Accuracy of responses should come secondary to critique of the notion that all forms of knowledge can be presented in terms of absolute, disembodied and universally truths. For example, when people ridicule AI’s inability to identify the capitols of various nation states, I see missed opportunities to challenge the value of any answer that anyone might provide. True subversion would be to reject or re-frame the question and the simplicity with which it is addressed.\nOne another related note, I see a lot of weird parallels between myths about truth claims made by AI and by practitioners of qualitative data analysis (QDA) — and, as a qualitative researcher, this is obviously a bit unsettling. Specifically, in both QDA and AI, there is no actual attempt to make absolute truth claims, but the focus is rather on attempting to identify and draw out meaningful elements of elicitations in a corpus, and to trace patterns between them. In my current opinion, the key difference lies in positionality. Any QDA researcher who laim that their cases are representative of all experiences will be laughed out of the room. Meanwhile, AI is lauded for the claims made by their creators that it can derive unambiguous and concrete knowledge from inherently situated and biased data sources. Humility is key while contributing to collective knowledge bases, and AI risks changing the dynamic away from deriving greater value from constructive discourse and toward a system where the loudest voice in the room wins.\nClimate change\nAI uses a lot of energy, and is therefore said to be wasteful. However I think there are certain wasteful components of AI. For instance, generative models that spit out a full sentence to wrap around the answer to a question don’t have to do all that extra work. Also, not everyone is reliant on fossil fuels, and the critique that AI is necessarily bad for the environment is laden with a thick American accent (as is the case with so many of the loudest opinions on the internet).\nThat being said, there are enormous problems with resource allocation in AI, and I’m not trying to dismiss all concerns. I see these concerns as relating to the distribution of power and wealth in society at large, and AI is one aspect of this. Sometimes I wonder if comparisons can be made between using AI in selective research contexts and eating a burger or a banana, which each have their own environmental costs. But thinking in this way is a bit of a trap.\n\nI also see that rhetoric, including anxieties about AI, differs in the various communities I participate in:\n\nIn digital-x, where x = {archaeology | humanities | librarianship | whatever}\nThere’s a lot of experimentation going on. Honestly, I don’t know much about it and I tend to scroll past any discussion about AI applications in archaeology that appears in my feed. Part of me sees it as a passing trend, but it could be better framed as a wild frontier, as is the case with many other things in digital archaeology. People are still in the process of taming the landscape, to make it better suit their needs, and maybe I’ll join in once the settlement is established. But I’m not personally motivated by the dynamism of the current landscape, at least in this particular domain.\nEpidemiology, biostats, public health\nI’m still too new in this community to really make sense of this yet. I’ll continue to watch and learn and listen.\nBroader social science and humanities, as well as libraries, archives and museums\nCritique tends to follow broader, more abstract, and more common-sense lines of thought. In my view, much of this does not really account for the material problems and imperfections in which the social sciences and humanities operate. AI is a lifeline for many people in an overworked, overburdened, under-resourced and hyper-competitive environment, and tut-tutting around how other people use AI sometimes comes across as tone-deaf and disrespectful. Some criticisms of AI being used in real, practical circumstances make me second guess critics’ supposed commitments to improving the social experience of research. The fundamental problem is inequitable access to financial and material resources, and AI’s prevalence is a major symptom of, or — depending on your perspective — resolution to that. People’s who recognize this have no choice but to post broader and more abstract criticisms, which come across as somewhat hollow when disconnected from real and tangible experiences.\nSenior faculty\nProbably the most ambivalent of all communities are senior faculty, who want AI to be useful and will test the waters without fully committing. Which is fine and very prudent, and honestly I identify most with this perspective, despite my position as a lowly postdoc.\nGrad students\nI engage with many grad students. I share my workspace with grad students and encounter them constantly in my day to day neighbourhood forays, where I overhear and sometimes participate in conversations about AI. In my new work environment (Epidemiology, Biostatistics and Occupational Health), the grad students who I engage with have a relatively positive perception of AI. They seem to find greater value in the ability to automate complex processes, using it as a black box of sorts, with predictable and abstracted inputs and outputs, which they see as especially helpful for coding. Outside of this space I’m encountering way more diversity of thought on AI, and I’m not quite sure how to group these viewpoints to structure a proper reaction. I think this in fact contributes to the multitude of perspectives, since no one really cares that much one way or the other to really have a strong opinion (though I sense an overwhelming dissatisfaction when it comes to AI in consumer contexts; this post is largely about productive uses of AI in research and pedagogy).\nI was also told about students learning RStats by just having AI generate their code. The person who pointed this out to me related this to the growing misconception that to learn stats you first need to learn how to code. This in turn relates to the sense that to learn how to do RStats, you just need to memorize a series of steps and copy the text from the slides into the IDE. So, in the end, AI reveals the inadequacy of the teaching mechanisms for programming and stats classes, similarly to how AI has revealed the inadequacy of essay-writing as a pedagogical technique.\nOn the other hand, some students are concerned about dulling their skills, or even not being able to take advantage of opportunities to learn new skills, due to the temptation to automate these tasks. Some upper-year PhD students are glad that they were trained in the fundamentals prior to the AI hype wave. This makes me wonder how students are determining what skills they think they need to know how to do on their own and what is worth running through an LLM. Does it basically operate as a bullshit sensor, where you can smell from a distance that the work is just gonna be tedium and irrelevant? Or is it more out of practical necessity, where you’re stretched so thin that you simply have to rely on these tools to achieve anything meaningful, almost as a mechanism for salvaging one’s work from the claws of austerity? In either case, this points to PhD programs’ inadequacy to match students’ needs and desires, and overwhelming amount of administravia or (seemingly) irrelevant work that students are made to do, which get in the way of their true interests.\n\nMaybe I’ll have more to share some other time."
  },
  {
    "objectID": "posts/2024-12-09-first-team-meeting.html",
    "href": "posts/2024-12-09-first-team-meeting.html",
    "title": "Reflection on first team meeting",
    "section": "",
    "text": "Last week (2024/12/04) I finally met with David Buckeridge, Tanya Murphy and Aklil Noza in person. The meeting was meant to convey my vision for the project to the whole team, to align perspectives, and to articulate how this will actually work in practice.\nThe gist is that I will be investigating the role of social and cultural factors in data-sharing initiatives such as CITF and other Maelstrom-affiliated projects, and how these relate to, overlap with, or conflict with technical and institutional/administrative factors. To be clear, these are all very important aspects of data-sharing, but we generally recognized that the social and cultural aspects are under-explored relative to their impact.\nWe briefly talked about how we will go about selecting cases, and I emphasized the importance of strategic case selection. This also involves carefully articulating the project’s goals so that the cases will complement them. We agreed that the dataset will likely comprise between 12-15 interviews of around 60-90 minutes in length with representatives from 4-5 cases (one of them being CITF), in addition to representatives of the Maelstrom team. Maelstrom will serve as a “fixed point” that limits the scope of the cases’ breadth and ensures that participants have a common frame of reference. It also potentially allows me to “offload” or “consolidate” reference to technical and administrative aspects of data-sharing through targeted interviews with Maelstrom personnel, instead of dealing with those things with the representatives for each case.\nWe discussed timelines and overlap with Aklil’s work, which will be more concerned with focus groups with CITF databank users. There is definitely overlap with the emphasis of my own work and we will coordinate data collection to enhance the potential for analytical alignment.\nAfter the meeting I chatted with Tanya and Aklil who helped familiarize me with the bigger-picture theoretical discourse and tensions in epidemiology. Much of it seemed familiar since these concerns are common across disciplines, but I still need to read more to concretize my understanding. Tanya recommended I read the “Baby Rothman” which is a condensed version of a very long-lived textbook in this field, among a few other papers she sent me.\nOverall, this meeting got me really excited about this project :)"
  },
  {
    "objectID": "notes/tricks-of-the-trade.html",
    "href": "notes/tricks-of-the-trade.html",
    "title": "Tricks of the Trade",
    "section": "",
    "text": "Becker (1998: 44):\n\nMaking activities the starting point focuses analysis on the situation the activity occurs in, and on all the connections what you are studying has with all the other things around it, with its context. Activities only make sense when you know what they are a response to, what phenomena provide inputs and necessary conditions for the thing you want to understand. If the character of the person or object is so immutable as to resist all situ ational variation, so unchanging that no input is a necessary condition for it to do whatever it does, that will be an empirical finding rather than a theoretical commitment made before the research began and thus immune to disproof by evidence.\n\n\nBecker (1998: 50):\n\nObjects, then, are congealed social agreements, or rather, con gealed moments in the history of people acting together. The analytic trick consists of seeing in the physical object before you all the traces of how it got that way, of who did what so that this thing should now exist as it does. I often act out the exercise in class: picking up any ob ject that comes to hand—a student’s notebook, my shoe, a pencil—and tracking down all the earlier decisions and activities that produced this thing sitting before us.\n\n\nBecker (1998: 60-61):\n\nAssume that whatever you want to study has, not causes, but a history, a story, a narrative, a “first this happened, then that happened, and then the other happened, and it ended up like this.” On this view, we understand the occurrence of events by learning the steps in the process by which they came to happen, rather than by learning the conditions that made their existence necessary.\n\nContinued on Becker (1998: 63):\n\nOne shows “why something was or became necessary;” the other shows “how something was or became possible.”\n\n\nBecker (1998: 64):\n\nThe procedures used in studies based on this logic depend on comparing cells in a table (the cells containing cases that embody dif ferent combinations of the variables being studied), and the comparis ons will not withstand standard criticisms unless they rest on large numbers of cases. The results of such studies consist of probabilistic statements about the relations between the variables, statements whose subjects are not people or organizations doing things but rather variables having an effect or producing some measurable degree of variation in the dependent variable. The conclusions of such a study — that the cases studied have a particular probability of showing this or that result — are intended to apply to an entire universe of simil ar cases.\n\nContinued on Becker (1998: 65-66):\n\nAnother approach, which Ragin (1987) describes as multiple and conjunctival, has a quite different image of causality. It recognizes that causes are typically not really independent, each making its independ ent contribution to some vector that produces the overall outcome in a dependent variable. It suggests instead that causes are only effective when they operate in concert. … In multiplicative images of causality all the elements have to be there to play their part in the conjunction or combination of relevant causal circum stances. If any one of them is missing, no matter how big or important the others are, the answer will still be zero—the effect we are inter ested in will not be produced. The “multiple” part of the argument says that more than one such combination can produce the result we’re interested in. In these causal images, there’s more than one way to get there. Which combination works in a case depends on context: historically and socially specific conditions that vary from case to case. … I’ll conclude this chapter by referring to another kind of image, our image of the social scientist at work. A standard image in contemporary social science is of the brave scientist submitting his (I use the masculine pronoun because the imagery is so macho) theories to a crucial empirical test and casting them aside when they don’t measure up, when it isn’t possible to reject the null hypothesis. Ragin draws a contrasting picture that I find quite compelling, of a social scientist engaged in “a rich dialogue” of data and evidence, a picture that looks a lot more like the scientific activity Blumer envisioned: pondering the possibilities gained from deep familiarity with some aspect of the world, systematizing those ideas in relation to kinds of information one might gather, checking the ideas in the light of that information, dealing with the inevitable discrepancies between what was expected and what was found by rethinking the possibilities and getting more data, and so on, in a version of Kuhn’s image of the development of science as a whole.\n\n\nThis also brings to mind the brief story from the closing paragraphs of Becker (1998: 218-219), which I read yesterday:\n\nI once heard a Zen scholar tell the following story. He was from Japan and did not speak English well, although well enough. He impressed me, at first, with his high good humor. Despite problems of language, he smiled and laughed a lot, and his pleasure in talking to us was infectious. Then he told the following story, which he meant, I think, as an explanation of the Zen idea of satori or enlightenment. It is as good a parable as I know for what it means to have gotten a social science way of thinking into your bones. Since I have never been able to find anyone who could tell me where this story has been written down, I have to reproduce it from memory.\n\nIn the middle of the ocean, there is a special place, which is a Dragon Gate. It has this wonderful property: any fish that swims through it immediately turns into a dragon. However, the Dragon Gate does not look any different from any other part of the ocean. So you can never find it by looking for it. The only way to know where it is is to notice that the fish who swim through it become dragons. However, when a fish swims through the Dragon Gate, and becomes a dragon, it doesn’t look any different. It just looks like the same fish it was before. So you can’t tell where the Dragon Gate is by looking closely to find just where the change takes place. Furthermore, when fish swim through the Dragon Gate and become dragons, they don’t feel any different, so they don’t know that they have changed into dragons. They just are dragons from then on.\n\nYou could be a dragon.\n\n\n\n\n\nReferences\n\nBecker, Howard Saul. 1998. Tricks of the Trade: How to Think about Your Research While You’re Doing It. https://press.uchicago.edu/ucp/books/book/chicago/T/bo3683418.html.\n\n\nRagin, Charles C. 1987. The Comparative Method: Moving Beyond Qualitative and Quantitative Strategies. Berkeley: University of California Press."
  },
  {
    "objectID": "notes/theoretical-sampling.html",
    "href": "notes/theoretical-sampling.html",
    "title": "Theoretical sampling",
    "section": "",
    "text": "According to Morse and Clark (2019), theoretical sampling can be done with a few targets in mind:\n\n\nConcerned with understanding “what is going on” with the phenomenon. Get a feel for the phenomenon. Primarily relies on retrospective accounts, rather than through perspectives of people who are in the midst of it.\nThere are a few main ways to collect such broad-level understanding: - Sampling to obtain an initial understanding - Understand the phenomenon of interest and the characteristics/attributes involved - Allows me to recognize the phenomenon in other or adject settings - This is generally depends on a biased and convenience sample\n\nSampling for process\n\nAt this time, you begin working with abstract categories and must transition to theoretical sampling\nOnce the main storyline of the narrative has veen identified, the researcher can go back to the participants to gain a better understanding of their experiences in the midst of the phenomenon\nThis will enable the researcher to develop an understanding of processes that occur within the phenomenon\nIn other words, it’s about the how, as opposed to the what in sampling to obtain initial understanding.\n\nSampling for maximum variation\n\nTwo kinds of variation: diversity contextual variation, and diversity/demographic variation\nDiversity contextual variation examines different experiences that people face within the target population\nDiversity/demographic variation prompts the researcher to identify “missing” viewpoints, in proportion to what would be required to gain a full understanding of the phenomenon of interest\n\nMales and females?\nClass?\nInsiders and outsiders?\nVarious cultural and ethnic groups?\nOther characteristics that are relevant to your topic?\n\n\nSampling to chase important leads\n\nQualitative research is unique in that it allows participants to provide info about people other than themselves\n“As the participants sort their world for you, they are telling how you can organize your data, and where to go for further information.” (Morse and Clark 2019: 156)\n\n\n\n\n\n\nSampling to locate and create concepts\n\nOccurs through the process of coding and building categories\nAs you code, you develop a sense of what is important to your participants\nThese perceptions acrue into categories, and may be divided into smaller perspectives or subcategories (I may use the term “facet”) as you build more data and write comprehensive memos\n\nSampling for identifying trajectories\n\nIt can be helpful to identify trajectories, transitions, causes and consequences\nProcess coding helps keep things action-orienteed\n\nSampling for pattern identification\n\nReturn to the interviews and see where in the story each concept or category appears\n\nSampling for confirmation\n\nSee if the concept is consistent across several participants\nOne account may support another, even if they are not exactly the same\n\nSampling for negative cases\n\nNegative cases are those accounts that contradict or do not support the other data\nMay arise due to the fact that something was omitted, so it may be necessary to go back to the source and verify through additional questions\n\n\n\n\n\n\nSampling to link concepts\nSampling to obtain certainty\nSampling for saturation\nSampling for verification\n\n\n\n\n\nSampling to determine equivalency across contexts\nSampling for theoretical group interviews"
  },
  {
    "objectID": "notes/theoretical-sampling.html#grasping-the-phenomenon",
    "href": "notes/theoretical-sampling.html#grasping-the-phenomenon",
    "title": "Theoretical sampling",
    "section": "",
    "text": "Concerned with understanding “what is going on” with the phenomenon. Get a feel for the phenomenon. Primarily relies on retrospective accounts, rather than through perspectives of people who are in the midst of it.\nThere are a few main ways to collect such broad-level understanding: - Sampling to obtain an initial understanding - Understand the phenomenon of interest and the characteristics/attributes involved - Allows me to recognize the phenomenon in other or adject settings - This is generally depends on a biased and convenience sample\n\nSampling for process\n\nAt this time, you begin working with abstract categories and must transition to theoretical sampling\nOnce the main storyline of the narrative has veen identified, the researcher can go back to the participants to gain a better understanding of their experiences in the midst of the phenomenon\nThis will enable the researcher to develop an understanding of processes that occur within the phenomenon\nIn other words, it’s about the how, as opposed to the what in sampling to obtain initial understanding.\n\nSampling for maximum variation\n\nTwo kinds of variation: diversity contextual variation, and diversity/demographic variation\nDiversity contextual variation examines different experiences that people face within the target population\nDiversity/demographic variation prompts the researcher to identify “missing” viewpoints, in proportion to what would be required to gain a full understanding of the phenomenon of interest\n\nMales and females?\nClass?\nInsiders and outsiders?\nVarious cultural and ethnic groups?\nOther characteristics that are relevant to your topic?\n\n\nSampling to chase important leads\n\nQualitative research is unique in that it allows participants to provide info about people other than themselves\n“As the participants sort their world for you, they are telling how you can organize your data, and where to go for further information.” (Morse and Clark 2019: 156)"
  },
  {
    "objectID": "notes/theoretical-sampling.html#identifying-the-theoretical-components",
    "href": "notes/theoretical-sampling.html#identifying-the-theoretical-components",
    "title": "Theoretical sampling",
    "section": "",
    "text": "Sampling to locate and create concepts\n\nOccurs through the process of coding and building categories\nAs you code, you develop a sense of what is important to your participants\nThese perceptions acrue into categories, and may be divided into smaller perspectives or subcategories (I may use the term “facet”) as you build more data and write comprehensive memos\n\nSampling for identifying trajectories\n\nIt can be helpful to identify trajectories, transitions, causes and consequences\nProcess coding helps keep things action-orienteed\n\nSampling for pattern identification\n\nReturn to the interviews and see where in the story each concept or category appears\n\nSampling for confirmation\n\nSee if the concept is consistent across several participants\nOne account may support another, even if they are not exactly the same\n\nSampling for negative cases\n\nNegative cases are those accounts that contradict or do not support the other data\nMay arise due to the fact that something was omitted, so it may be necessary to go back to the source and verify through additional questions"
  },
  {
    "objectID": "notes/theoretical-sampling.html#constructing-theory",
    "href": "notes/theoretical-sampling.html#constructing-theory",
    "title": "Theoretical sampling",
    "section": "",
    "text": "Sampling to link concepts\nSampling to obtain certainty\nSampling for saturation\nSampling for verification"
  },
  {
    "objectID": "notes/theoretical-sampling.html#sampling-for-theory-development",
    "href": "notes/theoretical-sampling.html#sampling-for-theory-development",
    "title": "Theoretical sampling",
    "section": "",
    "text": "Sampling to determine equivalency across contexts\nSampling for theoretical group interviews"
  },
  {
    "objectID": "notes/theoretical-sampling.html#when-do-you-cease-sampling",
    "href": "notes/theoretical-sampling.html#when-do-you-cease-sampling",
    "title": "Theoretical sampling",
    "section": "When do you cease sampling?",
    "text": "When do you cease sampling?\n\nOne key characteristic of a completed grounded theory is theoretical transference\n\nWell-developed theoretical concepts and theories should be applicable beyond the context and situation in which they were first identified\nMorse and Clark (2019: 163) argues that qualitative research can indeed be generalizable, and that the onus is on the researcher to illustrative the application of the concept or theory in multiple situations\n\nQuantifiable demographics are merely proxies for more complex webs of characteristics; qualitative research moves beyond these concrete descriptors into keveks of abstraction that transcend the particular, and the abstract findings may therefore be applied to other situuations framed by similar characteristics\n\n\n\n\nUrquhart (2019: 94-101) and Vaast and Urquhart (2017) also addresses theoretical sampling and how it can conctrtibute to theory-building. However much of their emphasis seems to be on how theoretical sampling was applied in the original conception of grounded theory, in a retrospective sense. It is somewhat notable that the data sources that they rely on (social media posts and their linked content) are relatively well-structured, the “distance” from a core backbone dataset could be rather easily ascertained, and the dataset can be readily re-samples numerous times."
  },
  {
    "objectID": "notes/potential-cases.html",
    "href": "notes/potential-cases.html",
    "title": "Potential cases",
    "section": "",
    "text": "Isabel Fortier came up with a shortlist based on consultations to help determine which Maelstrom partner projects may serve as potential cases. We then met on 2025-02-04, when, among other topics, we discussed the shortlist.\nSee the case selection protocol for further details on the parameters that guide how cases are to be determined."
  },
  {
    "objectID": "notes/potential-cases.html#general-notes",
    "href": "notes/potential-cases.html#general-notes",
    "title": "Potential cases",
    "section": "",
    "text": "Isabel Fortier came up with a shortlist based on consultations to help determine which Maelstrom partner projects may serve as potential cases. We then met on 2025-02-04, when, among other topics, we discussed the shortlist.\nSee the case selection protocol for further details on the parameters that guide how cases are to be determined."
  },
  {
    "objectID": "notes/potential-cases.html#possible-candidates",
    "href": "notes/potential-cases.html#possible-candidates",
    "title": "Potential cases",
    "section": "Possible candidates",
    "text": "Possible candidates\n\nCITF\n\n\nReACH\nTitle: Stress and Anxiety During Pregnancy and Length of Gestation Harmonization Initiative + ReACH\nContact: Julie Bergeron \nReason: A small project and a very complex infrastructure-oriented network coordinated by Julie Bergeron. Both projects are finalized.\nNotes: Julie Bergeron was a PhD student at Maelstrom, and Isabel says that she is probably the most knowledgeable person regarding data harmonization who I will encounter during my research. She worked on her dissertation project (Stress and Anxiety During Pregnancy and Length of Gestation Harmonization Initiative) while also leading work on ReACH, and her dissertation essentially served as one of a few pilot projects under the aegis of ReACH. ReACH was led by Isabel as its PI.\nBoth projects are complete, but Isabel thinks that Julie Bergeron will be able to share some significant insights on this past work. My instinct is that this presents an opportunity to explore how/whether harmonization is suited for doctoral training, the role of pilot projects within broader initiatives, and impact that closeness to the method of data harmonization might have.\nLinks:\n\nhttps://www.maelstrom-research.org/network/reach\n\n\n\nCAPACIty\nTtle: Capacity: Building CAPACIty for pediatric diabetes research and quality improvement across Canada\nAcronym: CAnadian PediAtric diabetes ConsortIum (CAPACIty)\nContact: Shazhan Amed \nReason: Across Canada, focus on clinical data. A lot of work already achieved, and harmonization will start soon. Will use a federated approach for statistical analysis.\nNotes: A network of 15 childhood diabetes centers from across Canada. Went through four years of administrative work, and is now just starting harmonization after finally going through all theose hurldes, despite being very eager to get into the data work early on. Despite these challenges, Isabel thinks they will be very receptive to participating in the study.\n\n\nSHAIRE\nTitle: SHAIRE: Scientific & Health Data Assets In Respiratory Research\nContact: Sanja Stanojevic \nReason: New project just starting, very interesting and dynamic.\nNotes: Extremely new, just got funding very recently. I don’t know that much, to be honest. Could potentially provide redundant value to my study as Capacity, but need to find out more.\n\n\nRespiratory study\nTitle: High-dose docosahexaenoic acid for bronchopulmonary dysplasia severity in very preterm infants: A collaborative individual participant data meta-analysis\nContact: Isabelle Marc \nReason: Very specific and small project, very clinical.\nNotes: A very small project, harmonizing two datasets. I asked if this scale of work is common and Isabel says that it is, so it’s not an outlier.\nLinks:\n\nhttps://maelstrom-research.org/study/n3pi-hi\n\n\n\nMORGAM\nTitle: MOnica Risk, Genetics, Archiving and Monograph\nContact: Kari Kuulasmaa \nReason: European, started several years ago.\nNotes: Older project, ended around 10 years ago, the PI is retired. Might be valuable for looking at broader impact and potential offshoots after the work has been completed.\nLinks:\n\nhttps://www.maelstrom-research.org/network/morgam\n\n\n\nLifecycle\nVery improvized approach to data harmonization, did a lot of “manual” work. According to Isabel, Julie Bergeron will be able to tell me more.\nLinks:\n\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC7387322"
  },
  {
    "objectID": "notes/potential-cases.html#recommended-but-no-longer-considered",
    "href": "notes/potential-cases.html#recommended-but-no-longer-considered",
    "title": "Potential cases",
    "section": "Recommended but no longer considered",
    "text": "Recommended but no longer considered\n\nCanPath\nTitle: CanPath, formerly called the Canadian Partnership for Tomorrow’s Health\nContact: Noah Frank \nReason: One of the most important harmonization initiative, infrastructure oriented, long-term started more than 10 years ago.\nNotes: Other contacts include John Mclaughlin and Paula Robson.\nMy instinct is to look at how things change over the course of a very long and institutional initiative, especially across discrete phases marked by leadership transitions. But the history here is so vast and I will probably not get much of it through a series of interviews.\n\n\nMindmap\nPromoting mental well-being and healthy ageing in cities. Seems very similar to CanPath in terms of scope and governance, and I would likely face similar challenges."
  },
  {
    "objectID": "notes/maelstrom-readings.html",
    "href": "notes/maelstrom-readings.html",
    "title": "Maelstrom reading notes",
    "section": "",
    "text": "Initial overview of data harmonization procedures, using the Healthy Obesity Project (HOP) as an illustrative case.\nOutlines the technical apparatus, especially for DataShield, but also broadly describes the discursive process of arriving at a DataSchema that is both functional and flexible.\n\nThis description is quite broad and abstracy, seems somewhat ideal and aspirational.\n\nDescribes reliance on international standards, such as the International Labour Organization’s International Standard Classification of Occupations.\n\nIt seems like these are used as black boxes that encapsulate a series of tensions which epidemiologists are unconcerned with; in effect, they simplify the need for stretching the collaborative ties even further than they are already extended, they represent matters out of scope for deeper discursive engagement.\n\nIt is notable that they emphasize that it’s easy to set up and use DataShield and Maelstorm toolkits independently of university IT and that it can be run using RStudio installed on a basic laptop.\n\nMaybe look into the historical context (2013) and the evolving role of university IT in software selection.\n\nThe conclusion states that the HOP project was successful in its harmonization efforts, but does not go as far as to state that it produced meaningful findings as a result of harmonization.\n\nI may take some time to find and read studies that used these data to see what’s what.\nThis seems like the main one: https://doi.org/10.1186/1472-6823-14-9, but these other papers may or not not also be relevant:\n\nhttps://doi.org/10.1016/j.smhl.2021.100263\nhttps://doi.org/10.1007/s10654-014-9977-1\nhttps://doi.org/10.1530/EJE-14-0540\nhttps://doi.org/10.1007/S13679-020-00375-0\nhttps://doi.org/10.1093/eurpub/ckac061"
  },
  {
    "objectID": "notes/maelstrom-readings.html#doiron2013",
    "href": "notes/maelstrom-readings.html#doiron2013",
    "title": "Maelstrom reading notes",
    "section": "",
    "text": "Initial overview of data harmonization procedures, using the Healthy Obesity Project (HOP) as an illustrative case.\nOutlines the technical apparatus, especially for DataShield, but also broadly describes the discursive process of arriving at a DataSchema that is both functional and flexible.\n\nThis description is quite broad and abstracy, seems somewhat ideal and aspirational.\n\nDescribes reliance on international standards, such as the International Labour Organization’s International Standard Classification of Occupations.\n\nIt seems like these are used as black boxes that encapsulate a series of tensions which epidemiologists are unconcerned with; in effect, they simplify the need for stretching the collaborative ties even further than they are already extended, they represent matters out of scope for deeper discursive engagement.\n\nIt is notable that they emphasize that it’s easy to set up and use DataShield and Maelstorm toolkits independently of university IT and that it can be run using RStudio installed on a basic laptop.\n\nMaybe look into the historical context (2013) and the evolving role of university IT in software selection.\n\nThe conclusion states that the HOP project was successful in its harmonization efforts, but does not go as far as to state that it produced meaningful findings as a result of harmonization.\n\nI may take some time to find and read studies that used these data to see what’s what.\nThis seems like the main one: https://doi.org/10.1186/1472-6823-14-9, but these other papers may or not not also be relevant:\n\nhttps://doi.org/10.1016/j.smhl.2021.100263\nhttps://doi.org/10.1007/s10654-014-9977-1\nhttps://doi.org/10.1530/EJE-14-0540\nhttps://doi.org/10.1007/S13679-020-00375-0\nhttps://doi.org/10.1093/eurpub/ckac061"
  },
  {
    "objectID": "notes/maelstrom-readings.html#doiron2017",
    "href": "notes/maelstrom-readings.html#doiron2017",
    "title": "Maelstrom reading notes",
    "section": "Doiron et al. (2017)",
    "text": "Doiron et al. (2017)\n\nAn overview of the key software that facilitates data harmonization practices under Maelstrom, also briefly touched upon in Doiron et al. (2013).\nPage 1373 refers to graphical and programmatic interfaces and assumes certain roles and tasks associated with each.\nBriefly describes its use by the Canadian Longitudinal Study on Aging (CLSA), the Canadian Partnership for Tomorrow Project (CPTP) and InterConnect, primarily by describing the range and quantity of data that these systems manage in each case.\n\n\nOpal provides a centralized web-based data management system allowing study coordinators and data managers to securely import/export a variety of data types (e.g. text, nu merical, geolocation, images, videos) and formats (e.g. SPSS, CSV) using a point-and-click interface. Opal then converts, stores and displays these data under a standar dized model.\n\n\nMica is used to create websites and metadata portals for individual epidemiological studies or multi-study consor tia, with a specific focus on supporting observational co hort studies. The Mica application helps data custodians and study or network coordinators to efficiently organize and disseminate information about their studies and net works without significant technical effort."
  },
  {
    "objectID": "notes/maelstrom-readings.html#fortier2010",
    "href": "notes/maelstrom-readings.html#fortier2010",
    "title": "Maelstrom reading notes",
    "section": "Fortier et al. (2010)",
    "text": "Fortier et al. (2010)\n\nA very grandiose paper presenting the grand vision for DataSHaPER, which would eventually become Maelstrom.\n\nLots of co-authors!\n\nInvokes the pan-European EPIC project (European Prospective Investigation into Cancer and Nutrition), which faced numerous data synthesis challenges despite its proactive effort to coordinate work across numerous research centres.\n\n\nTwo complementary approaches may be adopted to support effective data synthesis. The first one principally targets ‘what’ is to be synthesized, whereas the other one focuses on ‘how’ to collect the required information. Thus: (i) core sets of information may be identified to serve as the foundation for a flexible approach to harmonization; or (ii) standard collection devices (questionnaires and stand ard operating procedures) may be suggested as a required basis for collection of information.\n\n\nDataSHaPER is an acronym for DataSchema and Harmonization Platform for Epidemiological Research.\n\n\nIn an ideal world, information would be ‘prospectively harmonized’: emerging studies would make use, where possible, of harmonized questionnaires and standard operating procedures. This enhances the potential for future pooling but entails significant challenges —- ahead of time -— in developing and agree ing to common assessment protocols. However, at the same time, it is important to increase the utility of existing studies by ‘retrospectively harmonizing’ data that have already been collected, to optimize the subset of information that may legitimately be pooled. Here, the quantity and quality of infor mation that can be pooled is limited by the heterogeneity intrinsic to the pre-existing differences in study design and conduct.\n\nCompares prospective and retrospective harmonizatiom, with the former being presented as ideal, and the latter being a pragmatic reconciliation in acknowledgement that the former is essentially impossible to achieve.\n\nDataSHaPER is strikingly similar to OCHRE:\n\nXML-based data structures\nGenesis of a generic and ultimately optional base-level schema that illustrates the kind of data that the data structure may hold in ways that are immediately recognizable to all practitioners (at OCHRE it was associations between contexts and finds)\nSeparate harmonization platform where users can edit and manipulate records and associations between them\n\n\n\nThe question ‘What would constitute the ultimate proof of success or failure of the DataSHaPER approach’ needs to be addressed. Such proof will necessarily accumulate over time, and will involve two fundamental elements: (i) ratification of the basic DataSHaPER approach; and (ii) confirmation of the quality of each individual DataSHaPER as they are developed and/or extended. An important indication of the former would be provided by the widespread use of our tools. However, the ultimate proof of principle will necessarily be based on the generation of replicable scientific findings by researchers using the approach. But, for such evidence to accumulate it will be essential to assure the quality of each individual DataSHaPER. Even if the fundamental approach is sound, its success will depend critically on how individual DataSHaPERs are constructed and used. It seems likely that if consistency and quality are to be assured in the global development of the approach, it will be necessary for new DataSHaPERs to be formally endorsed by a central advisory team."
  },
  {
    "objectID": "notes/maelstrom-readings.html#fortier2011",
    "href": "notes/maelstrom-readings.html#fortier2011",
    "title": "Maelstrom reading notes",
    "section": "Fortier et al. (2011)",
    "text": "Fortier et al. (2011)\nThis paper responds to Hamilton et al. (2011), which presents an effort to devise a standardized nomenclature. The response is basically to advocate for a more flexible approach, rather than a stringent one promoted by Hamilton et al. (2011). It draws extensively from concepts published in the foundational paper by Fortier et al. (2010).\n\nTwo complementary approaches to harmonization may be adopted to support effective data synthesis or comparison across studies. The first approach makes use of identical data collection tools and procedures as a basis for harmoni zation and synthesis. Here we refer to this as the ‘‘stringent’’ approach to harmonization. The second approach is con sidered ‘‘flexible’’ harmonization. Critically, the second ap proach does not demand the use of identical data collection tools and procedures for harmonization and synthesis. Rather, it has to be based on sound methodology to ensure inferential equivalence of the information to be harmonized. Here, standardization is considered equivalent to stringent harmonization. It should, however, be noted that the term standard is occasionally employed to refer to common con cepts or comparable classification schemes but does not necessarily involve the use of identical data collection tools and procedures (12, 13).\n\nThis directly parallels the distinction made in Fortier et al. (2010) between “ideal” prospective and more pragmatic retrospective approaches to data harmonization.\n\nSynthesis of data using a flexible harmonization approach may be either prospective or retrospective. To achieve flexible prospective harmonization, investigators from several studies will agree on a core set of variables (or measures), compatible sets of data collection tools, and standard operating procedures but will allow a certain level of flexibilit in the specific tools and procedures used in each study (16, 17). Retrospective harmonization targets synthesis of information already collected by existing legacy studies (15, 18, 19). As an illustrative example, using retrospective harmonization, researchers will define a core set of variables (e.g., body mass index, global level of physical activity) and, making use of formal pairing rules, assess the potential for each participating study to create each variable (15). The ability to retrospectively harmonize data from existing studies facilitates the rapid generation of new scientifi knowledge.\n\nI wonder why there is no example provided for prospective data harmonization. Is it because it is ideal and not realistic? I’d argue that it is simply what occurs within individual projects."
  },
  {
    "objectID": "notes/maelstrom-readings.html#fortier2017",
    "href": "notes/maelstrom-readings.html#fortier2017",
    "title": "Maelstrom reading notes",
    "section": "Fortier et al. (2017)",
    "text": "Fortier et al. (2017)\nExplicit statement regarding the rationale and presumed benefits of harmonization right in the first paragraph:\n\nThe rationales underpinning such an approach include ensuring: sufficient statistical power; more refined subgroup analysis; increased exposure hetero geneity; enhanced generalizability and a capacity to under take comparison, cross validation or replication across datasets. Integrative agendas also help maximizing the use of available data resources and increase cost-efficiency of research programmes.\n\nSummarized in bullet points:\n\nensuring sufficient statistical power\nmore refined subgroup analysis\nincreased exposure heterogeneity\nenhanced generalizability\na capacity to undertake comparison, cross validation or replication across datasets.\nmaximizing the use of available data resources\nincrease cost-efficiency of research programmes\n\nClearly defines harmonization and its benefits:\n\nEssentially, data harmonization achieves or improves comparability (inferential equivalence) of similar measures collected by separate studies.\n\nAdds an additional argument for retrospective harmonization on top of prior discussion of retrospective/prospective approaches (cf. Fortier et al. (2010); Fortier et al. (2011)):\n\nRepeating identical protocols is not necessarily viewed as providing evidence as strong as that obtained by exploring the same topic but using different designs and measures.\n\nAlso relates retrospective harmonization from systematic meta reviews. In fact, the paper basically responds to calls for more structured guidelines for data harmonization, similar to those that had been produced to support structured metareviews in the years prior to this publication. The authors identify several papers that have done similar guidelines or reports on harmonization practices, which they claim are too broad. Those papers include:\n\nRolland et al. (2015)\n\nhttps://doi.org/10.1093/aje/kwv133\n\nSchaap et al. (2011)\n\nhttps://doi.org/10.1186/1471-2474-12-272\n\nBennett et al. (2011)\n\nhttps://doi.org/10.1002/gepi.20564\n\nHohmann et al. (2012)\n\nThis paper reports the findings of a systematic inquiry made to data harmonization initiatives, whose data comprise responses to a questionnaire. The findings indicate that procedures were more attentively follows during earlier stages, such as when matching and aligning available data with the project’s designated scope. However, procedures were less sound with regards to documenting procedures, validating the results of data processing, and dissemination strategy. There is a notable division between work that occurs before and after people actually begin handling the data, which indicates a tension between aspirational plans and tangible experiences.\n\nRespondents were asked to delineate the specific procedures or steps undertaken to generate the harmonized data requested. Sound procedures were generally described; however, the terminologies, sequence and technical and methodological approaches to these procedures varied considerably. Most of the procedures mentioned were related to defining the research questions, identifying and selecting the participating studies (generally not through a systematic approach), identifying the targeted variables to be generated and processing data into the harmonized variables. These procedures were reported by at least 75% of the respondents. On the other hand, few reported steps related to validation of the harmonized data (N=4; 11.8%), documentation of the harmonization process (N=5; 14.7%) and dissemination of the harmonized data outputs (N=2; 5.9%).\n\nThe paper summarizes some specific “potential pitfalls” reported by respondents to their survey:\n\nensuring timely access to data;\nhandling dissimilar restrictions and procedures related to individual participant data access;\nmanaging diversity across the rules for authorship and recognition of input from study-specific investigators;\nmobilizing sufficient time and resources to conduct the harmonization project;\ngathering information and guidance on harmonization approaches, resources and techniques;\nobtaining comprehensive and coherent information on study-specific designs, standard operating procedures, data collection devices, data format and data content;\nunderstanding content and quality of study-specific data;\ndefining the realistic, but scientifically acceptable, level of heterogeneity (or content equivalence) to be obtained;\ngenerating effective study-specific and harmonized datasets, infrastructures and computing capacities;\nprocessing data under a harmonized format taking into account diversity of: study designs and content, study population, synchronicity of measures (events measured at different point in time or at different intervals when repeated) etc;\nensuring proper documentation of the process and decisions undertaken throughout harmonization to ensure transparency and reproducibility of the harmonized datasets;\nmaintaining long-term capacities supporting dissemination of the harmonized datasets to users.\n\nIt’s not made clear how these responses were distributed among respondents.\nThe authors then identify several absolute essential requirements needed to achieve success:\n\nCollaborative framework: a collaborative environment needs to be implemented to ensure the success of any harmonization project. Investigators involved should be open to sharing information and knowledge, and investing time and resources to ensure the successful implementation of a data-sharing infrastructure and achievement of the harmonization process.\nExpert input: adequate input and oversight by experts should be ensured. Expertise is often necessary in: the scientific domain of interest (to ensure harmonized variables permit addressing the scientific question with minimal bias); data harmonization methods (to support achievement of the harmonization procedures); and ethics and law (to address data access and integration issues).\nValid data input: study-specific data should only be harmonized and integrated if the original data items collected by each study are of acceptable quality.\nValid data output: transparency and rigour should be maintained throughout the harmonization process to ensure validity and reproducibility of the harmonization results and to guarantee quality of data output. The common variables generated necessarily need to be of acceptable quality.\nRigorous documentation: publication of results generated making use of harmonized data must provide the information required to estimate the quality of the process and presence of potential bias. This includes a description of the: criteria used to select studies; process achieved to select and define variables to be harmonized; procedures used to process data; and characteristics of the study-specific and harmonized dataset(s) (e.g. attribute of the populations).\nRespect for stakeholders: all study-specific as well as network-specific ethical and legal components need to be respected. This includes respect of the rights, intellectual property interests and integrity of study participants, investigators and stakeholders.\n\nThe authors describe how they arrived at guidelines following the results of this study:\n\nA consensus approach was used to assemble information about pitfalls faced during the harmonization process, establish guiding principles and develop the guidelines. The iterative process (informed by workshops and case studies) permitted to refine and formalize the guide lines. The only substantive structural change to the initial version proposed was the addition of specific steps relating to the validation, and dissemination and archiving of harmonized outputs. These steps were felt essential to em phasize the critical nature of these particular issues.\n\nThe paper outlines a checklist of stages that data harmonization initiatives need to go through to produce ideal outcomes. For each task, they describe a scenario in which the task can be said to be complete, whhich resembles an ideal outcome. This is described in the paper, summarized in a table, and more comprehensively documented in the supplementary materials.\nAlso worth noting, this paper includes a list of harmonization initiatives that I may consult when selecting cases. I’m not quite sure how useful it will be since the findings don’t really break down the distribution of responses in any detail, but maybe the authors have done this analysis and not published it."
  },
  {
    "objectID": "notes/maelstrom-readings.html#bergeron2018",
    "href": "notes/maelstrom-readings.html#bergeron2018",
    "title": "Maelstrom reading notes",
    "section": "Bergeron et al. (2018)",
    "text": "Bergeron et al. (2018)\nThe authors reference the drive for efficiency as a motivating factor that drives open data:\n\nHowever, many cohort databases remain under-exploited. To address this issue and speed up discovery, it is essential to offer timely access to cohort data and samples.\n\nHowever the paper is actually about the need for better and more publicly accessible documentation about data.\nThe authors state that catalogues exist to promote discoverability of data and samples and to answer the data documentation needs of individual studies.\nThey draw attention to the importance of catalogues in research networks (analyzing data across studies), which establish portals that document “summary statistics on study subjects, such as the number of participants presenting specific characteristics (e.g. diseases or exposures)”.\nThe authors outline several challenges that inhibit or limit the potential value of catalogues:\n\nThe quality of a catalogue directly depends on the quality and comprehensiveness of the study-specific information documented. But, maintaining and providing access to understandable and comprehensive documentation to external users can be challenging for cohort investigators, and require resources not always available, particularly for the very small or long-established studies. In addition, the technical work required to build and maintain a catalogue is particularly demanding. For example, gathering comprehensive and comparable information on study designs necessitates the implementation of rigorous procedures and working in close collaboration with study investigators. Manual classification of variables is also a long and a tedious process prone to human error. Moreover, the information collected needs to be regularly revised to update metadata with new data collections. These challenges, among others, can lead to the creation of catalogues with partial or disparate information across studies, documenting limited subsets of variables (e.g. only information collected at baseline) or including only studies with data dictionaries available in a specific language or format.\n\nBullet point summary:\n\nA catalogue’s quality depends on the quality and comprehensiveness of documentation provided by individual studies\nCohort investigators, i.e. leaders of individual studies, are under-equipped to provide such comprehensive documentation\n\nDo they just need material support? Or also guidance on how to do it, what factors to account for, etc?\n\nTechnical work for building and maintaining a catalogue is demanding\n\nI’m not sure if they example they provide to illustrate these challenges aligns with what I would call “technical work”; they refer to precise and detailed documentation in direct consultation with individual study maintainers, and I suppose the discussion about and documentation of methodological details is technical in that it corresponds with the work that was actually done on the ground, using data collection and processing instruments\n\nClassification of variables is a long and tedious process\n\nWhat makes it long and tedious? This isn’t really specified\nThey recite that this is prone to human error, but I wonder what successful or error-ful (?) outcomes would look like and how they would differ\n\nThe information needs to be regularly revised and updated\n\nThe authors recommendations to resolve these concerns:\n\nHowever, to truly optimize usage of available data and leverage scientific discovery, implementation of high quality metadata catalogues is essential. It is thus important to establish rigorous standard operating procedures when developing a catalogue, obtain sufficient financial support to implement and maintain it over time, and where possible, ensure compatibility with other existing catalogues.\n\nBullet point summary:\n\nestablish rigorous standard operating procedures when developing a catalogue\nobtain sufficient financial support to implement and maintain it over time\nwhere possible, ensure compatibility with other existing catalogues"
  },
  {
    "objectID": "notes/maelstrom-readings.html#bergeron2021",
    "href": "notes/maelstrom-readings.html#bergeron2021",
    "title": "Maelstrom reading notes",
    "section": "Bergeron et al. (2021)",
    "text": "Bergeron et al. (2021)\nIdentifies several registries of relevant cohorts, but notes that they face challenges getting the data together. Namely, issues concerning institutional policies concerning data-sharing, lack of open access to cohort data and to documentation about the data, the data’s complexity which makes it difficult to harmonize across studies, and lack of access to funding, secure data environments, and specialized expertise and resources.\nThe Research Advancement through Cohort Cataloguing and Harmonization (ReACH) initiative was establihed in collaboration with Maelstrom to overcome some of these barriers in the context of Developmental Origins of Health and Disease (DOHaD) research.\nThe authors briefly summarize some projects that rely on ReACH data, and provide a more comprehensive table of ongoing and unpublished work.\nIn the supplementary materials, the authors also include an illustrative example specific tasks, decisisions and actions that one might got through when using ReACH data. It is a broad-level but fairly sober account of how one would navigate the catalogue and engage with collaborators."
  },
  {
    "objectID": "notes/maelstrom-readings.html#wey2024",
    "href": "notes/maelstrom-readings.html#wey2024",
    "title": "Maelstrom reading notes",
    "section": "Wey and Fortier (2024)",
    "text": "Wey and Fortier (2024)\nAn overview of the harmonization procedires applied in CanPath and MINDMAP. Authored by two members of the Maelstrom team, but no one from these initiatives.\nAt first, comes across as another broad-level overview of processes. But, as is elicited in the conslusion, the paper highlights some subtle divergent approaches, some of which are relevant to my project and I picked up here.\nInteresting bit about use of centralized systems or systems that are more conducive to end-users’ individual workflows.\n\nIn both illustrative projects, information about participating studies and data collected was gathered and made available on the central data server. In the CanPath BL-HRFQ, harmonization documentation and processing scripts were held centrally and only updated by Maelstrom Research. In MINDMAP, because multiple groups simultaneously worked on generating harmonization processing scripts for different areas of information, working versions of the DataSchema and processing scripts were held on a GitHub, allowing for better version control and dynamic updating of scripts by multiple remote data harmonizers.\n\nMore about the balance being struck at MINDMAP between institutional and popular tech and documentation platforms:\n\nIn the MINDMAP project, R markdown documents with applied harmonization scripts (Figure 13.1b) and any comments were preserved in the GitHub repository, which is also publicly accessible. Furthermore, summary statistics for MINDMAP variables are only available to approved data users through the secure server, as harmonized datasets are only intended for use within the MINDMAP network. Overviews of the harmonization process and outputs have also been published as open-access, peer-reviewed articles for both projects (Fortier et al. 2019; Wey et al. 2021).\n\nBit about existing collaborative ties making things much easier:\n\nthe prospective coordination and unu sually high ability for the harmonization team (Maelstrom Research) to work directly with study data managers on recently collected and documented study data resulted in high standardization and ability to resolve questions.\n\nData curation as an explicitly creative task, involving impactful decisions. Interesting that this was unexpected enough to warrant inclusion as a key challenge:\n\nIn MINDMAP, these differences were clearly documented in comments about harmonization for each source dataset to allow researchers to decide how to use the harmonized variable. In-depth exploration of the best statistical methods to harmonize these types of measures to maintain integrity of content while minimizing loss of information and methodological bias are important and active areas of research (Griffith et al. 2016; Van den Heuvel and Griffith 2016). The approach taken in this case was to apply simpler harmonization meth ods (i.e. rescaling) and document the transformation, leaving investigators the flexibility to further explore and analyze the harmonized datasets as appropriate for their research questions.\n\nAs with the above excerpt, documentation was considered as a viable mode of resolving or accounting for significant discrepancies:\n\nThe papers describing the harmonization projects attempt to highlight these considerations, for example, providing some comparison of demographics of the harmonized populations against the general populations from which they were drawn (Dummer et al. 2018; Fortier et al. 2019; Wey et al. 2021), listing sources of study-specific heterogeneity in the harmonized datasets to consider (Fortier et al. 2019), and pointing users to individual study documentation where more information on weights to use for analysis should be con sidered (e.g. Wey et al. 2021).\n\nThe bit about use of github was rationalized as a way of facilitating collaboration across insitutional boundaries. They refer to R and RMarkdown being open standards as the main reason, but I wonder if a part of it is that GitHub, being a non-institutional platform, was easier to use from an oboarding perspective:\n\nIn MINDMAP, due to the need for multiple international groups to work simultaneously and flexibly on the harmonization processing and frequently evolving versions of study-specific harmonization scripts, scripts for harmonization processing were written and applied entirely through R markdown in an RStudio interface, and the DataSchema and R markdown versions were maintained and frequently updated in a GitHub repository.\n\nPerhaps ironically, the private company and platform may have been used due to the strength of pan-european collaborative ties that institutions may not be able to keep up with. Whereas in Canada, with centralized project-oriented work, it may be much easier to enforce adoption of centralized tooling. This is just speculation."
  },
  {
    "objectID": "notes/maelstrom-readings.html#doiron2013a",
    "href": "notes/maelstrom-readings.html#doiron2013a",
    "title": "Maelstrom reading notes",
    "section": "Doiron, Raina, and Fortier (2013)",
    "text": "Doiron, Raina, and Fortier (2013)\nThis paper summarizes what was discussed at a workshop bringing together stakeholders who would contribute to two large data harmonization initiatives: the Canadian Longitudinal Study on Aging (CLSA) and the Canadian Partnership for Tomorrow Project (CPTP). It is therefore representative of plans and challenges that were held at an early stage when collaborations were being established.\nThe authors identify series of reasons for linking data, which I summarize here:\n\nMaximizing potential of disparate information resources\n\n\nenriching study datasets with additional data not being collected directly from study par ticipants\noffer vital information on health outcomes of participants\nvalidate self-reported information\n\n\nDrawing maximum value from data produced from public expenditure\n\n\noffers a cost-effective means to maximize the use of existing publicly funded data collections\n\n\nDevelop interdisciplinary collaborative networks\n\n\nby combining a wide range of risk factors, disease endpoints, and relevant socio-economic and biological measurements at a population level, linkage lays the groundwork for multidisciplinary health-research initiatives, which allow the exploration of new hypotheses not foreseeable using independent datasets\n\n\nEstablish long-lasting infrastructure and instill a collaborative culture\n\n\nLast, a coordinated pan-Canadian cohort-to-administrative linked database would establish legacy research infrastructures that will better equip the next generation of researchers across the country\n\nThe authors use the term “data linkage”:\n\nData linkage is “the bringing together from two or more different sources, data that relates to the same individual, family, place or event”. When linking data at the individual level, a common identifier (or a combination of identifiers) such as a personal health number, date of birth, place of residence, or sex, is used to combine data related to the same person but found in separate databases. Data linkage has been used in a number of research fields but is an especially valuable tool for health research given the large amount of relevant information collected by institutions such as governments, hospitals, clinics, health authorities, and research groups that can then be matched to data collected directly from consenting individuals participating in health research.\n\nThis is distinct from harmonization in that it is not meant to combine data with similar scope and schematic structure, but rather to relate information collected under various domains so that they could be more easily queried in tandem. I imagine this as reminiscient of establishing links between tables in a relational database.\nThe authors identify the open-endedness of the linked data as a unique challenge, without elaborating on this point:\n\nCLSA/CPTP-to-AHD linkage also poses unique challenges in that, in contrast to more traditional requests to link data to answer one-off research questions, it aims to establish a rich data repository that will allow investigators to answer a multitude of research questions over time.\n\nThe workshop participants established a 5-point plan:\n\nbuild strong collaborative relationships between stakeholders involved in data sharing (e.g., researchers, data custodians, and privacy commissioners);\nidentify an entity which could provide overall leadership as well as individual “champions” within each province;\nfind adequate and long-term resources and funding;\nclarify data linkage and data-sharing models and develop a common framework within which the data linkage process takes place; and\ndevelop a pilot project making use of a limited number of linked variables from participating provinces\n\nThe second point, about identifying “champions”, is kind of interesting, and I’d like to know more about what qualities these people were expcted to have, their domains of expertise, their collaborative/soft or technical skills, and how this plays into access to funds and general governance structures\nNeed to look at Roos, Menec, and Currie (2004), which they cite in the conclusion, specifically with reference to the aspiration to develop “information rich environments”. Seems like it is a primary source for the background on linked data in Manitoba and Australia."
  },
  {
    "objectID": "notes/maelstrom-readings.html#fortier2023",
    "href": "notes/maelstrom-readings.html#fortier2023",
    "title": "Maelstrom reading notes",
    "section": "Fortier et al. (2023)",
    "text": "Fortier et al. (2023)\nRelates harmonization to the FAIR principles, which has not really been featured much in arguments for harmonization in prior Maelstrom papers. Specifically, this paper frames harmonization as a necessary additional condition that enables FAIR data to be made useful; FAIR is deemed not enough.\n\nIn the following paper, we aim to provide an overview of the logistics and key ele ments to be considered from the inception to the end of collabo rative epidemiologic projects requiring harmonizing existing data.\n\nInteresting acronym/framework for defining research questions:\n\nThe research questions addressed and research plan proposed by harmonization initiatives need to be Feasible, Interesting, Novel, Ethical, and Relevant (FINER). (Cummings, Browner, and Hulley 2013)\n\nTable 1 lists examples of questions that could be addressed to help delineate analytical approach, practical requirements, and operations of a harmonization initiative. These are all questions that look toward or project specific goals, which then inform the strategies through which they may be achieved.\nThe supplementary materials include very detailed information about specific practices and objectives pertaining to the REACH initiative. However, it’s unclear how well this reflects any specific challenges experienced. In other words, I was hoping for something more candid.\nMoreover, this paper is also based on survey responses from 20 harmonization initiatives, but neither the findings resulting from the analysis, nor the data, are referenced or included. Is this the same as the one that informed Fortier et al. (2017)?\nLooming in the background of this paper is the DCC lifecycle model. However they do not cite DCC, or the field of digital curation in general. The DCC lifecycle model has always been presented as a guideline, sort of divorced from practical experience, or at least that’s how I’ve always understood it. Basically, and literally, a model for expected behaviours and interstitial outcomes. I think it would be interesting to explore (a) why the authors perceived need to present a model and (b) how they arrived at the phases and principles that are included in it. Is this tacit or common-sense thinking? Or was this directly informed by my concrete thinking from the field of digital curation?\nI should brush up on more recent work regarding the DCC, and specifically, critiques of it. Through a quick search, a few papers seem directly relevant:\n\nCox and Tam (2018)\nChoudhury, Huang, and Palmer (2020)\nRhee (2024)\n\nI think Cox and Tam (2018) may be especially relevant as a critique of the lifecycle metephor in a general sense. From the abstract, it seems like they identify various downsides pertaining to lifecycle models, specifically that they “mask various aspects of the complexity of research, constructing it as highly purposive, serial, uni-directional and occurring in a somewhat closed system.” I’m not sure how explicit the connection is, but I sense this ties into the general paradigm shift toward greater recognition of curation “in the wild”, as per Dallas (2016)."
  },
  {
    "objectID": "notes/maelstrom-readings.html#gaye2014",
    "href": "notes/maelstrom-readings.html#gaye2014",
    "title": "Maelstrom reading notes",
    "section": "Gaye et al. (2014)",
    "text": "Gaye et al. (2014)\nIntroduces DataShield.\nFrames DataShield as a technical fix to administrative problems:\n\nMany technical and policy measures can be enacted to render data sharing more secure from a governance per spective and less likely to result in loss of intellectual prop erty. For example, data owners might restrict data release to aggregate statistics alone, or may limit the number of variables that individual researchers might access for speci fied purposes. Alternatively, secure analysis centres, such as the ESRC Secure Data Service and SAIL represent major informatics infrastructures that can provide a safe haven for remote or local analysis/linkage of data from selected sources while preventing researchers from down loading the original data themselves. However, to comple ment pre-existing solutions to the important challenges now faced, the DataSHIELD consortium has developed a flexible new way to comprehensively analyse individual level data collected across several studies or sources while keeping the original data strictly secure. As a technology, DataSHIELD uses distributed computing and parallelized analysis to enable full joint analysis of individual-level data from several sources, e.g. research projects or health or administrative data—without the need for those data to move, or even be seen, outside the study where they usually reside. Crucially, because it does not require underpin ning by a major informatics infrastructure and because it is based on non-commercial open source software, it is both locally implementable and very cost effective.\n\nAdds a social/collaborative element to earlier arguments about the challenges inherent of prospective harmonization, highlighting a need for engagement with individual studies (either through direct or peripheral participation) to conduct research that was not initially planned for:\n\nUnfortunately, both [study-level metadata] SLMA and [individual-level metadata] ILMA present significant problems Because SLMA com bines analytical results (e.g. means, odds ratios, regression coefficients) produced ahead of time by the contributing studies, it can be very inflexible: only the pre-planned analyses undertaken by all the studies can be converted into joint results across all studies combined. Any additional analyses must be requested post hoc. This hinders exploratory analysis for example the investigation of sub-groups, or interactions between key variables.\n\nProvides a detailed overview of how DataShield was implemented for HOP (Healthy Obesity Project), including the code used to generate specific figures and analyses. Hoever it does not really describe or reflect upon the processes through which the code was developed.\nThe authors highlight the fact that certain analytical approaches are not possible using DataShield, especially analysis that visualize individual data points. It’s unclear how they enforce this, or whether it’s an implicit limitation based on the data that DataShield participants provide.\n\nBecause in DataSHIELD potentially disclosive com mands are not allowed, some analyses that are possible in standard R are not enabled. In essence, there are two classes of limitation on potential DataSHIELD functional ity: (i) absolute limitations which require an analysis that can only be undertaken by enabling one of the functional ities (e.g. visualizing individual data points) that is explicitly blocked as a fundamental element of the DataSHIELD philosophy. For example, this would be the case for a standard scatter plot. Such limitations can never be circumvented and so alternatives (e.g. contour and heat map plots) are enabled which convey similar information but without disclosing individual data points; (ii) current limitations which are functions or models that we believe are implementable but we have not, as yet, under taken or completed the development work required. As examples, these latter include generalized linear mixed model (including multi-level modelling) and Cox regression.\n\nThe authors list numerous other limitations and challenges. Some have to do with what kinds of data DataShield can handle (something about horizontal and vertical that I do not yet fully understand). Other challenges include the need for data to be harmonized, and having to deal with governance concerns.\nNotably, the first challenge mentioned seems to contradict the statement earlier on (and made by Doiron et al. (2013)) that this is relatively easy to set up. The authors acknowledge the fact that coding for analysis using DataShield has a steep learning curve and requires some pre-planning to enable results from satellite computers to be properly combined. Their mitigation is to black-box these concerns by implementing simpler client-side functions that mask the more complex behaviours (and presumably translate error messages in ways that users can understand and act to resolve!).\n\nDespite its potential utility, implementation of DataSHIELD involves significant challenges. First, although set-up is fundamentally straightforward, application involves a relatively steep learning curve because the command structure is complex: it demands specification of the analysis to be undertaken, the studies to use and how to combine the results. In mitigation, most complex serverside functions are now called using simpler client-side functions and we are working on a menu-driven implementation.\n\nAlso interesting that they note how there may be unanticipated problems, either accidental or malicious, and their way of mitigating against this is to log all commands:\n\nFifth, despite the care taken to set up DataSHIELD so that it works properly and is non-disclosive, it is possible that unanticipated prob lems (accidental or malicious) may arise. In order to iden tify, describe and rectify any errors or loopholes that emerge and in order to identify deliberate miscreants, all commands issued on the client server and enacted on each data server are permanently logged.\n\nThis is even more interesting in light of their continuous reference to “data.care”, which they do not address in depth, but which seems to have been a scandal involving unauthorized release of personal health data used in research.\nThe authors add an additional caveat concerning the need to ensure that the data are cleaned in advance.\n\nBut, to be pragmatic, many of the routinely collected healthcare and administra tive databases will have to undergo substantial evolution before their quality and consistency are such that they can directly be used in high-quality research without exten sive preparatory work. By its very nature, such preparation—which typically includes data cleaning and data harmonization—cannot usually be undertaken in DataSHIELD, because it involves investigating discrepan cies and/or extreme results in individual data subjects: the precise functionality that DataSHIELD is designed to block. Such work must therefore be undertaken ahead of time by the data generators themselves—and this is de manding of time, resources and expertise that — at present — many administrative data providers may well be unwilling and/or unable to provide. That said, if the widespread us ability of such data is viewed as being of high priority, the required resources could be forthcoming.\n\nThis corresponds with another limitation identified earlier, namely with regards to identifying duplicate individual records across jurisdictional boundaries (which involves assumptions regarding nationality and identify – one of those weird myths that programmers can’t seem to let go!):\n\nSo far DataSHIELD has been applied in settings where individual participants in different studies are from different countries or from different regions so it is unlikely that any one person will appear in more than one source. However, going forward, that cannot al ways be assumed. We have therefore been consider ing approaches to identify and correct this problem based on probabilistic record linkage. In the genetic setting 48 the BioPIN provides an alternative solution. Ongoing work is required.\n\nNote the last line of the prior block quote regarding data cleaning:\n\nThat said, if the widespread us ability of such data is viewed as being of high priority, the required resources could be forthcoming.\n\nThis seems like a thread worth tugging at!"
  },
  {
    "objectID": "notes/maelstrom-readings.html#wolfson2010",
    "href": "notes/maelstrom-readings.html#wolfson2010",
    "title": "Maelstrom reading notes",
    "section": "Wolfson et al. (2010)",
    "text": "Wolfson et al. (2010)\nx"
  },
  {
    "objectID": "notes/coding-methods.html",
    "href": "notes/coding-methods.html",
    "title": "Coding methods",
    "section": "",
    "text": "Some notes on coding methods that stand out as relevant to my work. Largely drawn from Saldaña’s (2016) excellent coding manual.\nThese notes should be treated as relating to my research specifically, and should not be considered generalizable. They are more like my impressions of their practical value for my own purposes. I therefore priorize notes on methods that I deem especially relevant to my research."
  },
  {
    "objectID": "notes/coding-methods.html#grammatical-methods",
    "href": "notes/coding-methods.html#grammatical-methods",
    "title": "Coding methods",
    "section": "Grammatical methods",
    "text": "Grammatical methods\nRefer to the basic grammatical principles of a technique, by articulating the forms and combinations of codes and encoded elements in a dataset.\n\nAttribute coding\n\nCould be useful for identifying affiliations or former affiliations, including who is or was their doctoral supervisor, whether they are a student, research associate or faculty member, etc.\nBut I imagine this might be better represented in a memo outlining a “cast of characters” rather than as codings.\n\n\n\nMagnitude coding\n\nCan be applied to tag positive/negative attitudes, but also other gradients like hard/soft, technical/social skills.\nCan use colons to identify a magnitude facet for specific codes corresponding with the code’s usage in the specific context.\n\nFor example: using “programming: 1” or “programming: 5” to identify different instances of programming and the degree of difficulty elicited regarding each instance.\nSee additional examples at Saldaña (2016: 88-19).\n\n\n\n\nSubcoding\n\n\nSimultaneous coding"
  },
  {
    "objectID": "notes/coding-methods.html#elemental-methods",
    "href": "notes/coding-methods.html#elemental-methods",
    "title": "Coding methods",
    "section": "Elemental methods",
    "text": "Elemental methods\nBasic but focused filters for reviewing the corpus, which build a foundation for future coding cycles.\n\nStructural coding\n\nUseful for identifying components of an interview transcript that may not matter as much, such as interviewer questions, long pauses or interuptions, or standardized preambles.\n\n\n\nDescriptive coding\n\nI should minimize this kind of coding, and replace with “initial / open” coding, use of sensitizing concepts, or in vivo coding instead.\nSpeaking of sensitizing concepts, I could use the prefix “SC:” to denote these as a distinct kind of code.\nSaldaña likens them to hashtags on social media.\n\n\n\nIn vivo coding\n\nI imagine creating in vivo codes in an opportunistic way, whenever I see something that really stands out, and therefore is best applied during an initial or open pass through the document.\nFor this reason, I should distinguish these codes with a prefix “IV:”.\nI imagime this could be especially useful to articulate the improvised adoption of certain terms or imagery, which can be somewhat haphazard (as per my observation that the curatorial lens reifies and perpetuates certain notions of data based on common and/or authoritative use of specific terms across contexts).\n\n\n\nProcess coding\n\nThis one will be very important, I think.\nEssentially entails framing ordinary things in terms of broader improvised actions, as attempts, as interactions between people and between agents.\nSaldaña suggests embodying these codes through gestures, even while coding, as an arts-based heuristic.\n\nFrom Clarke (2003: 558) on process coding:\n\nIn a traditional grounded theory study, the key or basic social process is typically articulated in gerund form connoting ongoing action at an abstract level. Around this basic process are then constellated the particular and distinctive conditions, strategies, actions, and practices engaged in by human and nonhuman actors in volved with/in the process and their consequences. For example, subprocesses of disciplining the scientific study of reproduction include formalizing a scientific discipline, gleaning fiscal support for research, producing contraceptives and other techno scientific products, and handling any social controversies the science provokes (such as cloning and stem cell research).\n\n\n\nInitial coding\n\nAlso referred to as “open coding”.\nBreaks down data into discrete parts, closely examines the, and compares for similarities and differences.\nImportant to remain open to all possible theoretical directions suggested by your interpretations of the data.\nAn opportunity to reflect deeply on the contents and nuances of the data and to begin taking ownership over them.\nCharmaz (2014) suggests doing this line-by-line over each document as they are ingested into the dataset.\n\n\n\nConcept coding\n\nWhat Saldaña refers to as “concept coding” is what I have previously referred to as “theoretical coding”, to a certain extent.\nIt’s a form of lumping, identifying specific instances under the label of cohesive concepts."
  },
  {
    "objectID": "notes/coding-methods.html#affective-methods",
    "href": "notes/coding-methods.html#affective-methods",
    "title": "Coding methods",
    "section": "Affective methods",
    "text": "Affective methods\nInvestigate the subjective qualities of human experience (including emotions, values, conflicts and judgements) by directly acknowledging and naming those experiences. These affective qualities are core motives for human action, reaction and interaction.\n\nEmotion coding\n\nDistinguishes between emotions and moods:\n\nEmotion: a feeling and its distinctive thoughts, psychological and biological states, and range of propensities to act.\nMood: a general aura, sustained quality, or the perception of another’s emotional tone.\n\nMood may be more relevant to me, especially when identifying aspects of phases or contexts of work.\n\n\n\nValues coding\n\nActually entails coding values, attitudes and bliefs:\n\nA value is the importance we attribute to ourselves, another person, thing, or idea; the principles, moral codes, and situational norms people live by.\nAn attitude is the way we think and feel about ourselves, another person, thing or idea, and are part of a relatively enduring system of evaluative, affective reactions.\nA belief is part of a system that includes our values and attitudes, plus our personal knowledge, experiences, opinions, prejudices, morals, and other perceptions of the social world; they can be considered as “rules for action”.\n\nUse prefixes “V:”, “A:” and “B:” to distinguish these things.\nSome key phrases that signpost values, attitudes and beliefs include:\n\nIt’s important that…\nI like…\nI love…\nI need…\nI think…\nI feel…\nI want…\n\n\n\n\nVersus coding\n\nIdentifies in dichotomous or binary terms the individuals, groups, social systems, organizations, phenomena, processes, concepts, etc in direct conflict with each other.\nThese moities generally exhibit power imbalances.\nSaldaña does not specify this, but the framing of a duality also draws an association as things that may be compared along opposite ends of the same spectrum; this gives them something in common, and figuring out that commonality that exists alongside the differences could be really insightful, perhaps.\nSaldaña advises against getting too caught up in the hero/villain, protagonist/antogist dichotomy, and reminds us that these dualities are context-specific and should be treated as such.\n\n\n\nEvaluation coding\n\nCommonly used in situations where participants are asked to assign judgements to the merit, worth, or significance of programs or policy.\nEvaluation data may describe, compare or predict:\n\nDescription focuses on patterned observations or participant responses of attributes that assess their quality.\nComparison explores how the program measures up to a standard or ideal.\nPrediction provides recommendations for change, if needed, and how those changes1 might be implemented."
  },
  {
    "objectID": "notes/coding-methods.html#literary-and-language-methods",
    "href": "notes/coding-methods.html#literary-and-language-methods",
    "title": "Coding methods",
    "section": "Literary and language methods",
    "text": "Literary and language methods\nBorrow from established approaches in the analysis of literature and oral communication to explore underlying sociological, psychological and cultural constructs.\n\nDramaturgical coding\n\nApproaches naturalistic observations and interview narratives as “social drama”.\nLife is perceived as performance, with humans cast as characters in conflict.\nInterview transcripts are framed as dialogue, soliloquy and dialogue; field notes and naturalistic videos are framed as improvized scenarious with stage direction.\nEnvironments, dress and artefacts are viewed as scenery, costumes and props.\nThis is enacted by applying terms and conventions from scriptwriting as code prefixes:\n\n“OBJ”: participant-actor objectives\n“CON”: conflicts or obstacles\n“TAC”: tactics or strategies to deal with conflucts\n“ATT”: attitudes toward setting, others, or conflicts\n“EMO”: emotions experiences by actors\n“SUB”: subtexts, or the actors’ unspoken thought or impression management\n\nUseful in projects leading to narrative or arts-based presentational forms.\nAlso useful to trace daily routines or regular interactions among participants.\nBest applied to self-standing, inclusive vignettes, episodes or stories in the data record, which may even be divided into stanzas or scenes connected by plotting devies like curtain-raising or prologues.\n\n\n\nMotif coding\n\nUsed to classify types and elements of folk tales, myths and legends.\nA motif as a literary device is an element that sometimes appears several times within a narrative work.\nA type refers to the complete tale, whereas a motif refers to the smallest element in a tale that has something unique about it, such as kinds of characters, significant objects, and/or incidents of action.\nAppropriate for exploring identity studies and oral histories.\n\n\n\nNarrative coding\n\nIdentifies elements of narrative and story-telling, such as conflict, flashback/flashforward, vignette, resolution, aside, subtext, etc.\nSuitable for inquiries on identity development, psychological, social and cultural meanings and values, critical/feminist studies, and documentation of the life course.\n\n\n\nVerbal exchange coding\n\nEntails verbatim transcript analysis and interpretation of the types of conversation and personal meanings of key moments in the exchanges.\nCoding determines the “generic type” of conversation, and reflection examines the meaning of the conversation.\nSome generic types of conversation include: phatic communication or ritual interaction; ordinary conversation; skilled conversation; personal narratives; and dialogue.\nAfter classifying conversations, the analyst explores personal meanings and key moments by examining speech mannerisms, non-verbal communication habits and rich points of cultural knowledge (slang, jargon, etc).\nThis them proceeds to examine the practices or cultural performances evident in these interactions, which are situated on a continuum:\n\nroutines and rituals of structured, symbolically meaningful actions during our day\nsurprise-and-sense0making episodes of the unanticipated or unexpected\nrosl-taking episodes and face-saving episodes of conflict-laden exchanges\ncrises in a verbal exchange or as an overarching pattern of lived experience\nrites of passage, or what is done that significantly alters or changes our personal sense of self or out social or professional status or identity\n\nUseul for examining dialogical interaction."
  },
  {
    "objectID": "notes/coding-methods.html#exploratory-methods",
    "href": "notes/coding-methods.html#exploratory-methods",
    "title": "Coding methods",
    "section": "Exploratory methods",
    "text": "Exploratory methods\nExploratory and preliminary assignments of coces to the data before more refined coding systems are developed and applied.\n\nHolistic coding\n\nAttempts to grasp basic themes or issues in the data by absorbing them as a whole (by lumping them) rather than by analyzing them line by line (by splitting).\nThis is a preparatory approach to a unit of data before a more detailed coding or categorization process occurs.\nI imagine this as being somewhat similar to open/initial coding, but operating at a coarser grain.\nUseful for “chunking” the data into broader topics as a preliminary step for more detailed analysis.\nCould also be a time-saving method if there is a massive amount of data.\n\n\n\nProvisional coding\n\nEstablishes a predetermined list of codes prior to fieldwork, developed from anticipated categories or types of responses or actions that may arise in the data yet to be collected.\nThe provisional list is generated through literature reviews, the study’s conceptual framework and research questions, previous findings, pilot study fieldwork, the researcher’s prior knowledge and experiences, and researcher-formulated hypotheses or hunches.\nAs data are collected, coded and analyzed, provisional codes can be revised, modified, deleted or expanded.\nUseful in research that builds on or seeks to corroborate previous investigations.\nMay make use of keyword searches to locate terms and phrases that may correspond with the provisional codes.\n\n\n\nHypothesis coding\n\nThe application of a researcher-generated, predetermined list of codes specifically to assess a researcher’s hypothesis.\nThe codes are developed from a theory or prediction about what will be found in the data before they have been collected or analyzed.\nParticularly useful if seeking to identify rules, causes and explanations.\nCan also be applied midway or later in the analysis process to confirm or disconfirm any assertions or theories made thus far."
  },
  {
    "objectID": "notes/coding-methods.html#procedural-methods",
    "href": "notes/coding-methods.html#procedural-methods",
    "title": "Coding methods",
    "section": "Procedural methods",
    "text": "Procedural methods\nComprise pre-established coding systems or very specific ways of analyzing qualitative data.\n\nProtocol coding\n\nAlso referred to as “a priori coding”.\nA protocol is a detailed and specific set of procedural guidelines for conducting an experiment, administering a treatment or conducting field research and data analysis.\nA predefined list of codes and categories is provided to the researher and applied to the data after collection.\nThe defitions of all codes should be clear and inclusive of all possible types of responses to be collected.\nCould be useful when examining video-recorded data and focus group transcripts, especially in contexts with multiple coders.\n\n\n\nOutline of cultural materials coding\n\nA topical index for anthropologists and archaeologists, providing coding for the categories of social life that have traditionally been included in ethnographic description.\nServes to organize the database of the Human Relations Area Files (HLAF), which is a massive collection of ethnographic field notes about hundreds of world cultures collected over the past several decades.\nMay be especially useful for indexing, rather than coding, field notes.\n\n\n\nDomain and taxonomic coding\n\nA method for discovering the cultural knowledge people use to organize their behaviours and organize their experiences.\nWe call categories that categorize other categires domains, and the words that name them cover terms.\nTaxonomies are hierarchical lists of different things that are classified together under a domain word on the basis of some shared attributes.\nBetter to apply terms used by informants (referred to as folk terms) but when no term is offered then researchers may apply analytic terms.\nNine possible semantic relationships exist within domains (and which relate the items in the nested hierarchy):\n\nStrict inclusion: X is a kind of Y\nSpatial: X is a place in or a part of Y\nCause-effect: X is a result or cause of Y\nRationale: X is a reason for doing Y\nLocation for action: X is a place for doing Y\nFunction: X is used for Y\nMeans-end: X is a way to do Y\nSequence: X is a step in Y\nAttribution: X is an an attribute of Y\n\nStrict inclusion forms are generally nounce, and means-end forms are generally verbs.\nFor analysis, a semantic relationship is chosen, and the data are then reviewed to find examples of the semantic relationship.\nRelated forms and analytical terms are noted, and a folk taxonomy emerges as a set of categories organized on the basis of a single semantic relationship.\n\n\n\nCausation coding\n\nLocates, extracts and/or infers causal beliefs.\nAttempts to label the mental models participants use to uncover what people believe about events and their causes.\nAn attribution is an expression of the way a person things about the relationship between a cause and an outcome, and can consist of an event, action or characteristic.\nIt is important to remain focused on the individual who performs the action, rather than variables pertaining to the individual such as social class, sex, ethnicity, etc."
  },
  {
    "objectID": "notes/coding-methods.html#metasummary-and-metasynthesis",
    "href": "notes/coding-methods.html#metasummary-and-metasynthesis",
    "title": "Coding methods",
    "section": "Metasummary and metasynthesis",
    "text": "Metasummary and metasynthesis\n\nComprise systematic comparison of case studies to draw cross-case conclusions.\nThey reduce the accounts while preserving the esnse of the account through the selection of key metaphors and orhanizers, such as themes, concepts, ideas and perspectives.\nIt is the qualitative cousin of quantitative research’s meta-analysis.\nRelies on a researcher’s strategic collection and comparative analysis of themes that represent the essences and essentials of previous cases.\nIt is not a method designed to produce oversimplification; rather, it is one in which differences are trained and complexity enlightened.\nThe outcome will be more like a common understanding of the nature of a phenomenon, not a consensual worldview."
  },
  {
    "objectID": "notes/coding-methods.html#pattern-coding",
    "href": "notes/coding-methods.html#pattern-coding",
    "title": "Coding methods",
    "section": "Pattern coding",
    "text": "Pattern coding\n\nDeveleps the “meta code” — the category label that identifies similarly coded data.\nPattern codes don’t just organize the corpus, but also attempt to attribute meaning to that organization.\nAnalogous to cluster or factor analysis in quantitative analyses.\nPattern codes are explanatory or inferential codes in that they identify emergent themes, configurations or explanations.\nThey pull together material from first cycle coding into more meaningful and parsimonious units of analysis."
  },
  {
    "objectID": "notes/coding-methods.html#focused-coding",
    "href": "notes/coding-methods.html#focused-coding",
    "title": "Coding methods",
    "section": "Focused coding",
    "text": "Focused coding\n\nAlso sometimes referred to as selective coding or intermediate coding.\nTypically follows in vivo, process and/or initial coding.\nSearches for the most frequent or most significant codes to develop the most salient categories in the data corpus.\nRequires making decisions about which initial (open) codes make the most analytic sense.\nA more streamlined adaptation of axial coding; the goal is to develop categories without having to distract oneself with analysis of codes’ dimensions or properties.\nParticularly useful for comparing codes across different particpants’ data to assess their transferability."
  },
  {
    "objectID": "notes/coding-methods.html#axial-coding",
    "href": "notes/coding-methods.html#axial-coding",
    "title": "Coding methods",
    "section": "Axial coding",
    "text": "Axial coding\n\nExtends the analytical work from intial coding and focused coding.\nThe goal is to strategically reassemble data that were “split” or “fractured” during initial or open coding.\nAxial coding’s purpose is to determine which codes are dominant or less important and to reorganizing the dataset by crossing out synonyms, removing redundant codes, and selecting the best representative codes.\nThe “axis” of axial coding is like the axis on a wheel with wooden spokes that are the codes from the first cycle; axial coding aims to link categories with subcategories and ask how they are related, while also specifying the properties and dimensions of each category.\nProperties are characteristics or attributes, and dimensions are the location of a property along a continuum or range.\nHonestly I don’t really understand this and I agree with Charmaz’s characterization of it as overly cumbersome and systematic, in a way that stifles or suffocates gleanings obtained from first cycle methods."
  },
  {
    "objectID": "notes/coding-methods.html#theoretical-coding",
    "href": "notes/coding-methods.html#theoretical-coding",
    "title": "Coding methods",
    "section": "Theoretical coding",
    "text": "Theoretical coding\n\nAlso sometimes referred to as selective coding or conceptual coding.\nIn theoeretical coding, all categories and concepts become systematically integrated around a central or core category, which suggests a theoretical explanation for the whole phenomenon.\nThe theoretical code is not a theory itself, but an abstraction that models the integration; it is a key word or phrase that triggers a discussion of the theory itself.\nIf codes are the bones that form the skeleton of our analysis, then the central or core strategy is the spine that supports the corpus and aligns it.\nContinuous and detailed coding puts analytic meat on those bones."
  },
  {
    "objectID": "notes/coding-methods.html#elaborative-coding",
    "href": "notes/coding-methods.html#elaborative-coding",
    "title": "Coding methods",
    "section": "Elaborative coding",
    "text": "Elaborative coding\n\nThe process of analyzing data to develop theory further.\nSometimes framed as top-down because it begins coding with theoretical constructs from another study in mind.\nThe goal is to refine theoretical constructs from a previous study."
  },
  {
    "objectID": "notes/coding-methods.html#longitudinal-coding",
    "href": "notes/coding-methods.html#longitudinal-coding",
    "title": "Coding methods",
    "section": "Longitudinal coding",
    "text": "Longitudinal coding\n\nAttribution of selected change processes to data collected and compared across time."
  },
  {
    "objectID": "notes/coding-methods.html#footnotes",
    "href": "notes/coding-methods.html#footnotes",
    "title": "Coding methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSaldaña (2016: 144) notes that “change” is a slippery and contested term, and suggests using more precise terms like “impact”, “shift”, “transformation” andn”evolution” instead. This is just solid advice that applies in other contexts too.↩︎\nI think this is awkwardly situated in this book, and that it should have been treated as an overall coding strategy or workflow rather than as an interstitial method.↩︎"
  },
  {
    "objectID": "notes/caqdas.html",
    "href": "notes/caqdas.html",
    "title": "QDA Software",
    "section": "",
    "text": "Friese (2019) on the affordances of CAQDAS, including comparisson of various software. Distinguishes between QDA and GT, and responds to Glaser’s criticism of CAQDAS. Seems to also relate to Glaser’s (2003) critique of modern versions of GT as being overly descriptive. Gets into the distinction between coding and tagging, and may relate to Belgrave and Seide (2019). Includes an appendix that relates the terms used in various software with functional applications in a theoretical and methodological sense.\nGorra (2019) examines how CAQDAS is actually used in practice, and found that researchers never stay within the software’s boundaries. Comes down to how the software is being wielded as a tool. She found that data portability was a major concern. Emphasized importance of enabling researchers to interact with their data, and to do so in whatever ways best suit them. Enabling researchers to take advantages of the affordances of digital tech, specifically the ability to look at the data in different gestalts, sapes, forms or guises. Emphasies a need for researchers to tae ownership over their data (similar to how Saldana and Charmaz suggest). FLexibility enables enaction of what Becker referred to as the tricks of the trade.\nConcludes by identifying two different ways in which software could and should be used:\nHe specifically identifies specialist CAQDAS software as more suitable for the former, and other technologies in a more general sense for the latter.\nGorra (2019: 329) really emphasizes moving the data conciously between different media with the help of technology to see the data in different guises, shapes and formats.\nGorra (2019: 329) also aknowledges (and builds on prior related claims) that serendipity may have a larger role in GT, and suggests that “[s]erendipity can be encouraged by careful preparation, through the meticulous management of data – often supported by technology – but in addition what is also needed is the ‘creative free-play’ consisting of moving data and body.”1\nInaba and Kakai (2019) on text mining."
  },
  {
    "objectID": "notes/caqdas.html#footnotes",
    "href": "notes/caqdas.html#footnotes",
    "title": "QDA Software",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis may also relate to the notion of “Informed Grounded Theory” introduced by Thornberg (2012).↩︎"
  },
  {
    "objectID": "interview-protocol.html",
    "href": "interview-protocol.html",
    "title": "Interview Protocol",
    "section": "",
    "text": "Interviews may be held either in-person or through online video conference. All interviews will be held in quiet and comfortable environments, such as office spaces or conference rooms.\nI will record all in-person interviews using a SONY ICD-UX560 audio recorder to capture audio in the lossless 16 bit 44.1 kHz Linear PCM wav format, with additional audio filters to enhance playback during transcription, if necessary.\nI will also record in-person interview sessions using a GoPro Hero 4 Silver action camera, depending on participants willingness to be video recorded. Based on prior interviews with scientists about their research experiences, I found that interviewees like to show me, rather than merely tell me, about what they are working on and the means through which they engage with information systems. The camera may be leveraged to record spontaneous video records of these demonstrations and provide me with an additional rich data source for further analysis. Moreover, the camera provides an additional backup audio recording in case of data loss on the primary recording device.\nRemote interviews will be recorded using the video conferencing software’s built-in recording tools. Participants will be instructed to disable their microphones or video cameras prior to initiating recording if they have opted to not be recorded through these media. I will record all media locally and refrain from using any cloud services to store or modify the records which the video conference software may provide.\nI will also record handwritten or typed notes comprising descriptive accounts of activities and interactions when recording devices are switched off, as well as preliminary interpretations of observed behaviours and notes on things I plan to follow up on at a later time.\nAll records, including audio and video records, notes, transcripts, interview guides and planning documents are kept confidential until consent has been granted to release materials relating to participants’ responses.",
    "crumbs": [
      "Interview Protocol"
    ]
  },
  {
    "objectID": "interview-protocol.html#participants-goals-and-perspectives",
    "href": "interview-protocol.html#participants-goals-and-perspectives",
    "title": "Interview Protocol",
    "section": "1. Participants’ goals and perspectives",
    "text": "1. Participants’ goals and perspectives\nFollow a life-history method to better understand participants’ professional backgrounds and their roles within their projects. The goal is to obtain information about their paths, not the rehearsed origin story.\n\nTo start, can you please tell me a little bit about your background?\nWhat is the project, and what is your role?\nHow did you find yourself in this role?\nHow has your previous experience prepared you for your role?",
    "crumbs": [
      "Interview Protocol"
    ]
  },
  {
    "objectID": "interview-protocol.html#projects-missions-purposes-motivations",
    "href": "interview-protocol.html#projects-missions-purposes-motivations",
    "title": "Interview Protocol",
    "section": "2. Projects’ missions, purposes, motivations",
    "text": "2. Projects’ missions, purposes, motivations\nThis section is about the project in general, including its purpose, scope and value. Information about practices and procedures will be sought in a subsequent phase of the interview.\n\nWhat are the project’s goals?\nWhat makes the project unique?\nWhat is the project doing that no other similar project is doing?\nDo you consider this project as similar to any other initiatives?\nWhat are they, and in what ways are they similar or different?\n\n\n\nWhat are the expected outcomes?\nHave you achieved these goals and outcomes?\nIf not, are you on track to achieving them?\nWhat are some challenges that the project experienced, and how have you worked to overcome them?",
    "crumbs": [
      "Interview Protocol"
    ]
  },
  {
    "objectID": "interview-protocol.html#practices-procedures-relationships",
    "href": "interview-protocol.html#practices-procedures-relationships",
    "title": "Interview Protocol",
    "section": "3. Practices, procedures, relationships",
    "text": "3. Practices, procedures, relationships\nThis section asks about specific actions and interactions that the participant engages in.\n\nRoles and relationships\n\nWhat does your role entail?\nCan you provide a couple examples of things that you recently did in this capacity?\n\n\n\nWho else do you frequently rely on, and what are their roles?\nCan you describe what they do, and perhaps give a few examples drawn from their recent work?\n\nThe interview might proceed in different ways depending on their initial responses. Here are some questions I might ask, corresponding with the participants’ role and area of expertise.\n\n\nMaintaining the project community\n\nPlease briefly describe the process through which you obtain new partners or users.\nCan you please recall a recent example?\n\n\n\nHow well do you know each partner?\nDid you know them before their involvement?\n\n\n\nWould you describe the project as a tight knit community, or more open-ended?\n\n\n\nHow do you communicate with partners and contributors?\nWhat kinds of media or platforms do you use, and are they targeted for specific purposes? i.e. email, newsletters, social media, skype, personal communication at conferences\n\n\n\nAre there particular people in each project who you communicate with more frequently than others?\nWho are they, and why are these the people who you connect with?\n\n\n\nWhat do you consider your role or responsibility vis-a-vis the development/growth of this community?\nHow do you foster the community’s development and growth?\nDo you consider these efforts to be effective?\n\n\n\nDoes your role as someone who leads a data harmonization initiative differentiate you from other epidemiologists?\nHow has your relationship with other epidemiologists changed after initiating this project and taking on this role?\n\n\n\nReflections on data’s value\n\nHow has the data been used or analyzed?\nDo you track how the data is used?\nIs this tracking formal or informal?\n\n\n\nWhat patterns or trends are apparent based on this tracking?\nIn your view, has the data been used in productive ways?\nIn what ways are people either maximizing or not fully utilizing the data’s full potential?\n\n\n\nCan you tell me about any unexpected or alternative uses of the data?\nWhat made them significant to you?\n\n\n\nWhich skills and competencies do you think researchers need to possess in order to be able to make proper use of the data in their work?\n\n\n\nBased on your experience, what are the main obstacles for the effective and widespread adoption of these skills?\nWhat are some positive factors, or drivers, that can make that prospect more tangible?\n\n\n\nData ownership\n\nWho has rights (in the legal sense or informally) over the information contained in the system, or in related documents and datasets?\nCan you tell me about any conflicts or tensions that emerged relating to expressions of propriety or ownership over data?\n\n\n\nCollecting data\n\nDo projects collect data with future harmonization in mind?\nIf so, how does this affect data collection procedures, and does this play a role in subsequent decision-making?\n\n\n\nCurating data\n\nPlease describe the overall process of getting data into the system and then working with the data.\n\n\n\nPlease tell me about any unexpected or problematic cases that made working with data particularly challenging.\nWhat made these cases unique or challenging?\nHow did you resolve them or work towards a solution or viable outcome?\n\n\n\nAccessing data\n\nDo you consider the system easy to access?\nCan you identify some challenges that pose as barriers to access?\n\n\n\nWho has access to data?\nHow are decisions regarding access rights made?\nCan you tell me about any unnaceptable practices regarding accessing and sharing data?\n\n\n\nUsing data\n\nIf you engage with the data with specific questions in mind, how do these questions emerge?\nWhat role does the data play in shaping the questions and analytical approach?\n\n\n\nIs the system amenable to exploratory or serendipitous modes of discovery?\nPlease tell me about specific examples where you engaged with the data in this way.\n\n\n\nWhat features does the system have to view or export data?\nHow easy is it to view, export or visualize data the data?\nDo you use the tools that are designed to export of visualize data, or do you prefer to use your own tooling?\nWhat are the reasons behind this preference?\n\n\n\nDocumentation\n\nHow is the system documented?\nWho is responsible for creating documentation?\nCan you please tell me about a great example of documentation in your project?\n\n\n\nOverall, do you consider your project’s documentation to be helpful?\nWhy or why not?\n\n\n\nIn your opinion, does the documentation accurately reflect the true nature of the documented data or work practices?\nAre specific things more accurately documented than others?\nPlease tell me why you think some things are more accurately or less accurately documented.\n\n\n\nCan you recall any instances when documentation was updated?\nWhat prompted these updates?\n\n\n\nDo people ever get in touch to ask questions about specific aspects of the data or data curation procedures?\nWhat kinds of questions do they ask?\nWhat kinds of responses are given?\n\n\n\nRelationships with Maelstrom\n\nCan you please concisely describe the role of Maelstrom as part of your project’s overall initiative?\n\n\n\nWhat are the origins of your project’s relationship with Maelstrom?\nHow has this relationship changed over time?\n\n\n\nDoes your project present any unique challenges or require any special attention?\nIf so, please tell me about some unique cases or examples that demonstrate this unique relationship.\n\n\n\nDo you believe that Maelstrom is meeting your project’s needs and enabling it to achieve its goals?\nIn what ways is Maelstrom either satisfying or failing to meet your project’s expectations or needs?\nHow would you change the current system to better suit your project’s needs more effectively?\n\n\n\nDo you engage with Maelstrom’s other partners?\nIf so, what is the nature of these relationships?",
    "crumbs": [
      "Interview Protocol"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "data-sharing\nXXXX.\n\ndata-harmonization\nXXXX.\n\ndata-integration\nXXXX.\n\ncollaboration\nXXXX.\n\ndata-sharing initiative\nXXXX.\nCorresponds with the term “harmonization initiative” in Fortier et al. (2017).\ncatalogue\nXXXX.\nBergeron et al. (2018), Bergeron et al. (2021)\n\n\n\n\nReferences\n\nBergeron, Julie, Dany Doiron, Yannick Marcon, Vincent Ferretti, and Isabel Fortier. 2018. “Fostering Population-Based Cohort Data Discovery: The Maelstrom Research Cataloguing Toolkit.” PLOS ONE 13 (7): e0200926. https://doi.org/10.1371/journal.pone.0200926.\n\n\nBergeron, Julie, Rachel Massicotte, Stephanie Atkinson, Alan Bocking, William Fraser, Isabel Fortier, and the ReACH member cohorts’ principal investigators. 2021. “Cohort Profile: Research Advancement Through Cohort Cataloguing and Harmonization (ReACH).” International Journal of Epidemiology 50 (2): 396–97. https://doi.org/10.1093/ije/dyaa207.\n\n\nFortier, Isabel, Parminder Raina, Edwin R Van den Heuvel, Lauren E Griffith, Camille Craig, Matilda Saliba, Dany Doiron, et al. 2017. “Maelstrom Research Guidelines for Rigorous Retrospective Data Harmonization.” International Journal of Epidemiology 46 (1): 103–5. https://doi.org/10.1093/ije/dyw075.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "data/index.html",
    "href": "data/index.html",
    "title": "Data",
    "section": "",
    "text": "This is a where I keep interview recordings and transcripts, as well as memos and notes about the cases. These files are kept on a private git repo administered and hosted by the MCHI, which is made accessible to the working environment through a submodule. The data shall remain private until I obtain consent to share elicitations and/or finish anonymizing their responses."
  },
  {
    "objectID": "context.html",
    "href": "context.html",
    "title": "Context",
    "section": "",
    "text": "I am Zack Batist — a postdoctoral researcher at McGill University, in the School of Global and Public Health’s Department of Epidemiology, Biostatistics and Occupuational Health. I’m working with David Buckeridge, who leads the Covid-19 Immunity Task Force (CITF) Databank, to investigate data sharing in epidemiological research — with an emphasis on the practical and situated experiences involved in data sharing.\nThe CITF is a “data harmonization” initiative, which entails coordinating a systematic effort to align the information contained in datasets collected by distributed teams of epidemiologists. These efforts to integrate the records collected during various discrete studies are motivated by a desire to establish larger integrated datasets bearing greater statistical power and that facilitate comparison across cohorts. However, epidemiologists must reckon with the diversity of minor variations in data collection procedures, as well as ethico-legal concerns relating to the sharing of individual health records pertaining to human research subjects across numerous institutional and regional jurisdictions.\nAs a scholar of scientific practice, with a primary interest in data-sharing and the formation of information commons, data harmonization represents a fascinating mechanism through which scientists derive technical, administrative, social and epistemic frameworks to enhance the value of their collective endeavours in response to disciplinary needs, warrants, desires and expectations. This study therefore articulates the motivations for doing data harmonization, identifies how value is ascertained, and describes the strategies employed to achieve the desired goals — including perceived and actual challenges, setbacks, opportunities, realizations, and lessons learned.\nThis relates to my previous work that (a) explores tensions that arise when attempting to establish information commons in archaeology, specifically relating to inability to cope with a superficial perception of data’s stability and an intuitive understanding of their situated nature; and that (b) investigates how the open science movement attempts (and fails) to reshape practices relating to data sharing, integration and reuse. I continue in my approach that frames data-sharing — whether it occurs in relatively “closed” curcumstances between close colleagues, or as mediated by open data platforms among strangers — as comprising a series of collaborative commitments that govern who may contribute to and obtain value from the information commons, and in what ways.",
    "crumbs": [
      "Context"
    ]
  },
  {
    "objectID": "MCHI-2025/slides.html#outline",
    "href": "MCHI-2025/slides.html#outline",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "Outline",
    "text": "Outline\n\nMy background\nMy objectives\nPlan of action\nQuestions and feedback?"
  },
  {
    "objectID": "MCHI-2025/slides.html#about-me",
    "href": "MCHI-2025/slides.html#about-me",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "About me",
    "text": "About me\n\n\n\n\nPalaeolithic/Neolithic of Greece / Cyprus / Near East\n\n\n\n\nManaged archaeological project databases\n\n\n\n\nComputational methods using integrated datasets\n\n\n\n\nResearch about archaeological data work\n\n10.1515/opar-2024-0014\nzackbatist.info/locating-creative-agency\nzackbatist.info/fuzzy-concrete\n\n\n\n\n\nResearch about academic open source software development\n\nopen-archaeo.info: a list of archaeological software\n10.11141/ia.67.13\n\n\n\n\n\n\n\nzackbatist.info | archaeo.social/@zackbatist"
  },
  {
    "objectID": "MCHI-2025/slides.html#about-me-1",
    "href": "MCHI-2025/slides.html#about-me-1",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "About me",
    "text": "About me\n\n\n\n\nScholar of scientific practice\n\n\n\n\nSpecific interests:\n\ndata sharing\ndata documentaion\ninformation commons\nresearch infrastructure\nresearch software\nalternative publishing\ncollaborative experiences\nopen science in practice\n\n\n\n\n\n\n\nAdapted from xkcd.com/1838\n\n\n\n\nzackbatist.info | archaeo.social/@zackbatist"
  },
  {
    "objectID": "MCHI-2025/slides.html#what-am-i-doing-here",
    "href": "MCHI-2025/slides.html#what-am-i-doing-here",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "What am I doing here?",
    "text": "What am I doing here?\n\n\nFraming harmonization as information commons\n\nWho contributes? Who extracts?\nWhat are their converging and diverging interests?\nWhat commitments do these interactions entail?\n\n\n\n\n\nFraming harmonization as value-driven, improvized action\n\nWhat are the targeted outcomes?\nHow do they go about trying to achieve these objectives?\nAre their strategies successful? Why or why not?\n\n\n\n\n\nRevealing tensions between:\n\nnaive collective imaginations about data in technical and administrative systems\nthe messy, pragmatic and collaborative reality of data work in practice\n\n\n\n\nzackbatist.info/CITF-Postdoc"
  },
  {
    "objectID": "MCHI-2025/slides.html#interviews",
    "href": "MCHI-2025/slides.html#interviews",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "Interviews",
    "text": "Interviews\n\n\nInterviews with leaders and key stakeholders of data harmonization initiatives\n\n\n\n\nI will ask participants about:\n\nThe motivations for their initiatives\nThe challenges they experience\nHow they envision success and failure\nPerceptions of their own roles and the roles of others\nThe values that inform their decisions\nThe systems that enable them to achieved their goals\nWays in which they believe data sharing could be improved\n\n\n\n\nzackbatist.info/CITF-Postdoc/interview-protocol"
  },
  {
    "objectID": "MCHI-2025/slides.html#qualitative-data-analysis",
    "href": "MCHI-2025/slides.html#qualitative-data-analysis",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "Qualitative Data Analysis",
    "text": "Qualitative Data Analysis\n\n\n\n\nCoding interview transcripts\n\nEnables me to develop theory about social actions\n\n\n\n\n\nWriting analytic memos\n\nTo synthesize broader concepts, themes and theories based on the encoded data\n\n\n\n\n\nInformed by broader theoretical frameworks\n\nAbout scientific practice, research infrastructure, collaborative experiences, information commons\n\n\n\n\n\nTheory is therefore “grounded” in the data\n\n\n\n\n\n\nCodes applied to an interview transcript from my prior research."
  },
  {
    "objectID": "MCHI-2025/slides.html#sampling",
    "href": "MCHI-2025/slides.html#sampling",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "Sampling",
    "text": "Sampling\n\n\nTheoretical sampling\n\nSample emerges in response to ongoing theory-building\n\n\n\n\n\nClustered into cases\n\nCases are data harmonization initiatives\nEach constitute their own sets of goals and circumstances\nLeveraging overlap and difference to support comparison\n\n\n\n\n\nNot generalizable, but representative of work occurring under similar circumstances\n\n\n\n\nDoesn’t have to be fully generalizable to inform practical action"
  },
  {
    "objectID": "MCHI-2025/slides.html#current-work",
    "href": "MCHI-2025/slides.html#current-work",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "Current Work",
    "text": "Current Work\n\n\nFirst interview next week\n\n\n\n\nCreating “situational maps”\n\nIdentifying “elements” and potential relationships\nRelating my own understanding to interviewees’ persectives\nWriting memos about these elements"
  },
  {
    "objectID": "MCHI-2025/slides.html#questions-comments",
    "href": "MCHI-2025/slides.html#questions-comments",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "Questions / Comments ?",
    "text": "Questions / Comments ?\n\nProject Website:\n\nzackbatist.info/CITF-Postdoc\n\nSlides:\n\nzackbatist.info/CITF-Postdoc/MCHI-2025/\n\nGitHub:\n\ngithub.com/zackbatist/CITF-Postdoc"
  },
  {
    "objectID": "MCHI-2025/index.html",
    "href": "MCHI-2025/index.html",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "",
    "text": "My background\nMy objectives\nPlan of action\nQuestions and feedback?"
  },
  {
    "objectID": "MCHI-2025/index.html#outline",
    "href": "MCHI-2025/index.html#outline",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "",
    "text": "My background\nMy objectives\nPlan of action\nQuestions and feedback?"
  },
  {
    "objectID": "MCHI-2025/index.html#about-me",
    "href": "MCHI-2025/index.html#about-me",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "About me",
    "text": "About me\n\n\n\n\nPalaeolithic/Neolithic of Greece / Cyprus / Near East\n\n\n\n\nManaged archaeological project databases\n\n\n\n\nComputational methods using integrated datasets\n\n\n\n\nResearch about archaeological data work\n\n10.1515/opar-2024-0014\nzackbatist.info/locating-creative-agency\nzackbatist.info/fuzzy-concrete\n\n\n\n\n\nResearch about academic open source software development\n\nopen-archaeo.info: a list of archaeological software\n10.11141/ia.67.13\n\n\n\n\n\n\n\n\nzackbatist.info | archaeo.social/@zackbatist"
  },
  {
    "objectID": "MCHI-2025/index.html#about-me-1",
    "href": "MCHI-2025/index.html#about-me-1",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "About me",
    "text": "About me\n\n\n\n\nScholar of scientific practice\n\n\n\n\nSpecific interests:\n\ndata sharing\ndata documentaion\ninformation commons\nresearch infrastructure\nresearch software\nalternative publishing\ncollaborative experiences\nopen science in practice\n\n\n\n\n\n\n\nAdapted from xkcd.com/1838\n\n\n\n\n\nzackbatist.info | archaeo.social/@zackbatist"
  },
  {
    "objectID": "MCHI-2025/index.html#what-am-i-doing-here",
    "href": "MCHI-2025/index.html#what-am-i-doing-here",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "What am I doing here?",
    "text": "What am I doing here?\n\n\nFraming harmonization as information commons\n\nWho contributes? Who extracts?\nWhat are their converging and diverging interests?\nWhat commitments do these interactions entail?\n\n\n\n\n\nFraming harmonization as value-driven, improvized action\n\nWhat are the targeted outcomes?\nHow do they go about trying to achieve these objectives?\nAre their strategies successful? Why or why not?\n\n\n\n\n\nRevealing tensions between:\n\nnaive collective imaginations about data in technical and administrative systems\nthe messy, pragmatic and collaborative reality of data work in practice\n\n\n\n\nzackbatist.info/CITF-Postdoc"
  },
  {
    "objectID": "MCHI-2025/index.html#interviews",
    "href": "MCHI-2025/index.html#interviews",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "Interviews",
    "text": "Interviews\n\n\nInterviews with leaders and key stakeholders of data harmonization initiatives\n\n\n\n\nI will ask participants about:\n\nThe motivations for their initiatives\nThe challenges they experience\nHow they envision success and failure\nPerceptions of their own roles and the roles of others\nThe values that inform their decisions\nThe systems that enable them to achieved their goals\nWays in which they believe data sharing could be improved\n\n\n\n\nzackbatist.info/CITF-Postdoc/interview-protocol"
  },
  {
    "objectID": "MCHI-2025/index.html#qualitative-data-analysis",
    "href": "MCHI-2025/index.html#qualitative-data-analysis",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "Qualitative Data Analysis",
    "text": "Qualitative Data Analysis\n\n\n\n\nCoding interview transcripts\n\nEnables me to develop theory about social actions\n\n\n\n\n\nWriting analytic memos\n\nTo synthesize broader concepts, themes and theories based on the encoded data\n\n\n\n\n\nInformed by broader theoretical frameworks\n\nAbout scientific practice, research infrastructure, collaborative experiences, information commons\n\n\n\n\n\nTheory is therefore “grounded” in the data\n\n\n\n\n\n\nCodes applied to an interview transcript from my prior research."
  },
  {
    "objectID": "MCHI-2025/index.html#sampling",
    "href": "MCHI-2025/index.html#sampling",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "Sampling",
    "text": "Sampling\n\n\nTheoretical sampling\n\nSample emerges in response to ongoing theory-building\n\n\n\n\n\nClustered into cases\n\nCases are data harmonization initiatives\nEach constitute their own sets of goals and circumstances\nLeveraging overlap and difference to support comparison\n\n\n\n\n\nNot generalizable, but representative of work occurring under similar circumstances\n\n\n\n\nDoesn’t have to be fully generalizable to inform practical action"
  },
  {
    "objectID": "MCHI-2025/index.html#current-work",
    "href": "MCHI-2025/index.html#current-work",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "Current Work",
    "text": "Current Work\n\n\nFirst interview next week\n\n\n\n\nCreating “situational maps”\n\nIdentifying “elements” and potential relationships\nRelating my own understanding to interviewees’ persectives\nWriting memos about these elements"
  },
  {
    "objectID": "MCHI-2025/index.html#questions-comments",
    "href": "MCHI-2025/index.html#questions-comments",
    "title": "Articulating data harmonization as pragmatic, collaborative experiences",
    "section": "Questions / Comments ?",
    "text": "Questions / Comments ?\n\nProject Website:\n\nzackbatist.info/CITF-Postdoc\n\nSlides:\n\nzackbatist.info/CITF-Postdoc/MCHI-2025/\n\nGitHub:\n\ngithub.com/zackbatist/CITF-Postdoc"
  },
  {
    "objectID": "case-selection.html",
    "href": "case-selection.html",
    "title": "Case Selection",
    "section": "",
    "text": "Note\n\n\n\nThis document is still a work in progress.",
    "crumbs": [
      "Case Selection"
    ]
  },
  {
    "objectID": "case-selection.html#case-studies",
    "href": "case-selection.html#case-studies",
    "title": "Case Selection",
    "section": "Case Studies",
    "text": "Case Studies\nIn case-study research, cases represent discrete instances of a phenomenon that inform the researcher about it. The cases are not the subjects of inquiry, and instead represent unique sets of circumstances that frame or contextualize the phenomenon of interest (Stake 2006: 4-7). Cases usually share common reference to the overall research themes, but exhibit variations that enable a researcher to capture different outlooks or perspectives on matters of common concern. Drawing from multiple cases thus enables comprehensive coverage of a broad topic that no single case may cover on its own (Stake 2006: 23). In other words, the power of case study research derives from identifying consistencies that relate cases to each other, while simultaneously highlighting how their unique and distinguishing facets contribute to their representativeness of the underlying phenomon. Case study research therefore plays on the tensions that challenge relationships among cases and the phenomenon that they are being called upon to represent (Ragin 1999: 1139-1140).",
    "crumbs": [
      "Case Selection"
    ]
  },
  {
    "objectID": "case-selection.html#theoretical-sampling",
    "href": "case-selection.html#theoretical-sampling",
    "title": "Case Selection",
    "section": "Theoretical Sampling",
    "text": "Theoretical Sampling\nThe number of cases reflects the capacity to draw adequate comparison across unique circumstances while also complementing the meaningful number of individuals who may serve as interview participants. Breadth of perspective is my primary concern; the goal is to articulate the series of inter-woven factors that impact how epidemiological researchers coordinate and participate in data-sharing initiatives while explicitly accounting for and drawing from the unique and situational contexts that frame each case. The goal is not to define causal relationships or to derive generalizable findings, but rather to draw meaning from situations that many epidemiologists identify with.\nI adhere to a theoretical sampling strategy, which effectively entails developing the sample in response to ongoing theory-building. Theoretical sampling focuses on finding new data sources that can best address specific theoretical facets of an emerging analysis. The goal is to eventually reach a point of saturation, when new data fit into the emergent model with ease (Charmaz 2000: 519-520). This sampling strategy enables me to access new dimensions on a topic that arise during reflexive inquiry (Morse and Clark 2019: 146).",
    "crumbs": [
      "Case Selection"
    ]
  },
  {
    "objectID": "case-selection.html#key-factors",
    "href": "case-selection.html#key-factors",
    "title": "Case Selection",
    "section": "Key Factors",
    "text": "Key Factors\nTo reiterate, this project investigates the social and collaborative apparatus that scaffold data-sharing initiatives in epidemiology. Through analysis of data obtained through interviews with various relevant stakeholders attached to data-sharing initiatives, the project will ascertain the actions taken and challenges experienced to mediate the varied motivations, needs and values of those involved. In effect, the project aims to articulate the collaborative commitments that govern the constitution and maintenance of epidemiological information commons, and to relate these to technological, administrative and epistemic factors.\nIn other words, I aim to make certain under-appreciated social and collaborative commitments that underlie data-sharing initiatives more visible and to draw greater attention to certain sensibilities, attitudes, and apprehensions that are relevant to contemporary discourse on the nature of epidemiological data and ongoing development of information infrastructures designed to support data integration and re-use.\nHere I outline some key factors that will guide the selection of cases so as to ensure that the project meaningfully addressses its goals.\n\nLongevity\nInitiatives that have existed for different durations of time will have different capacity to reflect on their practices. Younger projects will not have had as much of a chance to produce any research outcomes, but may be valuable sources for insight on expectations. More established projects will be able to reflect on unexpected challenges they may have experienced.\nIt will be good to have at least one younger project representing an initiative still “in flux”, one or two “legacy” projects (no longer active), and one or two at intermediate stages (extracting data for meaningful analysis, expanding the initiative’s scope, etc).\n\n\nCommunity composition\nThe size and composition of the community, degree of familiarity among its members, and the mechanisms through which connections are managed constitute additional important factors to consider. Communication and decision-making may take different forms when teams are either smaller and locally-concentrated or larger and dispersed. Decision-making may also be significantly impacted by diffferent governance models and degrees of community participation. It would be interesting to identify how leaders are differentiated from other participants, norms and expectations for getting involved in leadership positions, and considerations that are made when making decisions that impact the community.\n\n\nSupport structures\nData-sharing may be supported by diverse funding models or tech stacks to support the work, which may significantly impact how the work progresses. Comparing sources of support for data-sharing will help me to explore how data-sharing is either integrated into or supplemented as a distinct outgrowth of “normal” science.\nSpecifically, it will be interesting to compare the extent to which projects are left to cobble together their own data-sharing infrastucture, and how this impacts attitudes and norms regarding the curation and nature of research data. I wonder whether lack of government support fosters creative, entrepeneurial, experimental or community-led models, how funding is provided to supporting the development of collaborative research networks, and how these feed back into norms and attitudes regarding the independence of individual research projects and the formation of collectively-maintained information commons.1\n1 There is some precedent for this in the social sciences and humanities, which are fields that open science policies and infrastructures are not really designed to handle. This marginizaliation had contributed to experimentation with community-based governance models (as per the Radical Open Access Collective) and broader community involvement in policy decisions concerning how the rich diversity of social science and humanities data should be curated.I expect a tendency for cases to be supported by limited-term, federally-funded grants, though it might be worth exploring how supplementary funding provided by non-government agencies, including private firms (through MITACS, for instance) and philanthropic organizations (such as the Gates Foundation) impact the work. I would therefore like to included cases funded through these kinds of initiatives in this project.\n\n\nDisciplinary trends\nData-sharing is undoubtably impacted by attitudes concerning the nature of data and their roles in scientific knowledge production, and it is therefore necessary to account for different perspectives. Although I am still somewhat unfamiliar with the diversity of thought on such matters in epidemiology, I intuit that much of the open science movement is driven by rather positivist attitude. I would like to include cases that take on alternative approaches to science.\n\n\nHistorical or contextual factors\nScience is beholden to political trends, which impact ability to obtain funding and collaborate accross borders (e.g. Brexit’s impact on trans-European funding, including initiatives to attract and retain talent). Moreover, certain events, such as the Covid-19 pandemic, trigger responses in the scientific community. Even if these events are not the focus of the research, they must still be accounted for due to their presumed impacts.\n\n\nKinds of data\nThe nature of the data will surely impact how they are shared. In epidemiology specifically, there are ethical limitations on sharing precise patient records. This may be especially salient in studies focusing in health in Indiginous populations, which may involve additional consideration in contexts of data-sharing.2 Moreover, controls on data collection procedures, including limited or controlled scope or decisions to account for specific factors (such as race, which is prevalent in American datasets but largely ignored elsewhere) may significantly impact what can be done with them when integrated at scale.\n2 The Data Governance and Management Toolkit for Self-Governing Indigenous Governments https://indigenousdatatoolkit.ca may be helpful for exploring these concerns, but I am still looking for epidemiologically-oriented resources on such matters.",
    "crumbs": [
      "Case Selection"
    ]
  },
  {
    "objectID": "case-selection.html#selection-process",
    "href": "case-selection.html#selection-process",
    "title": "Case Selection",
    "section": "Selection Process",
    "text": "Selection Process\nI rely on structured consultations with the research community to make sense of the data-sharing landscape and select initial cases accordingly. This helps me arrive at a general understanding of which cases are worth approaching while documenting the rationale behind these selections.\nAs the project evolves, I adhere to theoretical sampling to continually refine my sources of data. This ensures that the project may most effectively address specific theoretically interesting facets of the emergent analysis, in accordance with grounded theory principles.\nI will continue to consult with community experts as I make decisions to alter my sample. This will ensure that the case selection adheres to community will and reasoning, while also ensuring that cases are logistically feasible. These decisions and the reasoning behind them will be clearly documented in my analytical memos.",
    "crumbs": [
      "Case Selection"
    ]
  },
  {
    "objectID": "case-selection.html#fixed-cases",
    "href": "case-selection.html#fixed-cases",
    "title": "Case Selection",
    "section": "Fixed Cases",
    "text": "Fixed Cases\nMaelstrom will serve as a “fixed point” that limits the scope of the cases’ breadth, while also ensuring that participants (and myself) have a common frame of reference. Moreover, the practices and values that support Maelstrom’s operations have already been documented to a certain extent by its leaders (cf. Doiron et al. 2017; Fortier et al. 2017; Fortier et al. 2023; Bergeron et al. 2018), by its partners (cf. Doiron et al. 2013; Wey et al. 2021; Bergeron et al. 2021) and by scholars of scientific practice (cf. M. J. Murtagh et al. 2012; Demir and Murtagh 2013; Madeleine J. Murtagh et al. 2016; Tacconelli et al. 2022; Gedeborg et al. 2023). This prior work will serve as valuable resources supporting this project.\nAdditionally, the fact that all cases interact with Maelstrom for their technical infrastructure will greatly simplify the interviews by reducing the “overhead” of having to learn or be told about the technical systems, which may distract from the primary themes I seek to address during interviews.",
    "crumbs": [
      "Case Selection"
    ]
  },
  {
    "objectID": "case-selection.html#logistical-constraints-and-sources-of-bias",
    "href": "case-selection.html#logistical-constraints-and-sources-of-bias",
    "title": "Case Selection",
    "section": "Logistical Constraints and Sources of Bias",
    "text": "Logistical Constraints and Sources of Bias\nAfter identifying potential cases, I will reach out to project leaders to invite them to participate. I will prepare a document outlining this project’s objectives and the roles that cases will play in the work. I will also set up a meeting prior to them deciding whether they would like to participate so I can ascertain whether they understand the project and to help determine who may serve as people who can sit for interviews (I expect to hold 12-15 interviews ranging between 60-90 minutes in duration).\nI may prioritize local connections, which provide favourable conditions for holding interviews (i.e., people are more willing to show things that can not be conveyed through a screen, and the pre- and post-interview phases provide meaningful insight). This may introduce bias in that I may obtain more in-depth and nuanced information from local initiatives than those occurring abroad. This can be mitigated by travelling to conduct interviews in person, however the costs of travel may introduce their own biases favouring cases that are easier to reach.",
    "crumbs": [
      "Case Selection"
    ]
  },
  {
    "objectID": "data-management.html",
    "href": "data-management.html",
    "title": "Data Management Plan",
    "section": "",
    "text": "This project relies on various data sources, including interviews, bibliographic resources and structured datasets.\nInterviews generate audio, video and textual resources, formatted according to the WAV, MP4 and markdown specifications, respectively.\nWhile reviewing extant literature, I compile numerous published PDF and HTML documents. I maintain a bibliographic database using Zotero and continually export core information to BibLaTeX format using the Better BibTeX add-on.\nI prioritize writing all documents using plaintext file formats. I rely on Quarto, an open-source scientific and technical publishing system, to generate HTML, PDF and DOCX files from singular markdown files via the pandoc conversion egnine and the LaTeX typesetting system.\nWhen collecting structured data, I opt to use open text formats including CSV, JSON and XML, however I may also use XLSX format to maintain interoperability with collaborators.\nI embed all original code, including database retrieval queries and API requests, in or alongside quarto documents, where I annotate code with detailed and contextualizing comments.\nI use qc for my qualitative data analysis, which stores data across a series of text files and a barebones SQLite database.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "data-management.html#kinds-of-data",
    "href": "data-management.html#kinds-of-data",
    "title": "Data Management Plan",
    "section": "",
    "text": "This project relies on various data sources, including interviews, bibliographic resources and structured datasets.\nInterviews generate audio, video and textual resources, formatted according to the WAV, MP4 and markdown specifications, respectively.\nWhile reviewing extant literature, I compile numerous published PDF and HTML documents. I maintain a bibliographic database using Zotero and continually export core information to BibLaTeX format using the Better BibTeX add-on.\nI prioritize writing all documents using plaintext file formats. I rely on Quarto, an open-source scientific and technical publishing system, to generate HTML, PDF and DOCX files from singular markdown files via the pandoc conversion egnine and the LaTeX typesetting system.\nWhen collecting structured data, I opt to use open text formats including CSV, JSON and XML, however I may also use XLSX format to maintain interoperability with collaborators.\nI embed all original code, including database retrieval queries and API requests, in or alongside quarto documents, where I annotate code with detailed and contextualizing comments.\nI use qc for my qualitative data analysis, which stores data across a series of text files and a barebones SQLite database.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "ethics-protocol.html",
    "href": "ethics-protocol.html",
    "title": "Ethics Protocol",
    "section": "",
    "text": "Project Title: Articulating epidemiological data harmonization initiatives as practical and collaborative experiences\nSubmitted Materials: zackbatist.info/CITF-Postdoc/irb-docs.pdf\nPrincipal Investigator: Zachary Batist\nProtocol: 25-01-057\nSubmitted: 2025-01-30\nApproved: 2025-03-03\nAmended: 2025-06-17",
    "crumbs": [
      "Ethics Protocol"
    ]
  },
  {
    "objectID": "ethics-protocol.html#recruitment-and-consent",
    "href": "ethics-protocol.html#recruitment-and-consent",
    "title": "Ethics Protocol",
    "section": "Recruitment and consent",
    "text": "Recruitment and consent\nWill this study involve recruitment of human study participants?\n\nYes\nNo\n\nHow are potential study participants identified and/or recruited to the study? Explain how potential participants are identified or introduced to the study, and who will recruit participants. Will the investigator/s require any special permissions or access to the target population e.g. clinic access, patient registries or records, mailing lists, community access?\nThe study draws from semi-structured interviews with 10-15 individuals from a single case: the Covid-19 Immunity Task Force (CITF) Databank. The CITF was an initiative whose mandate was to catalyze, support, fund and harmonize knowledge on SARS-CoV-2 immunity for federal, provincial, and territorial decision-makers in their efforts to protect Canadians and minimize the impact of the COVID-19 pandemic. The CITF Databank provides continued support to the research community by centralizing research data provided by CITF-funded studies and by making the data accessible for extended research. This study draws from interviews with individuals who lead, support or participate in the CITF Databank’s operations, including professional researchers, research trainees and administrative and technical support staff.\nThe case was selected partially out of convenience, but this does not discount the prospective analytical value that it affords. As a community-led initiative, the CITF Databank presents an opportunity to investigate how epidemiologists balance the values deriving from their own epistemic culture with the challenges of coordinating collective efforts; it has established an explicit governance structure, which provides me with terms and concepts to examine through a critical lens; it comprises people working in a multitude of roles, many of whom are locally available; and it is a venue where multiple relevant epistemic conflicts and challenges manifest themselves, which are rich sources from which a qualitative researcher may ascertain competing and complementary value regimes. Interviews will be oriented by the study’s goal to document processes of reconciling different stakeholders’ interests as they converge in the formation of a common data resource.\nThe study participants are individuals who are affiliated with the CITF Databank. Participants are identified through consultation with project leaders, which helps determine the value that an interview will bring and to help make preparations prior to the interview. The principal investigator will be clear with project leaders that they should not pressure prospective participants to participate in the study, and that their participation should be treated as separate from their regular duties. The principal investigator will then approach the recommended individuals to introduce the study and its objectives and to invite them to participate as research subjects. If these individuals express interest in participating in the study, the principal investigator will schedule a time to sit for an interview. Some interviews may be conducted remotely using internet-based video conferencing software, depending on participants’ availability.\nDescribe the consent process. If alternate processes for seeking consent are planned (e.g. verbal, online, waiver), please provide a rationale and outline the procedure of obtaining and documenting consent and/or assent, where applicable.\nOnce individuals express their interest in participating, participants will be provided with an informed consent document that outlines in more detail the goals of the study, the roles of the participant, how they will be recorded, how data pertaining to them will be retained, and the potential risks and benefits pertaining to their involvement. This document will also describe how participants’ personally identifiable information will be managed and used. Participants will be asked to read and sign the document in order to obtain written informed consent. For interviews that will be held remotely using internet-based video conferencing software, participants will asked to send their signed informed consent documents in PDF format to the principal investigator. At the start of each interview the researcher will reiterate participants’ rights and ask them to orally reaffirm their consent before proceeding.\nIs there a relationship between the study participants and the person obtaining consent and/or the principal investigator/s?\n\nYes\nNo\n\nIf yes, please explain the nature of the relationship, and outline the steps that will be taken to avoid the perception of undue influence.\nThe project that serves as the case in this research is the Covid-19 Immunity Task Force (CITF) Databank, which the principal investigator currently serves as a postdoctoral researcher. The interviews will remain focused and limited in scope, and will not touch on matters relating to other aspects of their work. Moreover, prior to and throughout their involvement as research participants, frank and open discussion will be encouraged regarding collective expectations and to articulate the boundaries between participants’ relationships with the principal investigator as colleagues and as research subjects.",
    "crumbs": [
      "Ethics Protocol"
    ]
  },
  {
    "objectID": "ethics-protocol.html#risk-benefit-assessment",
    "href": "ethics-protocol.html#risk-benefit-assessment",
    "title": "Ethics Protocol",
    "section": "Risk-benefit assessment",
    "text": "Risk-benefit assessment\nDescribe the foreseeable risks to study participants. What risks are attributable to the research, including cumulative risks? Which risks are participants normally exposed to in the course of their clinical care or in their daily activities as they relate to the research questions/objectives?\nParticipation in this study does not involve any physical, psychological or legal risks. However, the principal investigator will be asking participants to share detailed information about their work practices and work relationships, and there is minimal risk that their responses may disrupt or complicate their professional reputations. Since this study is based on a single case whose identity has already been publicly established, it is not feasible to fully obfuscate the identities of prospective study participants.\nTo mitigate against the minimal risk of reputational harm, the principal investigator will provide participants with opportunities to shield their identities in public presentations of the findings when the risk is more clearly apparent. When the principal investigator senses that there may be heightened risk to reputational harm, he will approach the potentially affected participants and invite their input on how to proceed. Alternatively, the principal investigator may take action to shield the individuals’ identities independently of their added input. This aligns with this study’s emphasis on co-constituting knowledge about epidemiological research practices and value regimes through collaboration with the participants whose work will be examined.\n\n\nWhat procedures are in place to monitor and assess participant safety for the duration of the study?\nPrior to each interview, and as part of the procedure for obtaining informed consent, participants will be informed about their rights. The principal investigator will invite participants to reflect on their responses and express their desires to obfuscate their identities when reporting specific responses.\nThe principal investigator has established a consistent habit of maintaining reflexive notes regarding his encounters with research subjects, which include notes about signs of apprehension expressed by study participants. These notes inform his sensitivity to the degree of risk and support his decisions regarding whether to approach participants regarding potentially sensitive responses.\nDescribe the potential benefits of the study for: (1) the study participants; (2) the population under investigation, and (3) the field of research.\nThis study contributes to the development of better epidemiological data-sharing infrastructures by articulating the social, collaborative and discursive aspects of a well-established and exemplary data-sharing initiative. In particular, the study demonstrates how these factors relate to, overlap with or conflict with technical, institutional and epistemic factors. By explicitly framing data sharing as a social and collaborative activity, the study informs the design of more effective data-sharing infrastructures that better support the contextualization of data and enhance data’s value in contexts of reuse. This work therefore documents and reflects on the ways in which epidemiologists mobilize distributed records, and helps develop practical solutions that enable more effective collaborative workflows. Additionally, this study may directly benefit participants by framing the experiences they address during interviews in ways that they might not have otherwise considered, thereby encouraging greater reflexivity in their own work.",
    "crumbs": [
      "Ethics Protocol"
    ]
  },
  {
    "objectID": "ethics-protocol.html#privacy-and-confidentiality",
    "href": "ethics-protocol.html#privacy-and-confidentiality",
    "title": "Ethics Protocol",
    "section": "Privacy and confidentiality",
    "text": "Privacy and confidentiality\nPlease describe the measures in place for meeting confidentiality obligations. How is information and data safeguarded for the full cycle of the study: i.e. during its collection, use, dissemination, retention, and/or disposal?\nCommitment to maintaining full confidentiality is not warranted given the minimal risk profile associated with this study, and since the vast majority of recorded data will not contain sensitive information. At the same time, the principal investigator is committed to working with study participants to ensure that information that may carry elevated risk is handled with care and in accordance with participants’ wishes.\nIn situations whereby a participant desires to obscure their association with information they provided, the principal investigator will have access to unobfuscated versions of those responses containing information that may identify the participant. These unobfuscated records, which may include audio and video records of interview sessions, as well as unedited transcripts and textual notes containing information that may reveal the participants’ identities, will be kept in secure and encrypted media, and destroyed within five years of concluding the study, which provides sufficient time to revisit the data and produce additional research outputs. However, transcripts edited in a manner that reduces potential of elevated risk may be kept, published and archived.\nThe study is committed to adhering to fundamental data security practices, including those specified in McGill University’s Cloud Directive, which regulates the curation of sensitive research data. Physical records will be kept in a locked drawer in secure workspaces, either at McGill University’s School of Population and Global Health or at the principal researcher’s home office. Digital records will be stored on encrypted and password-protected drives and on secure servers approved or managed by McGill University under the Cloud Directive.\nRecordings of remote interviews conducted using internet-based video conferencing software will be made using the software’s built-in recording tools. Only video conferencing software approved by the Cloud Directive will be used.\nIf a contracted cloud/storage service provider or online survey tool is used, provide information on the service provider’s security and privacy policy, location of its servers, data ownership, and what happens to the stored data after the contract is terminated. For more information, please consult the University’s directive.\nThe study uses file-sharing software hosted by the CITF Databank at McGill University’s School of Population and Global Health to backup all files maintained for this study. These backups include unobfuscated versions of files containing potentially sensitive information. The software used to manage these backups is managed by McGill University and has been approved for storing sensitive research data by the Cloud Directive.\nThe study uses the secure GitLab instance hosted by the Surveillance Lab within the Clinical and Health Informatics Research Group at McGill University to store and track changes to sensitive research data. This software is managed by McGill University and has been approved for storing sensitive research data by the Cloud Directive.\nThe study maintains a website where the principal investigator shares documentation that supports the study and reflects on the work as it progresses. This is hosted using GitHub Pages and is backed up using Dropbox. No sensitive research data passes through these services.\nRecordings of remote interviews conducted using internet-based video conferencing software will be made using the software’s built-in recording tools. Only video conferencing software approved by the Cloud Directive will be used.\nPlease explain any reasonable and foreseeable disclosure requirements (e.g. disclosure to third parties such as government agencies or departments, community partners in research, personnel from an agency that monitors research, research sponsor, the REB/IRB, or regulatory agencies).\nNo disclosure requirements are foreseen.\nIf there are plans for retaining participant and/or study data for future use, please describe the context for its use, requirements for potentially re-contacting study participants and consent, and how the data will be stored and maintained for the long term.\nData will be published in compliance with ethical standards for sharing open social science research data. In particular, care will be taken to disassociate the identities of participants with statements they may have made that carry elevated risk.\nThe principal investigator may invite select participants to collaborate on papers presenting the findings or advocating for actions based on the project’s findings.\nSecondary use of data studies: if the study involves data linkage, please describe the data that will be linked and the likelihood that identifiable information will be created through the linkage.\nThis project does not rely on data deriving from other studies. The data may be reused in related work being undertaken under the same grant and by those who access the openly accessible data after they are published.",
    "crumbs": [
      "Ethics Protocol"
    ]
  },
  {
    "objectID": "ethics-protocol.html#managing-conflicts-of-interest",
    "href": "ethics-protocol.html#managing-conflicts-of-interest",
    "title": "Ethics Protocol",
    "section": "Managing conflicts of interest",
    "text": "Managing conflicts of interest\nConflicts of interest do not imply wrong-doing. It is the responsibility of the investigator to determine if any conflicts apply to any person/s involved in the design and/or conduct of the research study or any member of their immediate family. Disclose all contracts and any conflicts of interest (real, perceived, or potential) relating to this research project. Conflict of interest may also arise with regard to the disclosure of personal health information.\n\nNot applicable. There are no conflicts of interest to disclose.\nYes, there are conflicts of interest to disclose.\n\nIf yes, please describe the conflicts of interest (real, potential, and perceived), and the procedures for managing declared conflicts. Not applicable.",
    "crumbs": [
      "Ethics Protocol"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CITF-Postdoc",
    "section": "",
    "text": "This website serves as a hub for my postdoctoral research at McGill University’s Covid-19 Immunity Task Force Databank.\nThe project is concerned with articulating social, collaborative and discursive aspects of epidemiological data-sharing initiatives, and how they relate to, overlap with or conflict with technical, institutional and epistemic factors.\nThis website hosts a series of preparatory protocols that structure the project, as well as notes about key concepts and reflections on the progress of work. Please keep in mind that this is a continually evolving site and its contents may change as the project goes on. All content is hosted and tracked at github.com/zackbatist/CITF-Postdoc.\nHere’s an overview of what’s on this site:\nContext: My motivations for doing this work and the circumstances that surround the establishment of the project.\nResearch Protocol: Outlines the project’s overall vision and contextualizes it in relation to specific objectives.\nCase Selection: Articulates the parameters that inform how cases are selected.\nEthics Protocol: Specifies ethical considerations, including risks of harm and strategies for mitigating them.\nInterview Protocol: The questions I will be asking research participants, including the rationale for asking them.\nData Management: Procedures that circumscribe collection, management and curation of research data.\nQDA Protocol: The code system, memoing guidelines, and specific QDA procedures.\nGlossary: A series of key terms and their definitions, with reference to the literature and expanded notes about their meanings.\nNotes: Some semi-structured ideas that situate my work in relation to extant literature.\nBlog: Weekly updates, reflections on key events, notes on new workflows, and general thoughts I wish to share.\nGitHub: A link to the GitHub repository where this website’s files are hosted.\nBib: A continually-updated biblatex export of my zotero library.\nRSS: RSS feed for the blog.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Modified\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJun 23, 2025\n\n\nCataloguing\n\n\n \n\n\n\n\nJun 23, 2025\n\n\nOther research about harmonization\n\n\nreading-notes\n\n\n\n\nMay 12, 2025\n\n\nQDA Software\n\n\nreading, general thoughts\n\n\n\n\nApr 17, 2025\n\n\nMethodology notes\n\n\nreading, general thoughts\n\n\n\n\nApr 17, 2025\n\n\nTricks of the Trade\n\n\nreading\n\n\n\n\nApr 17, 2025\n\n\nTheory-building\n\n\nreading, general thoughts\n\n\n\n\nApr 17, 2025\n\n\nTheoretical sampling\n\n\nreading, general thoughts\n\n\n\n\nMar 25, 2025\n\n\nSituational analysis\n\n\nsituational analysis, QDA methods, reading\n\n\n\n\nMar 24, 2025\n\n\nCoding methods\n\n\nQDA methods, coding, reading\n\n\n\n\nMar 24, 2025\n\n\nMaelstrom reading notes\n\n\nreading\n\n\n\n\nMar 24, 2025\n\n\nPotential cases\n\n\ncases, brainstorming\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Notes"
    ]
  },
  {
    "objectID": "notes/cataloguing.html",
    "href": "notes/cataloguing.html",
    "title": "Cataloguing",
    "section": "",
    "text": "From Pan et al. (2023), who use a “crosswalk-cataloging-harmonization process”:\n\nWe applied the crosswalk-cataloging-harmonization process (Figure 2). During the crosswalk step, we used a spreadsheet to document available variables from each cohort and organized them by data concept (e.g., education, income, tobacco use). During the cataloging step, we identified common data elements (CDEs) from PhenX or the National Institutes of Health CDE Repository (https://cde.nlm.nih.gov/home) within each data concept for sociodemographic, lifestyle, and pregnancy-related baseline variables (Web Table 1). If a CDE could not be identified or an identified CDE could not be applied across studies, a definition was created to incorporate the maximum amount of information from each study (5).\n\nThey cite Pino et al. (2018) with regards to the crosswalk-cataloging-harmonization process:\n\nThe first step of harmonising data for the MULTITUDE consortium was the crosswalk of data measures (vari ables) across all studies. All available variables from individual studies within the consortium were identified and systematically entered in eight sections of (1) demo graphic data, (2) comorbidities, (3) laboratory values at diagnosis, (4) biomarkers, (5) medications, (6) ECG and echocardiogram (ECHO), complications related to diabetes and (8) events. This crosswalk allows assessment of each variable and, in turn, allows us to determine the level of comparability between studies. were identified and collected from each study without alteration of the original data or creation of new MULTI TUDE- specific target variables. For example, in determining a baseline diagnosis of T2DM, individual studies may have dichotomous ‘yes’/’no’ data on a history or diagnosis of T2DM. Alter natively, studies may only have data on fasting glucose levels, random plasma glucose levels or haemoglobin A1c levels. At this stage of the process, all relevant variables. Following data element crosswalk, all variables were then catalogued based on their key characteristics and rele vance in answering the research questions addressed. Recording of blood pressure may be different across studies: one study may have systolic and diastolic blood pressure measured by the technician and another could have reported values from medical records. Clinical outcomes can also be obtained from different sources such as medical records without independent adjudica tion, with independent adjudication or via self-report. All variables that are empirically similar or indicate the same measurement are grouped together and named under a common pooled variable. We evaluated which studies could provide data that enabled generation of each of the target variables and we qualitatively assessed the level of similarity between the study-specific and target variables.\n\nFortier et al. (2017) seems to make a similar distinction through sub-steps of their comprehensive guidelines:\n\nThe identification of studies of interest (Step 1) and evaluation of the harmonization potential (Step 2) are facilitated by the existence of central metadata catalogues providing comprehensive information on existing study designs and content. Catalogues can also provide information useful to guide the development of prospective data collections.\n\n\nStep 1: Assemble information and select studies.\nStep 1a: Document individual study designs, methods and content: ensure appropriate knowledge and understanding of &gt; each study. Data comparability can be affected by heterogeneity of study-, population-, procedural- and data-related characteristics. Information related to design, time frame and population background will, for example, be required to evaluate study eligibility. In addition, information related to the specific data collected and, where relevant, standard operating procedures used will be essential to evaluate harmonization potential and guide data processing.\nStep 1b: Select participant studies: select studies based on explicit criteria. To ensure consistency, designs of the studies included in a harmonization project must be similar enough to be considered compatible."
  },
  {
    "objectID": "notes/cataloguing.html#outside-maelstrom",
    "href": "notes/cataloguing.html#outside-maelstrom",
    "title": "Cataloguing",
    "section": "",
    "text": "From Pan et al. (2023), who use a “crosswalk-cataloging-harmonization process”:\n\nWe applied the crosswalk-cataloging-harmonization process (Figure 2). During the crosswalk step, we used a spreadsheet to document available variables from each cohort and organized them by data concept (e.g., education, income, tobacco use). During the cataloging step, we identified common data elements (CDEs) from PhenX or the National Institutes of Health CDE Repository (https://cde.nlm.nih.gov/home) within each data concept for sociodemographic, lifestyle, and pregnancy-related baseline variables (Web Table 1). If a CDE could not be identified or an identified CDE could not be applied across studies, a definition was created to incorporate the maximum amount of information from each study (5).\n\nThey cite Pino et al. (2018) with regards to the crosswalk-cataloging-harmonization process:\n\nThe first step of harmonising data for the MULTITUDE consortium was the crosswalk of data measures (vari ables) across all studies. All available variables from individual studies within the consortium were identified and systematically entered in eight sections of (1) demo graphic data, (2) comorbidities, (3) laboratory values at diagnosis, (4) biomarkers, (5) medications, (6) ECG and echocardiogram (ECHO), complications related to diabetes and (8) events. This crosswalk allows assessment of each variable and, in turn, allows us to determine the level of comparability between studies. were identified and collected from each study without alteration of the original data or creation of new MULTI TUDE- specific target variables. For example, in determining a baseline diagnosis of T2DM, individual studies may have dichotomous ‘yes’/’no’ data on a history or diagnosis of T2DM. Alter natively, studies may only have data on fasting glucose levels, random plasma glucose levels or haemoglobin A1c levels. At this stage of the process, all relevant variables. Following data element crosswalk, all variables were then catalogued based on their key characteristics and rele vance in answering the research questions addressed. Recording of blood pressure may be different across studies: one study may have systolic and diastolic blood pressure measured by the technician and another could have reported values from medical records. Clinical outcomes can also be obtained from different sources such as medical records without independent adjudica tion, with independent adjudication or via self-report. All variables that are empirically similar or indicate the same measurement are grouped together and named under a common pooled variable. We evaluated which studies could provide data that enabled generation of each of the target variables and we qualitatively assessed the level of similarity between the study-specific and target variables.\n\nFortier et al. (2017) seems to make a similar distinction through sub-steps of their comprehensive guidelines:\n\nThe identification of studies of interest (Step 1) and evaluation of the harmonization potential (Step 2) are facilitated by the existence of central metadata catalogues providing comprehensive information on existing study designs and content. Catalogues can also provide information useful to guide the development of prospective data collections.\n\n\nStep 1: Assemble information and select studies.\nStep 1a: Document individual study designs, methods and content: ensure appropriate knowledge and understanding of &gt; each study. Data comparability can be affected by heterogeneity of study-, population-, procedural- and data-related characteristics. Information related to design, time frame and population background will, for example, be required to evaluate study eligibility. In addition, information related to the specific data collected and, where relevant, standard operating procedures used will be essential to evaluate harmonization potential and guide data processing.\nStep 1b: Select participant studies: select studies based on explicit criteria. To ensure consistency, designs of the studies included in a harmonization project must be similar enough to be considered compatible."
  },
  {
    "objectID": "notes/cataloguing.html#formalizing-an-existing-tacit-procedure",
    "href": "notes/cataloguing.html#formalizing-an-existing-tacit-procedure",
    "title": "Cataloguing",
    "section": "Formalizing an existing tacit procedure?",
    "text": "Formalizing an existing tacit procedure?\nFrom Bergeron et al. (2018):\n\nThe present paper describes the approach and software developed by the Maelstrom Research team to answer the need for a general and customizable solution to support the creation of comprehensive and user-friendly study- and network-specific catalogues used to lever age epidemiological research making use of cohort data.\n\nIn other words, it seems more concerned with presenting the tooling, which complements documentation of procudure and methods in Fortier et al. (2017).\n\n\nSince 2004, maturing versions of the toolkit were produced and tested by these projects (Table 1). Throughout, comments and suggestions from investigators of these initiatives were integrated in a central repository. At least once a year, the most pressing or crucial demands for improvements were selected and the toolkit was, and still is customized to answer these requests. Improved versions of the toolkit are therefore regularly generated and tested by users.\n\nAgain, this implies that they had honed a procedure over many years, and were now simply formalizing the process I wonder if this was to make it more compatible with the software’s demands for formal processes, or whether it was merely inspired by a computational way of thinking.\n\nIn Bergeron et al. (2018), they identify a few key components to a catalog entry:\n1. Study outline\n\nstudy’s name\nlogo\nwebsite\nlist of investigators and contact persons\nthe objectives\ntimeline\nnumber of participants recruited and participants providing biological samples\ninformation on access to data and samples\n\n1. For each subpopulation of participants\n\ninformation related to the recruitment of participants and selection criteria\n\n3. Documentation of each data collection event\n\ngeneral description\nstart and end dates\ndata sources\ntype of information collected\n\n4. Lists of variables collected\n\ndataset metadata\n\nnames of the datasets\ndescription of the dataset content\n\nvariable metadata\n\nvariables’ names and labels\ncodes and labels of each variable category (if applicable)\nthe specific question used to collect the data\nmeasurement units\n\nannotations using various classification schemes\n\nThese are more formally defined in the supplement, copied here:\n\n\n\n\n\n\n\nSTUDY\n\n\n\nField\nDefinition\n\n\nName\nOfficial name of the study.\n\n\nAcronym\nStudy acronym.\n\n\nWebsite\nStudy website URL.\n\n\nInvestigators\nName, affiliated institution and contact information of the principal investigators.\n\n\nContacts\nName, affiliated institution and contact information of the person to be contacted to have more information about the study.\n\n\nObjectives\nMain objectives of the study.\n\n\nStudy timeline\nDate when first participants were recruited and study end date if the study is completed.\n\n\nStudy design\nInformation on specific study design. 1. Cohort 2. Case-control 3. Case only 4. Cross-sectional 5. Clinical trial 6. Other\n\n\nGeneral information on follow-up\nProfile and frequency of participants’ follow-up (e.g. Participants are followed-up every 5 years).\n\n\nSupplementary information about study design\nAdditional information about study design (e.g. Subgroups of the population were intentionally over-sampled).\n\n\nRecruitment target\nType of participant units targeted by the study. 1. Individuals 2. Families 3. Other\n\n\nNumber of participants\nNumber of participants planned to be recruited. If the study is completed, the final number of participants.\n\n\nNumber of participants with biological samples\nIf the study is collecting biological samples, number of participants that should provide samples. If the study is completed, the final number of participants that provided biological samples.\n\n\nSupplementary information about number of participants\nAdditional information about target number of participants (e.g. Additional biological samples will be collected for population 2).\n\n\nAccess\nWhether access to study data, biological samples or other study material by external researchers or third parties is allowed or foreseen.\n\n\nMarker paper(s)\nBibliographic citation(s) which should be used to refer to the study and, if applicable, the paper’s Pubmed ID.\n\n\nLogo\nLogo used by the study.\n\n\nDocuments\nRelevant documents about the study (e.g. Questionnaires, standard operating procedures, codebooks).\n\n\n\n\n\n\n\n\n\n\nPOPULATION\n\n\n\nField\nDefinition\n\n\nName\nName of the study population.\n\n\nDescription\nA brief description of the population.\n\n\nSources of recruitment\nSpecification of the sources of recruitment. 1. General population (volunteer enrolment, selected sample, random digit dialing) 2. Specific population (clinic patients, members of specific association, other specific population) 3. Participants from existing studies 4. Other source\n\n\nSupplementary information about sources of recruitment\nAdditional information about recruitment procedures (e.g. Participants were identified from the electoral register and general practice lists).\n\n\nSelection criteria\nIf relevant, specification for the following selection criteria of the participants. 1. Gender (women or men) 2. Age (minimum age and maximum age) 3. Residence (country, territory or city) 4. Pregnant women (first trimester, second trimester, third trimester) 5. Newborns 6. Twins 7. Ethnic origin 8. Health status 9. Other\n\n\nSupplementary information about selection criteria\nAdditional information about selection criteria of the population (e.g. All subjects identified at baseline as affected by cognitive impairment without dementia were eligible for the longitudinal phase conducted after one year).\n\n\nNumber of participants\nNumber of participants planned to be recruited for the population. If the study is completed, the final number of participants.\n\n\nNumber of participants with biological samples\nIf the study is collecting biological samples, number of participants that should provide samples for the population. If the study is completed, the final number of participants that provided biological samples.\n\n\nSupplementary information about number of participants\nAdditional information about number of participants. Usually the number of participants for each wave of the study (e.g. Number of participants for each data collection event Wave 1: 7175 participants Wave 2: 3145 participants Wave 3: 1733 participants).\n\n\n\n\n\n\n\n\n\n\nDATA COLLECTION EVENT\n\n\n\nField\nDefinition\n\n\nName\nName of the data collection event.\n\n\nDescription\nA brief description of the data collection event.\n\n\nData collection event date\nData collection start date and end date.\n\n\nData sources\nData sources from which the information is obtained. 1. Questionnaires 2. Physical measures 3. Cognitive measures 4. Biological samples (blood, cord blood, buccal cells, tissues, saliva, urine, hair, nail, other) 5. Administrative databases (health databases, vital statistics databases, socioeconomic databases, environmental databases) 6. Others (e.g. medical files)\n\n\n\n\n\n\n\n\n\n\nDATASET\n\n\n\nField\nDefinition\n\n\nName\nName of the dataset.\n\n\nAcronym\nDataset acronym.\n\n\nDescription\nShort description of the dataset specifying its content.\n\n\nEntity type\nWhat the data are about (usually the participant).\n\n\n\n\n\n\n\n\n\n\nVARIABLE\n\n\n\nField\nDefinition\n\n\nDataset\nName of the dataset in which the variable resides.\n\n\nName\nName of the variable.\n\n\nLabel\nShort description of the variable specifying its content (e.g. Type of diabetes). Further information can be added in the description field.\n\n\nDescription\nAdditional information about the variable such as: 1. For variables collected by questionnaire, the question itself or any relevant information about the variable (e.g. Have you ever been told by a doctor that you had diabetes?) 2. For variables about physical/laboratory measures, any relevant information describing the context of measurement (e.g. self-reported measure, measure by a trained professional) or related to the protocol (e.g. measure taken when the participant is at rest) 3. For derived or constructed variables, any relevant information about the derivation or construction of the variable (e.g. MMSE total score, total energy in Kcal per day derived from diet questionnaire).\n\n\nValue type\nType of variable: 1. Boolean (two possible values (usually denoted true or false)) 2. Date (values written in a defined date format) 3. Datetime (values written in a defined date and time format) 4. Decimal (numerical values with a fractional component) 5. Integer (numerical values without a fractional component) 6. Text (alphanumerical values) 7. Other types (Point, line string, or polygon, etc.) (e.g. Type of diabetes has an integer value type: 1, 2, 3, 8, 9).\n\n\n\nFor continuous variables (where relevant)\n\n\nUnit\nMeasurement unit of the variable (e.g. cm, mmol/L).\n\n\n\nFor categorical variables\n\n\nCategory name\nValue assigned to each variable category (e.g. Type of diabetes has 5 categories: 1, 2, 3, 8, 9).\n\n\nCategory label\nShort description of the category (e.g.: 1: Type 1 diabetes 2: Type 2 diabetes 3: Gestational diabetes 8: Prefers not to answer 9: Missing)"
  },
  {
    "objectID": "notes/cataloguing.html#cataloguing-procedures",
    "href": "notes/cataloguing.html#cataloguing-procedures",
    "title": "Cataloguing",
    "section": "Cataloguing procedures",
    "text": "Cataloguing procedures\nBergeron et al. (2018: 9) explains the procedure for creating catalogue entries:\n\nTo ensure quality and standardization of the metadata documented across networks, standard operating procedures were implemented. Using information found in peer-reviewed journals or on institutional websites, the study outline is documented using Mica and validated by study investigators. Where possible, data dictionaries or codebooks are obtained, completed for missing information (e.g. missing labels) and formatted to be uploaded in Opal. Variables are then manually classified by domains and subdomains and validated with the help of an in house automated classifier based on a machine learning method. When completed, study and variable-specific metadata are made publicly available on the Maelstrom Research website.\n\nThe procedure is described in more detail in the supplementary materials, which they divide into three steps: study description, variables documentation and variables annotation.\n\nStep 1: Completion of the study description\nAim: Document the study design, targeted population(s) and data collection event(s).\nProcedures:\n\nGather information about the study from different sources including published papers and study website.\nComplete the fields of the study description model available in Mica.\nEnsure validation of the study description by a second person to ascertain the adequacy and quality of its content.\nObtain validation and, if required, additional information from the study investigators.\nMake any required modifications and publish the study description on the Maelstrom Research website.\n\n\n\nStep 2: Documentation of the study variables\nAim: Generate standardized variable dictionaries.\nProcedures:\n\nObtain the questionnaires and data dictionary from the study investigator. The data dictionary can be in different formats (SPSS, Excel, csv, etc.).\nFormat the data dictionary to be compatible with Opal.\nEvaluate completeness of the data dictionary content.\nCorrect any missing or unclear information with the help of the questionnaires (label, category codes and labels). Variables should at least have a name, a label, and if applicable, codes and labels for categories. If impossible, ask study investigators to add the missing information and send back the complete data dictionary.\n\n\n\nStep 3: Annotation of variables by domains and sub-domains\nAim: Classify each study variable in at least one domain and subdomain of the Maelstrom Research classification.\nProcedures:\n\nFirst research assistant: attribution of each variable to one or more subdomains of the areas of information with the help of the questionnaires and the information documented in the previous cataloguing steps. The context surrounding the variable should prevail on blindly applying the rules.\nValidation of the classification using an in-house automated classifier based on a machine learning method. This, to identify discrepancies between the human and the machine annotations.\nSecond research assistant: validation of all variables for which a divergence was observed and where relevant, suggestion of modifications to the initial classification.\nFirst research assistant: review of the suggested modifications.\nIf disagreement on the classification of a variable remains, group discussion to take final decision.\nUpload annotated variables on Opal and publish variables data dictionaries and related annotation on the Maelstrom Research website."
  },
  {
    "objectID": "notes/cataloguing.html#taxonomy",
    "href": "notes/cataloguing.html#taxonomy",
    "title": "Cataloguing",
    "section": "Taxonomy",
    "text": "Taxonomy\nBergeron et al. (2018: 7) explains how interoperability is achieved through the use of classification schemes:\nVariables are annotated using various classification schemes, which effectively “facilitate browsing and extraction of variables by topics of interest and enables the generation of tables comparing domain-specific data collected across studies, subpopulations and data collection events.”\nThey do not use the term taxonomy, but that is what they are presenting here.\nIn the supplement, they describe the process in further detail:\n\nThe Maelstrom model supports usage of multiple variables annotations that can be used to better inform variable metadata content (e.g. name of the measure or standardized questionnaire used, source of the data, etc.). However, a classification index, was develop as complementary to the cataloguing toolkit. The Maelstrom Research classification can be used to facilitate variable search and was specifically developed to serve the needs of the platform users. It aims to facilitate selection of variables by topics of interest and generation of tables comparing variables content across studies, subpopulations and data collection events. This classification can theoretically be used to categorize all type of information collected by a study and is divided into 18 domains and 135 subdomains (see section below). Development of the classification was done through a series of workshops with cohorts’ investigators, computer scientists, statisticians and data managers. When is was possible, we used existing classifications, but is was not always the case. Some of the domains are thus based on international classification systems (e.g. International Classification of Diseases (ICD)) or are elements of existing classifications (International Classification of Functioning, Disability and Health (ICF)). However, for other domains no existing classification were available or could be used to classify variables provided by our partners. It was thus required to create new classes.\n\nThe supplement then goes on to define the 18 domains and 135 subdomains.\n\nThe taxonomies are also accessible by request, despite being licensed with CCBY and and some other files available on GitHub (https://github.com/maelstrom-research/maelstrom-taxonomies).\nFrom the readme:\n\nThese classification schemes allow you to annotate study variables with a standardized list of areas of information and, when applicable, the standardized scales / questionnaires used to collect them. A specific taxonomy also allows annotating harmonized datasets. To the end user, these taxonomies facilitate metadata browsing and enhances data discoverability in the Mica web data portal.\n\nThe OBiBa documentation also makes reference to the Maelstrom taxonomy: https://opaldoc.obiba.org/en/latest/web-user-guide/administration/taxonomies.html\n\nThe Maelstrom taxonomy is referenced in various papers, most notably those that document work being done under the aegis of the NFDI4Health Covid-19 Task Force:\n\nPigeot et al. (2024)\nSchmidt et al. (2020)\nVorisek et al. (2022)\nSchmidt et al. (2021)\nDarms et al. (2021)\netc\n\n\nSasse et al. (2024) documents the process for using AI to assign variables to the Maelstrom taxonomy. The code is here: https://github.com/nfdi4health/workbench-AI-model"
  },
  {
    "objectID": "notes/critical-research-about-harmonization.html",
    "href": "notes/critical-research-about-harmonization.html",
    "title": "Other research about harmonization",
    "section": "",
    "text": "From Rolland et al. (2015: 1034):\n\nFew investigators write extensively about their data-harmonization procedures, despite the widespread use of harmonized data. Even papers that reference the methodological issues of data pooling tend to gloss over the actual process of data harmonization itself (3–5). In the paper by Fortier et al., for example, there are details in the Methods section on how the data are selected; then the harmonization process itself is summed up thus:\n\nIn order to classify the assessment items and to ensure the validity and reproducibility of the pairing results, sets of comprehensive ‘pairing rules’ specific to each variable are defined. Development of pairing rules is context specific and involves a systematic process of iteration between scientific experts and trained research officers. Using these pairing rules, trained research officers determine whether or not a variable can be recreated using the assessment items collected by each participating study (4, p. 1317).\n\nThere are no details on how the scientific experts and trained research officers derived and refined their pairing rules or how long it took. Though such an explanation might be complex, without it, researchers are unable to apply the methods themselves or even fully evaluate the methods proposed. This lack of discussion on how to pool data for a new analysis means that investigators in each study craft their own methods of harmonization, with little empirical evidence to support any one method. Pooled studies have markedly increased power because of their larger sample sizes; it behooves us to be sure that the conclusions being drawn are as accurate as possible. To that end, we describe here the harmonization processes needed for 4 different studies, the analyses of which were directed by 2 of us (M.T., Z.F.), both senior biostatisticians.\n\nFrom Rolland et al. (2015: 1035):\n\nIdentification of these high-level data concepts is not an easy process, as it requires researchers to take a step back from the detailed data, think conceptually about their research questions, and then negotiate around those concepts with their colleagues until they reach agreement on what is impor tant. For investigators accustomed to moving directly to individual data points, it may feel like a waste to start at such a high level, but it has been our experience that time spent on this step makes the work that follows substantially smoother and quicker. Such conversations need to involve a variety of people, including the Principal Investigators (PIs) of the original studies and their data managers, who collectively have an understanding of the subtle nuances within the data they collect and manage, including questions of data reliability and availability. Answers to these questions can influence the final form of the project’s scientific questions."
  },
  {
    "objectID": "notes/critical-research-about-harmonization.html#rolland2015",
    "href": "notes/critical-research-about-harmonization.html#rolland2015",
    "title": "Other research about harmonization",
    "section": "",
    "text": "From Rolland et al. (2015: 1034):\n\nFew investigators write extensively about their data-harmonization procedures, despite the widespread use of harmonized data. Even papers that reference the methodological issues of data pooling tend to gloss over the actual process of data harmonization itself (3–5). In the paper by Fortier et al., for example, there are details in the Methods section on how the data are selected; then the harmonization process itself is summed up thus:\n\nIn order to classify the assessment items and to ensure the validity and reproducibility of the pairing results, sets of comprehensive ‘pairing rules’ specific to each variable are defined. Development of pairing rules is context specific and involves a systematic process of iteration between scientific experts and trained research officers. Using these pairing rules, trained research officers determine whether or not a variable can be recreated using the assessment items collected by each participating study (4, p. 1317).\n\nThere are no details on how the scientific experts and trained research officers derived and refined their pairing rules or how long it took. Though such an explanation might be complex, without it, researchers are unable to apply the methods themselves or even fully evaluate the methods proposed. This lack of discussion on how to pool data for a new analysis means that investigators in each study craft their own methods of harmonization, with little empirical evidence to support any one method. Pooled studies have markedly increased power because of their larger sample sizes; it behooves us to be sure that the conclusions being drawn are as accurate as possible. To that end, we describe here the harmonization processes needed for 4 different studies, the analyses of which were directed by 2 of us (M.T., Z.F.), both senior biostatisticians.\n\nFrom Rolland et al. (2015: 1035):\n\nIdentification of these high-level data concepts is not an easy process, as it requires researchers to take a step back from the detailed data, think conceptually about their research questions, and then negotiate around those concepts with their colleagues until they reach agreement on what is impor tant. For investigators accustomed to moving directly to individual data points, it may feel like a waste to start at such a high level, but it has been our experience that time spent on this step makes the work that follows substantially smoother and quicker. Such conversations need to involve a variety of people, including the Principal Investigators (PIs) of the original studies and their data managers, who collectively have an understanding of the subtle nuances within the data they collect and manage, including questions of data reliability and availability. Answers to these questions can influence the final form of the project’s scientific questions."
  },
  {
    "objectID": "notes/methodology-notes.html",
    "href": "notes/methodology-notes.html",
    "title": "Methodology notes",
    "section": "",
    "text": "This document is an overview of methodological topics and concerns. It is a place where I think through and justify my methodological decisions, and identify the methods and procedures through which I implement them."
  },
  {
    "objectID": "notes/methodology-notes.html#multicase-studies",
    "href": "notes/methodology-notes.html#multicase-studies",
    "title": "Methodology notes",
    "section": "Multicase Studies",
    "text": "Multicase Studies\nThese notes describe the features, affordances and limitations of case study research, and articules factors correspoding with variable kinds of case studies.\nI do notice a distinction between two schools of thought, which seem to be spearheaded by Stake and Yin. I generally favour Stake’s flexible approach, and it seems well aligned with other methodological works I’ve been reading (e.g. Abbott 2004; Ragin and Becker 1992).\n\nStake’s Approach\nIn case-study research, cases represent discrete instances of a phenomenon that inform the researcher about it. The cases are not the subjects of inquiry, and instead represent unique sets of circumstances that frame or contextualize the phenomenon of interest (Stake 2006: 4-7).\nCases usually share common reference to the overall research themes, but exhibit variations that enable a researcher to capture different outlooks or perspectives on matters of common concern. Drawing from multiple cases thus enables comprehensive coverage of a broad topic that no single case may cover on its own (Stake 2006: 23). In other words, cases are contexts that ascribe particular local flavours to the activities I trace, and which I must consider to account fully for the range of motivations, circumstances and affordances that back decisions to perform activities and to implement them in specific ways.\nMoreover, the power of case study research derives from identifying consistencies that relate cases to each other, while simultaneously highlighting how their unique and distinguishing facets contribute to their representativeness of the underlying phenomon. Case study research therefore plays on the tensions that challenge relationships among cases and the phenomenon that they are being called upon to represent (Ragin 1999: 1139-1140).\nStake (2006: 4-6) uses the term quintain1 to describe the group, category or phenomenon that bind together a collection of cases. A quintain is an object, phenomenon or condition to be studied – “a target, not a bull’s eye” (Stake 2006: 6). “The quintain is the arena or holding company or umbrella for the cases we will study” (Stake 2006: 6). The quintain is the starting point for multi-case research.\nAccording to Stake (2006: 6):\n\nMulticase research starts with the quintain. To understand it better, we study some of its single cases — its sites or manifestations. But it is the quintain we seek to understand. We study what is similar and different about the cases in order to understand the quintain better.\n\nStake (2006: 8) then goes on:\n\nWhen the purpose of a case is to go beyond the case, we call it an “instrumental” case study When the main and enduring interest is in the case itself, we call it “intrinsic” case study (Stake 1988). With multicase study and its strong interest in the quintain, the interest in the cases will be primarily instrumental.\n\nAbbott’s (2004: 22) characaterization of Small-N comparison is very reminiscient of Stake’s (2006) account of the case-quintain dialectic:\n\nSmall-N comparison attempts to combine the advantages of single-case analysis with those of multicase analysis, at the same time trying to avoid the disadantages of each. On the one hand, it retains much information about each case. On the other, it compares the different cases to test arguments in ways that are impossible with a single case. By making these detailed comparisons, it tries to avoid the standard critcism of single-case analysis — that one can’t generalize from a single case — as well as the standard criticism of multicase analysis — that it oversimplifies and changes the meaning of variables by removing them from their context.\n\nIt should be noted that case study research limits my ability to define causal relationships or to derive findings that may be generalized across the whole field of epidemiology. This being said, case study research allows me to articulate the series of inter-woven factors that impact how epidedemiological researchers coordinate and participate in data-sharing initiatives, while explicitly accounting for and drawing from the unique and situational contexts that frame each case.\nStake (2006: 23) recommends selecting between 4-10 cases and identifies three main criteria for selecting cases:\n\nIs the case relevant to the quintain?\nDo the cases provide diversity across contexts?\nDo the cases provide good opportunities to learn about complexity and contexts?\n\n\nFor qualitative fieldwork, we will usually draw a purposive sample of cases, a sample tailored to our study; this will build in variety and create opportunities for intensive study (Stake 2006: 24).2\n\nStake’s (2010: 122) prioritizes doing research to understand something or to improve something, and I generally agree with his rationalization; research helps reframe problems and establish different decision options.\n\n\nYin’s Approach\nAccording to Yin (2014: 16), “a case study is an empirical inquiry that investigates a contemporary phenomenon (the ‘case’) in depth and within its real-world context, especially when the boundaries between phenomenon and context may not be clearly evident.”\nHe goes on to document some features of a case study: “A case study inquiry copes with the technically distinctive situation in which there will be many more variables of interest than data points, and as one result relies on multiple sources of evidence, with data needing to converge in a triangulating fashion, and as another result benefits from the prior development of theoretical propositions to guide data collection and analysis.” (Yin 2014: 17)\nYin (2014) is more oriented toward what he refers to as a realist perspective, which he pits against relativist and interpretivist perspectives (used interchangably, it seems), and which I might refer to as constructivist. He characterizes relativist perspectives as “acknowledging multiple realities having multiple meanings, with findings that are observer dependent”. His prioriting of a realist approach corresponds with the analysis by Yazan (2015), who compared Yin with Stake and Merriam. According to Yazan (2015: 137), Yin evades making statements about his epistemic commitments, and is characterized as post-positivist.\nYin (2014) is very concerned with research design in case study research He posits that, in a colloquial sense, “a research design is a logical plan for getting from here to there, where here may be defined as the initial set of questions to be answered, and there is some set of conclusions (answers) about these questions.” (Yin 2014: 28)\nYin distinguishes between a research design and a work plan. A research design deals with a logical problem, whereas a work plan deals with a logistical problem. Seems reminiscient of Brian Cantwell Smith’s distinction between skeletons and outlines.\nYin lists five components of a research design:\n\nA case study’s questions;\nits propositions, if any;\nits unit(s) of analysis;\nthe logic linking the data to the propositions; and\nthe criteria for interpreting the findings.\n\nInterestingly, I have been instinctively following these steps, and am currently hovering somewhere between components 3 and 4, while dipping back to 2 once in a while too.\nThe problem of defining the unit of analysis is salient to me right now. According to Yin (2014: 32), the unit of analysis may change as the project progresses, depending on initial misconceptions (he uses the example of a unit of analysis changing from neighbourhoods to small groups, as contextualized by the socio-geographical entity of the neighbourhood, which is laden with issues of class, race, etc). In my own situation, the unit of analysis may hover between the harmonization initiative, the people, activities or infrastructures that make it work.\nIn the section on criteria for interpreting the findings, Yin emphasizes the role of rival theories, which is akin to a concern with falsifiability as a means of validating truth claims, and which betrays his positivist leanings. This may be compared with Stake’s emphasis on triangulation, which is more concerned with internal cohesiveness. Similarly, Yin cites Corbin and Strauss regarding the role of theory or theoretical propositions in research design, which similarly reveals a concern with rigorous upfront planning and strict adherence to research design as a key aspect of deriving valid findings.\nRegarding generalizability, Yin (2014: 40-41) states that “Rather than thinking about your case as a sample, you should think of it as the opportunity to shed empirical light about some theoretical concepts or principles, not unlike the motive of a laboratory investigator in conceiving of and then conducting a new experiment.” He goes on to state that case studies tend to strive for analytic generalizations that go beyond the specific case that has been studied, and which apply to other concrete situations rather than just abstract theory building.\n\n\nLogistics of case study design\n\nPreparing to select case study data\nYin (2014: 72-73) identifies five desired attributes for collecting case study data:\n\nAsk good questions — and interpret answers fairly.\n\n“As you collect case study evidence, you must quickly review the evidence and continually ask yourself why events or perceptions appear as they do.” (73)\nA good indicator of having asked good questions is mental and emotional exhaustion at the end of each fieldwork day, due to the depletion of “analytic energy” associated with being attention on your toes. (73-74)\n\nBe a good “listener” not trapped by existing ideologies or preconceptions.\n\nSensing through multiple modalities, not just spoken words.\nAlso subtext, as elicited through choices of terms used, mood and affective components. (74)\n\nStay adaptive, so that newly encountered situations can be seen as opportunities, not threats.\n\nRemember the original purpose but willing to adapt to unanticipated circumnstances. (74)\nEmphasize balancing adaptability with rigour, but not with rigidity. (75)\n\nHave a firm grasp of what is being studied, even when in an exploratory mode.\n\nNeed to do more than merely record data, but interpret information as they are being collected and to know immedately whether there are contradictions or complementary statements to follow-up on. (75-76)\n\nAvoid biases of being sensitive to contrary evidence, also knowing how to conduct research ethically.\n\nMaintain strong professional competence, including keeping up with related research, ensuring accuracy, striving for credibility, and knowledging and mitigating against bias.\n\n\nYin advocates for adoption of case study protocols. He provides an example of a table of contents for case study protocols, which generally comprise four sections:\n\nOverview of the case study\nData collection procedures\nData collection questions\nGuide for the case study report\n\n\n\nTriangulation\nTriangulation is a process of gaining assurance. Also sometimes called crystallization.\n“Each important finding needs to have at least three (often more) confirmations and assurances that key meanings are not being overlooked.” (Stake 2006: 33) Triangulation is a process of repetitous data gathering and critical review of what is being said. (Stake 2006: 34)\nWhat needs triangulation? (Stake 2006: 35-36)\n\nIf the description is trivial or beyond question, there is no need to triangulate.\nIf the description is relevant and debatable, there is much need to triangulate.\nIf the data are critical to a main assertion, there is much need to triangulate.\nIf the data are evidence for a controversial finding, there is much need to triangulate.\nIf a statement is clearly a speaker’s interpretation, there is little need to triangulate the quotation but not its content.\n\nStake (2006: 37) cites Denzin (1989) who highlighted several kinds of triangulation, leading to a few advisories:\n\nFind ways to use multiple rather than single observers of the same thing.\nUse second and third perspectives, i.e. the views of teachers, student and parents.\nUse more than one research method on the same thing, i.e. document review and interview.\nCheck carefully to decide how much the total description warrants generalization.\n\nDo your conclusions generalize across other times or places?\nDo your conclusions about the aggregate generalize to individuals?\nDo findings of the interaction among individuals in one group pertain to other groups?\nDo findings of the aggregate of these people generalized to a population?\n\n\n\n\nCross-Case Analysis Procedure\nStake (2006: Chapter 3) lays out a procedure for deriving synthetic findings from data collected across cases. He frames this in terms of a dialectic between cases and quintains. He identifies three tracks (Stake 2006: 46):\n\nTrack 1: Maintains the case findings and the situationality.\nTrack 2: Merges similar findings, maintaining a little of the situationality.\nTrack 3: The most quanitative track, shifts the focus from findings to factors.\n\nAccording to Stake, case reports should be created independently and then brought together by a single individual when working in a collaborative project. In keeping with the case-quintain dialectic, this integration must involve strategically putting the cases aside and bringing them back in to identify convergences and divergences, similarities and differences, normalitities and discrepancies among them.\nThere is some detailed discussion about different kinds of statements, i.e. themes, findings, factors and assertions, but I find this a bit too much detail for me to get at at this point in mymethodological planning. In general though, Stake documents a process whereby an analyst navigates back and forth between the general and the situational, presenting tentativr statements that are shored up, modified or discarded through testing compatability of the evidence across cases.\n\n\nSingle cases\nStake (2000) is concerned with identifying what can be learned from a single case. He (2000: 437) identifies three kinds of cases:\n\nIntrinsic case studies as being driven by a desire to understand the particular case.\nInstrumental case studies are examined “mainly to provide insight into an issue or to redraw a generalization.”\nCollective case studies “investigate a phenomenon, population or general condition”.\n\nStake (2000) frames case research around a tension between the particular and the general, which echoes the case-quintain dilemma he described in (Stake 2006: 4-6).\n\n\nSome scattered practical guidance\nStake (2006: 18-22) provides a detailed and realistic overview of common challenges involved in collaborative qualitative research. This could be handy in future work when planning a multicase project involving multiple researchers.\nStake (2006: 29-33) provides guidance on how to plan and conduct interviews in multicase research, including a series of helpful prompts and questions to ask yourself while designing the interview. One thing that stands out is his recommendation that an interview should be more about the interviewee than about the case. It’s necessary to find out about the interviewee to understand their interpretations, but what they reveal about the quintain is more important.\nOn page 34, Stake (2006) also provides some practical tips for documenting and storing data, after Huberman and Miles (1994).\nStake (2006: Chapter 4) includes a chapter on procedures for reporting the findings, and I may return to this later on once I need to initiative this phase of work. It addresses concerns about how to articulate comparisons, concerns about generalization, and how to handle advocacy based on findings.\nSee Stake (2006) Chapter 5 for a step-by-step overview of a multicase study analysis. The rest of the volume after that includes three very detailed examples from his own work."
  },
  {
    "objectID": "notes/methodology-notes.html#grounded-theory",
    "href": "notes/methodology-notes.html#grounded-theory",
    "title": "Methodology notes",
    "section": "Grounded theory",
    "text": "Grounded theory\nThese notes are largely drawn from Charmaz (2000), which I understand to be a fairly balanced and comprehensive overview of the Glaser / Strauss and Corbin debate, and of the situation of specific methods and techniques in relation to these different stances. I also value Charmaz’s position as someone who subscribes to her constructivist approach.\nAccording to Charmaz(2000: 509):\n\nEssentially, grounded theory methods consist of systematic inductive guidelines for collecting and analyzing data to build middle-range theoretical frameworks that explain the collected data.\n\nCharmaz(2000: 511) goes on to situate grounded theory in relation to what was the norm prior to its invention:\n\nGlaser and Strauss’s (1967) work was revolutionary because it challenged (a) arbitrary divisions between theory and research, (b) views of qualitative research as primarily a precursor to more “rigorous” quantitative methods, (c) claims that the quest for rigor made qualitative research illegitimate, (d) beliefs that qualitative methods are impressionistic and unsystematic, (e) separation of data collection and analysis, and (f) assumptions that qualitative research could produce only descriptive case studies rather than theory development (Charmaz 1995).\n\nPrior to Glaser and Strauss (1967), qualitative analysis was taught rather informally — they led the way in providing written guidelines for systematic qualitative data analysis with explicit procedures for data analysis (Charmaz 2000: 512)\nGlaser brought his very positivist assumptions from his work at Columbia, and Strauss’ work in Chicago with Herbert Blumer and Robert Park infused a pragmatic philosophical approach to the study of process, action and meaning that reflects symbolic interactionism.\n\nGlaser\nGlaser’s position comes close to traditional positivism, with assumptions of an objective, external reality and a neutral observer who discovers data. and a reductionist form of inquiry of manageable research problems. According to Charmaz (2000: 511), regarding Glaser’s approach:\n\nTheoretical categories must be developed from analysis of the collected data and must fit them; these categories must explain the data they subsume. This grounded theorists cannot shop their disciplinary stores for preconceived concepts and dress their data in them. Any existing concept must earn its way into the analysis. … The relevance of a grounded theory derives from its offering analytic explanations of actual problems and basic processes in the research setting. A grounded theory is durable because researchers can modify their emerging or established analyses as conditions change or further data are collected.\n\n\n\nCorbin and Strauss\nStrauss and Corbin assume an objective reality, aim toward unbiased data collection, propose a series of technical procedures, and espouses verification. However, they are postpositivism because they propose giving voice to their respondents,3 representing them as accurately as possible, discovering and reckoning with how their respodents’ views on reality differ from their own, and reflecting on the research process as one way of knowing.\nCorbin and Strauss (1990) “gained readers but lost the sense of emergence and open-ended character of Strauss’s earlier volume and much of his empirical work. The improved and more accessible second edition of Basics (Strauss and Corbin 1998) reads as less prescriptive and aims to lead readers to a new way of thinking about their research and about the world.” (Charmaz 2000: 512)\nStrauss apparently became more insistent that grounded theory should be more verificational in nature in personal communications.\nGlaser (1992) responded to Strauss and Corbin (1990), repudiating what he perceived as forcing preconceived questions and frameworks on the data. Glaser considered it better to allow theory to “emerge” from the data, i.e. to let the data speak for themselves.\nCharmaz identifies these two approaches as having a lot in common: hey both advocate for mitigating factors that would hinder objectivity and minimize intrusion of the researcher’s subjectivity, and they are both embedded in positivist attitudes, with a researcher sitting outside the observed reality; Glaser exemplifies these through discovering and coding data, and using systematic comparative methods, whereas Strauss and Corbin maintain a similar distance through their analytical questions, hypotheses and methodological applications. They both engage in “silent authorship” and usually write about their data as distant experts (Charmaz and Mitchell 1996).\n\n\nConstuctivist Grounded Theory\n\nConstructivist grounded celebrates firsthand knowledge of empirical worlds, takes a middle ground between postmodernsm and positivism, and offers accessible methods for taking qualitative research into the 21st century. (510)\n\n\nThe power of grounded theory lies in its tools for understanding empirical worlds. We can reclaim these tools from their positivist underpinnings to form a revised, more open-ended practice of grounded theory that stresses its emergent, constructivist elements. We can use grounded theory methods as flexible, heuristic strategies rather than as formulaic procedures. (510)\n\nThree aspects to Charmaz’s argument (510):4\n\nGrounded theory strategies need not be rigid or prescriptive;\na focus on meaning while using grounded theory furthers, rather than limits, interpretive understanding; and\nwe can adopt grounded theory strategies without embracing the positivist leanings of earlier proponents of grounded theory.\n\nRepudiation of the notion that data speak for themselves, that data do not lie. Recognition that data are constructs of the rsearch process, are framed by the questions we ask informants and the methodological tools of our collection procedures.\nCharmaz (2000: 515) advocates for what seems to be a dialogical approach to coding, between researcher and the data:\n\nWe should interact with our data and pose questions to them while coding. Coding helps us to gain a new perspective on our material and to focus further data collection, and may lead us in unforeseen directions. Unline quantitative research that requires data to fit into preconceived standardized codes, the researcher’s interpretations of data shape his or her emergent codes in grounded theory.\n\nDistinguishes articulates open/initial coding as proceeding line by line to get a general sense of what the data contains. It is meant to keep the researcher close to the data, to remain attuned to the subjects’ views of their realities.\n\nLine-by-line coding sharpens our use of sensitizing concepts — that is, those background ideas that inform the overall research problem. Sensitizing concepts offer eays of seeing, organizing, and understanding experience; they are embedded in our disciplinary emphases and perspectival proclivities. Although sensitizing conceots may deepen perception, they provide starting points for building analysis, not ending points for evading it. We may use sensitizing concepts only as points of departure from which to study the data.\n\nMuch of the rest of the Charmaz (2000) paper is an overview of coding and memoing methods, as well as theoretical sampling. The emphasis is on situating these techniques in the Glaser / Strauss and Corbin debate, and it will be better to refer to Charmaz (2014) for in-depth notes on these techniques.\nCharmaz (2000: 521-522) provides an apt account of a significant critique of grounded theory, and poses her constructivist approach as a potential means of resolving it. Specifically, she refers to the notion that grounded theory (as traditionally conceived by both Glaser and Strauss and Corbin) “fractures” the data, making them easier to digest in an analytical sense, but also making it more difficult to engage with in a holistic manner. This is precisely the point of the original approach, to present qualitative data as data — as conceived and valued by quantitative researchers, i.e. as discrete, corpuscular, disembodied, re-arrangable and distant entities. The text of these two large paragraphs is copied here:\n\nConrad (1990) and Riessman (1990) suggest that “fracturing the data” in grounded theory research might limit understanding because grounded theorists aim for analysis rather than the portrayal of subjects’ experience in its fullness. From a grounded theory perspective, fracturing the data means creating codes and categories as the researcher defines themes within the data. Glaser and Strauss (1967) propose this strategy for several reasons: (a) to help the researcher avoid remaining immersed in anecdotes and stories, and subsequently unconsciously adopting subjects’ perspectives; (b) to prevent the researcher’s becoming immobilized and overwhelmed by voluminous data; and (c) to create a way for the researcher to organize and interpret data. However, criticisms of fracturing the data imply that grounded theory methods lead to separating the experience from the experiencing subject, the meaning from the story, and the viewer from the viewed. In short, the criticisms assume that the grounded theory method (a) limits entry into subjects’ worlds, and thus reduces understanding of their experience; (b) curtails representation of both the social world and subjective experience; (c) relies upon the viewer’s authority as expert observer; and (d) posits a set of objectivist procedures on which the analysis rests.\nResearchers can use grounded theory methods to further their knowledge of subjective experience and to expand its representation while neither remaining external from it nor accepting objectivist assumptions and procedures. A constructivist grounded theory assumes that people create and maintain meaningful worlds through dialectical processes of conferring meaning on their realities and acting within them (Bury 1986; Mishler 1981). Thus social reality does not exist independent of human action. Certainly, my approach contrasts with a number of grounded theory studies, methodological statements, and research texts (see, e.g., Chenitz and Swanson 1986; Glaser 1992; Martin and Turner 1986; Strauss and Corbin 1990; Turner 1981). By adopting a constructivist grounded theory approach, the researcher can move grounded theory methods further into the realm of interpretive social science consistent with a Blumerian (1969) emphasis on meaning, without assuming the existence of a unidimensional external reality. A constructivist grounded theory recognizes the interactive nature of both data collection and analysis, resolves recent criticisms of the method, and reconciles positivist assumptions and postmodernist critiques. Moreover, a constructivist grounded theory fosters the development of qualitative traditions through the study of experience from the standpoint of those who live it.\n\nCharmaz’s (2000: 523) proposal for a re-visioned grounded theory poses research as a materializing process:\n\nA re-visioned grounded theory must take epistemological questions into account. Grounded theory can provide a path for researchers who want to continue to develop qualitative traditions without adopting the positivistic trappings of objectivism and universality. Hence the further development of a constructivist grounded theory can bridge past positivism and a revised future form of interpretive inquiry. A revised grounded theory preserves realism through gritty, empirical inquiry and sheds positivistic proclivities by becoming increasingly interpretive.\n\nCharmaz (2000: 523) addresses realism and truth in constructivist grounded theory, and explicitly relates it to Blumerian situated interactionism:\n\nA constructivist grounded theory distinguishes between the real and the true. The constructivist approach does not seek truth — single, universal, and lasting. Still, it remains realist because it addresses human realities and assumes the existence of real worlds. However, neither human realities nor real worlds are unidimensional. We act within and upon our realities and worlds and thus develop dialectical relations among what we do, think, and feel. The constructivist approach assumes that what we take as real, as objective knowledge and truth, is based upon our perspective (Schwandt 1994). The pragmatist underpinnings in symbolic interactionism emerge here. Thomas and Thomas (1928: 572) proclaim, “If human beings define their situations as real, they are real in their consequences”. Following their theorem, we must try to find what research participants define as real and where their definitions of reality take them. The constructivist approach also fosters our self-consciousness about what we attribute to our subjects and how, when, and why researchers portray these definitions as real. Thus the research products do not constitute the reality of the respondents’ reality. Rather, each is a rendering, one interpretation among multiple interpretations, of a shared or individual reality. That interpretation is objectivist only to the extent that it seeks to construct analyses that show how respondents and the social scientists who study them construct those realities — without viewing those realities as unidimensional, universal, and immutable. Researchers’ attention to detail in the constructivist approach sensitizes them to multiple realities and the multiple viewpoints within them; it does not represent a quest to capture a single reality.\nThus we can recast the obdurate character of social life that Blumer (1969) talks about. In doing so, we change our conception of it from a real world to be discovered, tracked, and categorized to a world made real in the minds and through the words and actions of its members. Thus the grounded theorist constructs an image of a reality, not the reality — that is, objective, true, and external.\n\nOn the other hand, Charmaz (2000: 524) frames objectivist grounded theory as believing in some kind of truth:\n\nObjectivist grounded theory accepts the positivistic assumption of an external world that can be described, analyzed, explained, and predicted: truth, but with a small t. That is, objectivist grounded theory is modifiable as conditions change. It assumes that different observers will discover this world and describe it in similar ways That’s correct — to the extent that subjects have comparable experiences (e.g., people with different chronic illnesses may experience uncertainty, intrusive regimens, medical dominance) and viewers bring similar que-tions, perspectives, methods, and, subsequently, concepts to analyze those experiences. Objectivist grounded theorists often share assumptions with their research participants — particularly the professional participants. Perhaps more likely, they assume that respondents share their meanings. For example, Strauss and Corbin’s (1990) discussion of independence and dependence assumes that these terms hold the same meanings for patients as for researchers.\n\nCharmaz (2000: 525) further embeds construvist grounded theory as a way to fulfill Blumer’s symbolic interactionism:\n\nWhat helps researchers develop a constructivist grounded theory? How might they shape the data collection and analysis phases? Gaining depth and understanding in their work means that they can fulfill Blumer’s (1969) call for “intimate familiarity” with respondents and their worlds (see also Lofland and Lofland 1984, 1995). In short, constructing constructivism means seeking meanings — both respondents’ meanings and researchers’ meanings.\n\nCharmaz (2000: 524) on the concretization of procedures from what were orginally meant to be guidelines:\n\nGuidelines such as those offered by Strauss and Corbin (1990) structure objectivist grounded theorists’ work. These guidelines are didactic and prescriptive rather than emergent and interactive. Sanders (1995: 92) refers to grounded theory procedures as “more rigorous than thou instructions about how information should be pressed into a mold”. Strauss and Corbin categorize steps in the process with scientific terms such as axial coding and conditional matrix (Strauss 1987; Strauss and Corbin 1990, 1994). As grounded theory methods become more articulated, categorized, and elaborated, they seem to take on a life of their own. Guidelines turn into procedures and are reified into immutable rules, unlike Glaser and Strauss’s (1967) original flexible strategies. By taking grounded theory methods as prescriptive scientific rules, proponents further the positivist cast to obiectivist grounded theory.\n\n\nFrom Denzin (2019: 450):\n\nLike Charmaz, Clarke’s postmodern, situational analysis (SA), deploys constructivist models of truth, and rejects objectivist views of the inquirer. Compared to traditional GT, Clarke’s social arena framework privileges poststructural feminism, Foucault over the study of analytic maps, systems of discourse, and the effect of nonhuman (buildings, technology) elements on situations of inquiry (Clarke 2005: 32, 291; Clarke 2015; Clarke, Friese, and Washburn 2016).\nYet underneath, these differences, there are commonalities: flexible guidelines for data collection (and analysis), including interviewing, archival analysis, observation, and participant observation. Most importantly, the commitment is to remain close to the world being studied, while developing integrated theoretical concepts grounded in data that show process, relationship, and social world connectedness (Charmaz 2005: 508; Clarke 2005: 292; Clarke, Friese, and Washburn 2015).\n\nThis seems to be elaborated in parts of Mruck and Mey (2019).\n\nOn the modes of reasoning behind grounded theory\nKelle (2005) is an overview of the Glaser / Strauss and Corbin split. References to Kelle (2005) have no page numbers since it is published in an online-only journal and does not specify paragraph numbers.\nHighlights a primary impetus behind Glaser and Strauss (1967), which used political analogies to distinguish between “theoretical capitalists” and “proletariat testers”, and unify the field of sociology by de-centering emphasis on theories developed by “great men”.\nA common thread in this paper is sensitivity to the practical challenges of actually doing grounded theory according to Glaser’s approach:\n\nThe infeasibility of an inductivist research strategy which demands an empty head (instead of an “open mind”) cannot only be shown by epistemological arguments, it can also be seen in research practice. Especially novices in qualitative research with the strong desire to adhere to what they see as a basic principle and hallmark of Grounded Theory — the “emergence” of categories from the data — often experience a certain difficulty: in open coding the search for adequate coding categories can become extremely tedious and a subject of sometimes numerous and endless team sessions, especially if one hesitates to explicitly introduce theoretical knowledge. The declared purpose to let codes emerge from the data then leads to an enduring proliferation of the number of coding categories which makes the whole process insurmountable.\n\nKelle (2005) basically takes down the original Glaser and Strauss (1967) and subsequent reflection on theoretecal sensitivity (Glaser 1978). He highlights fundamental contraditions and oversights with regards to the role of theory in grounded theory, specifically with regards to the notion that such research can be accomplished with inductive purity:\n\nConsequently, in the most early version of Grounded Theory the advice to employ theoretical sensitivity to identify theoretical relevant phenomena coexists with the idea that theoretical concepts “emerge” from the data if researchers approach the empirical field with no preconceived theories or hypotheses. Both ideas which have conflicting implications are not integrated with each other in the Discovery book. Furthermore, the concept of theoretical sensitivity is not converted into clear cut methodological rules: it remains unclear how a theoretically sensitive researcher can use previous theoretical knowledge to avoid drowning in the data. If one takes into account the frequent warnings not to force theoretical concepts on the data one gets the impression that a grounded theorist is advised to introduce suitable theoretical concepts ad hoc drawing on implicit theoretical knowledge but should abstain from approaching the empirical data with ex ante formulated hypotheses.\n\nKelle (2005) recognizes that Glaser identified a series of “theoretical families” to help assist with the practical experience of coding. I find it somewhat interesting that many of the terms in these first families are very reminiscient of so-called “natural language”, as used in the wave of cybernets that was contemporary with Glaser (1978) and which largely dealt with “expert systems”.\n\nIn the book “Theoretical Sensitivity” (1978) GLASER presents an extended list of terms which can be used for the purpose of theoretical coding loosely structured in the form of so called theoretical “coding families”. Thereby various theoretical concepts stemming from different (sociological, philosophical or everyday) contexts are lumped together, as for example:\n\nterms, which relate to the degree of an attribute or property (“degree family”), like “limit”, “range”, “extent”, “amount” etc.,\nterms, which refer to the relation between a whole and its elements (“dimension family”), like “element”, “part”, “facet”, “slice”, “sector”, “aspect”, “segment” etc.,\nterms, which refer to cultural phenomena (“cultural family”) like “social norms”, “social values”, “social beliefs” etc.\n\n\nThis is substantiated by other observations by Kelle (2005) that ad hoc coding actually follows implicit theoretical knowledge:\n\nOne of the most crucial differences between GLASER’s and STRAUSS’ approaches of Grounded Theory lies in the fact that STRAUSS and CORBIN propose the utilization of a specified theoretical framework based on a certain understanding of human action, whereas GLASER emphasises that coding as a process of combining “the analyst’s scholarly knowledge and his research knowledge of the substantive field” (1978, p.70) has to be realised ad hoc, which means that it has often to be conducted on the basis of a more or less implicit theoretical background knowledge.\n\nand that the Glaserian approach is better suited for more experienced, rather than novice sociologists, who will have internalized the theory that they then apply in their coding.\nKelle then goes on to address how grounded theory can or can not be applied in alignment with inductivist or hypothetic-deductivist reasoning, and raises abductive reasoning an an alternative means of arriving at legitimate and verifiable conclusions. There is too much detail in the paper to copy here.\nBut here is another nice conclusive gem from the end:\n\nWhereas STRAUSS and CORBIN pay a lot of attention to the question how grounded categories and propositions can be further validated, GLASER’s concept shows at least a gleam of epistemological fundamentalism (or “certism”, LAKATOS 1978) especially in his defence of the inductivism of early Grounded Theory. “Grounded theory looks for what is, not what might be, and therefore needs no test” (GLASER 1992, p.67). Such sentences carry the outmoded idea that empirical research can lead to final certainties and truths and that by using an inductive method the researcher may gain the ability to conceive “facts as they are” making any attempt of further corroboration futile.\n\n\n\nRebuttals by Glaser\nGlaser (2002) constitutes a rebuttal to Charmaz (2000). As Bryant (2003) points out in his response to Glaser (2002), it is very angry, polemical and irrational. I don’t want to go too in depth with the fundamental problems with Glaser’s response (see Bryant’s paper for the details), but the gist is that Glaser never really got the message about data being inherently constructed by researchers decisions, actions and circumstances. Glaser seems to continue believing in the inherent neutrality of data as a matter of faith.\nThis being said, Glaser (2002) did highlight the large emphasis on descriptive rather than explanatory potential in Charmaz’s approach. This aligns with my own apprehensions when I try to address the relevance of my work. I tend to use the term “articulate” as a way to frame my work as descriptive, but in a way that lends value, and this very fuzzy distinction between the power of identying the shapes and relationships among things and explaining their causes and effects in a generalizable way (i.e., theories, or explanations), still somehow troubles me. I wonder if Glaser is drawing a false distinction here, and through that, a false prioritization of explanation over description as a desired outcome. This would put my mind at ease, as would dismissing Glaser’s dismissal of people who simply don’t know how to do the “real” grounded theory (which, in his mind, include all feminist and critical researchers).\n\n\nOn the functional and pragmatic roots of grounded theory\nI completely agree with this statement from Clarke (2003: 555):\n\nTo address the needs and desires for empirical understandings of the complex and heterogeneous worlds emerging through new world orderings, new methods are requisite (Haraway 1999). I believe some such methods should be epistemologically/ ontologically based in the pragmatist soil that has historically nurtured symbolic interactionism and grounded theory. Through Mead, an interactionist grounded theory has always had the capacity to be distinctly perspectival in ways fully com patible with what are now understood as situated knowledges. This fundamental and always already postmodern edge of a grounded theory founded in symbolic interactionism makes it worth renovating.\n\nThis is super interesting, and really contextualizes how Strauss imagined grounded theory to be useful for him:\n\nSome years ago, Katovich and Reese (1993:400–405) interestingly argued that Strauss’s negotiated order and related work recuperatively pulled the social around the postmodern turn through its methodological [grounded theoretical] recognition of the partial, tenuous, shifting, and unstable nature of the empirical world and its constructedness. I strongly agree and would argue that Strauss also furthered this “postmodernization of the social” through his conceptualizations of social worlds and arenas as modes of understanding the deeply situated yet always also fluid orga nizational elements of negotiations. He foreshadowed what later came to be known as postmodern assumptions: the instability of situations; characteristic changing, porous boundaries of both social worlds and arenas; social worlds seen as mutually constitutive/coproduced through negotiations taking place in arenas; negotiations as central social processes hailing that “things can always be otherwise”; and so on. Significantly, negotiations constitute discourses that also signal micropolitics of power as well as “the usual” meso/macrostructural elements—power in its more fluid forms (e.g., Foucault 1979, 1980). Through integrating the social worlds/arenas/ negotiations framework with grounded theory as a new conceptual infrastructure, I hope to sustain and extend the methodological contribution of grounded theory to understanding and elaborating what has been meant by “the social” in social life — before, during, and after the postmodern turn.\n\nIt also echoes Charmaz’s vision of grounded theory as a powerful too, and Bryant’s (2003) call to “look at what Glaser and Strauss actually did, rather than what they claimed — and continued to claim — they were doing” to uncover “the basis for a powerful research approach”. Bryant (2003) further cites Baszanger and Dodier (1997), who characterize grounded theory as a method “consisting of accumulating a series of individual cases, of analyzing them as a combination between different logics of action that coexist not only in the field under consideration, but even within these individuals or during their encounters”. Bryant (2003) summarizes this by stating that “[t]he aim of such methods is generalization rather than totalization, with the objective of producing”a combinative inventory of possible situations”."
  },
  {
    "objectID": "notes/methodology-notes.html#theoretical-sampling",
    "href": "notes/methodology-notes.html#theoretical-sampling",
    "title": "Methodology notes",
    "section": "Theoretical sampling",
    "text": "Theoretical sampling\nFrom Charmaz (2000: 519):\n\nWe use theoretical sampling to develop our emerging categories and to make them more definitive and useful. Thus the aim of this sampling is to refine ideas, not to increase the size of the original sample. Theoretical sampling helps us to identify conceptual boundaries and pinpoint the fit and relevance of our categories.\n\nFrom Charmaz (2000: 519) on the role of theoretical sampling in re-iterative data collection:\n\nThe necessity of engaging in theoretical sampling means that we researchers cannot produce a solid grounded theory through one-shot interviewing in a single data collection phase. Instead, theoretical sampling demands that we have completed the work of comparing data with data and have developed a provisional set of relevant categories for explaining our data. In turn, our categories take us back to the field to gain more insight about when, how, and to what extent they are pertinent and useful.\n\nFrom Charmaz (2000: 520) on the notion of “saturation”:\n\nGrounded theory researchers take the usual criteria of “saturation” (i.e., new data fit into the categories already devised) of their categories for ending the research (Morse 1995). But what does saturation mean? In practice, saturation seems elastic (see also Flick 1998; Morse 1995). Grounded theory approaches are seductive because they allow us to gain a handle on our material quickly. Is the handle we gain the best or most complete one? Does it encourage us to look deeply enough? The data in works claiming to be grounded theory pieces range from a handful of cases to sustained field research. The latter more likely fulfills the criterion of saturation and, moreover, has the resonance of intimate familiarity with the studied world. As we define our categories as saturated (and some of us never do), we rewrite our memos in expanded, more analytic form. We put these memos to work for lectures, presentations, pa-pers, and chapters. The analytic work continues as we sort and order memos, for we may discover gaps or new relationships.\n\nFrom Clarke (2003: 557):\n\nUnique to this approach has been, first, its requiring that analysis begin as soon as there are data. Coding begins immediately, and theorizing based on that coding does as well, however provisionally (Glaser 1978). Second, “sampling” is driven not necessarily (or not only) by attempts to be “representative” of some social body or population (or its heterogeneities) but especially and explicitly by theoretical concerns that have emerged in the provisional analysis. Such “theoretical sampling” focuses on finding new data sources (persons or things) that can best explicitly address specific theoretically interesting facets of the emergent analysis. Theoretical sampling has been integral to grounded theory from the outset, remains a fundamental strength of this analytic approach, and is crucial for the new situational analyses.\n\n\nButler, Copnell, and Hall (2018) provide concrete examples of theoretical sampling in practice. They point out that many studies that claim to follow grounded theory do not adequately document their implementation of theoretical sampling, providing no evidence of how it was used or failing to link theoretical sampling to stages of theory development.\nButler, Copnell, and Hall (2018) on the difference between “purposeful” sampling which occurs at the start of a project, and theoretical sampling whichn occurs after you get the ball rolling:\n\nIn constructivist grounded theory studies, data collection begins with purposeful sampling. Initial participants or sources of data are chosen based on their experiences of the area under study or abil- ity to inform the early research questions (Charmaz 2014; Currie 2009). However, according to Charmaz (2014), this early sampling strategy offers only a starting point: somewhere to launch the data collection process rather than a definitive strategy to develop the overall theory. The criteria used in early purposeful sampling are not the same as those used during the theoretical sampling process. Instead, the criteria which guide theoretical sampling decisions change throughout a study, as ideas and insights into the data develop and change.\n\nButler, Copnell, and Hall (2018) on identifying when to switch from purposeful to theoretical sampling:\n\nThough some grounded theorists believe that theoretical sampling can start after a single interview, suggesting that all that is required are beginning concepts that warrant further exploration (Corbin and Strauss 2014), Charmaz (2014) asserts that theoretical sampling cannot begin until tentative categories have developed, which is unlikely to occur after a single interview. This is because, from a constructivist standpoint, the purpose of theoretical sampling is to narrow the researcher’s focus towards the developing categories in order to refine them, explore their boundaries, identify their properties, and discover relationships between them (Charmaz 2014).\n\nButler, Copnell, and Hall (2018) on identifying when saturation occurs (the point at which no category properties are gleaned when new data are added, and the categories are robust enough to encompass the variations present in the study):\n\nTheoretical sampling contin-ues until data saturation occurs. It is often difficult to determineexactly when this occurs, but for most grounded theorists, saturation marks the point at which no new properties of the categoriesare gleaned when new data is added, and the categories are robustenough to encompass the variations present in the study (Charmaz 2014; Maz 2013).\n\nThe examples provided by Butler, Copnell, and Hall (2018) focus on how theoretical sampling helped introduce new research sites, adapt the interview questions, and seeking new participant characteristics.\n\nMorse and Clark (2019: 146) on the purpose of sampling in qualitative research: “In qualitative inquiry, sampling enables access to new dimensions of the topic that arise during reflexive inquiry.”\nThey go on:\n\nIn qualitative inquiry, first sampling is based on the researcher’s need to understand the phenomenon. The researcher’s understanding builds incrementally as the study pro gresses: who is invited to participate in the study (i.e., the sample) is determined by what they know about the topic: that is, what they may contribute –– their experience, role, and so forth. As this requisite knowledge changes throughout the study, so does the type of par ticipant who is invited to participate change.\n\nThe rest of Morse and Clark (2019) identifies various factors with regards to sample coverage that are important to consider as the project evolves, and suggests ways to refine the sample as the work progresses. I think I may return to this if/when I hit any roadblocks, but at this point I’m not getting much out of this text."
  },
  {
    "objectID": "notes/methodology-notes.html#interviews",
    "href": "notes/methodology-notes.html#interviews",
    "title": "Methodology notes",
    "section": "Interviews",
    "text": "Interviews\nFrom Charmaz (2000: 525):\n\nA constructivist approach necessitates a relationship with respondents in which they can cast their stories in their terms. It means listening to their stories with openness to feeling and experience. … Furthermore, one-shot interviewing lends itself to a partial, sanitized view of experience, cleaned up for public discourse. The very structure of an interview may preclude private thoughts and feelings from emerging. Such a structure reinforces whatever proclivities a respondent has to tell only the public version of the story. Researchers’ sustained involvement with research participants lessens these problems.\n\nFontana and Frey (2000) spend some time writing about the emergence of an “interview society”, whereby interviews are commonly used to seek various forms of biographical information. They cite Gubrium and Holstein (1998), who noted that “the interview has become a means of contemporary storytelling, where persons divulge life accounts in response to interview inquiries”. They then go over a brief history of interviewing in the context of sociological research, which largely tracks the values underlying positivist and postmodernist transitions as you might expect.\nYin (2014: 110-113) differentiates between three kinds of interviews:\n\nProlonged interviews: Usually over two hours long, or over an extended period of time covering multiple sittings.\n\nYou can ask interviewees about their interpretations and opinions about people and events or their insights, explanations, and meanings related to certain occurrences. You can then use such propositions as the basis for further inquiry, and the interviewee can suggest other persons for you to interview, as well as other sources of evidence. The more that an interviewee assists in this manner, the more that the role may be considered one of an “informant” rather than a participant. Key informants are often critical to the success of a case study. Such persons can provide you with insights into a matter and also give you access to other interviewees who may have corroboratory or contrary evidence.\n\nShorter interviews: More focused, around one hour, open-ended and conversational but largely following the protocol.\n\nA major purpose of such an interview might simply be to corroborate certain findings that you already think have been established, but not to ask about other topics of a broader, open-ended nature. In this situation, the specific questions must be carefully worded, so that you appear genuinely naive about the topic and allow the interviewee to provide a fresh commentary about it; in contrast, if you ask leading questions, the corroboratory purpose of the interview will not have been served. … As an entirely different kind of example, your case study protocol might have called for you to pay specific attention to an interviewee’s personal rendition of an event. In this case, the interviewee’s perceptions and own sense of meaning are the material to be understood. … In both [situations], you need to minimize a methodological threat created by the conversational nature of the interview.\n\nSurvey interviews: A more structured questionnaire. Usually works best as one component of multiple sources of evidence.\n\n\nStructured interviewing\nFrom Fontana and Frey (2000: 649-651):\nInterviewers ask respondents a series of preestablished questions with a limited set of response categories. The interview records responses according to a preestablished coding scheme.\nInstructions to interviewers often follow these guidelines:\n\nNever get involved in long explanations of the study; use the standard explanation provided by the supervisor.\nNever deviate from the study introduction, sequence of questions, or question wording.\nNever let another person interrupt the interview; do not let another person answer for the respondent or offer his or her opinions on the question.\nNever suggest an answer or agree or disagree with an answer. Do not give the respondent any idea of your personal views on the topic of the question or the survey.\nNever interpret the meaning of a question; just repeat the question and give instructions or clarifications that are provided in training or by the supervisors.\nNever improvise, such as by assing answer categories or making wording changes.\n\nThe interviewer must establish a “balanced rapport”, being casual and friendly while also directive and impersonal. Interviewers must also perfect a style of “interested listening” that rewards respondents’ participation but does not evaluate their responses.\nFrom Fontana and Frey (2000: 651):\n\nThis kind of interview often elicits rational responses, but it overlooks or inadequately assesses the emotional dimension.\n\n\nMorse and Clark (2019: 150) describe various interview strategies in relation to their reflexive potential vis-a-vis theoretical sampling. With regards to semi-structured interviews,5 which is my favoured approach for this project, they note that this technique is often used in grounded theory, “because data are analyzed all at once at the end of data collection, much of the reflexivity required for the sampling strategies necessary for excellent grounded theory … is lost”.\n\n\nGroup interviews\nFrom Fontana and Frey (2000: 651-652):\nCan be used to test a methodological technique, try out a definition of a research problem or to identify key informants. Pre-testing a questionnaire or survey design.\nCan be used to aid respondents’ recall of specific events or to stimulate embellished descriptions of events, or experiences shared by members of a group.\nIn formal group interviews, participants share views through the coordinator.\nLess formal group interviews are meant to establish the widest range of meaning and interpretation on a topic, and the objective is “to tap intersubjective meaning with depth and diversity”.\n\n\nUnstructured interviewing\nFrom Fontana and Frey (2000: 652-657):\nThe essence of an unstructured interview is establishing a human-to-human relation with the respondent and a desire to understand rather than to explain.\nFontana and Frey (2000) then goes on with some practical guidance on how to engage in unstructured interviews, largely concerned with how to access a community and relate with respondents.\n\n\nPostmodern takes on interviewing\nFontana and Frey (2000) address some “new” takes on interviewing emerging from the postmodern turn. I kinda think there is some potential behind approaches that emphasize interviews as negotiated accomplishment, or product of communal sensemaking between interviewer and respondent. I think it could be really helpful in the context of my research, which is very concerned with drawing out tensions that respondents have in mind but are not really able to articulate in a systematic way.\nHowever, Fontana and Frey (2000) also draws attention to crticism of highly engaged interviewing approaches, which seem to equate closeness with the respondent as getting closer to their “true self”, and which may not actually be fixed (especially in the context of the artificial environment of the interview setting). Critiques of such “closeness” use the term “romantic” or “crusading” as epithets. Moreover, there is additional reference to the culturally-embedded assumption that interviews are necessarily valuable sources of information, as if speaking ones mind can adequately convey one’s thoughts and experiences — this is criticized as a particularly western approach to information extraction surrounding internalized and externalized thoughts and behaviour, as instilled through participation in the “interview society” addressed earlier in the text.\n\n\nTranscribing\nThis section describes how I transcibe interviews and accounts for the decisions to encode certain things and not others. It goes on to explains the procedures for transcribing spoken dialog into textual formats, including the notation applied to encode idiosyncratic elements of conversational speech.\n\nTranscript notation\nDerived from the transcription protocol applied for the E-CURATORS project.\n\n\nCleaning audio\nTo clean the audio:\n\nSelect a clip that is representative of a single source of background noise, and then filter that wavelength throughout the entire audio file.\nAfter selecting the clip, go to Effect &gt;&gt; Noise Reduction and select Get Noise Profile, then press OK.\nClose the noise reduction menu, select the entire range of audio using the keyboard shortcut Command + A.\nThen go back to the noise reduction window (Effect &gt;&gt; Noise Reduction) to apply the filter based on the noise profile identified for the noisy clip.\nExport the modified audio file to the working directory (File &gt;&gt; Export &gt;&gt; Export as .WAV).\nUse ffmpeg to replace the dirty audio track with the clean one:\n\n  ffmpeg -i dirty.mp4 -i clean.wav -c:v copy -map 0:v:0 -map 1:a:0 clean.mp4"
  },
  {
    "objectID": "notes/methodology-notes.html#field-notes",
    "href": "notes/methodology-notes.html#field-notes",
    "title": "Methodology notes",
    "section": "Field notes",
    "text": "Field notes\nSee (Yin 2014: 124-125)"
  },
  {
    "objectID": "notes/methodology-notes.html#focus-groups",
    "href": "notes/methodology-notes.html#focus-groups",
    "title": "Methodology notes",
    "section": "Focus groups",
    "text": "Focus groups\nFrom Kitzinger (1994); Wilkinson (1998); Parker and Tritter (2006); Morgan and Hoffman (2018).\n\nCrucially, focus groups involve the interaction of group participants with each other as well as with the moderator, and it is the collection of this kind of interactive data which distinguishes the focus group from the one-to-one interview (c.f. Morgan 1988), as well as from procedures which use multiple participants but do not permit interactive discussion (c.f. Stewart and Shamdasani 1990). The ‘hallmark’ of focus groups, then, is the ‘explicit use of group interaction to produce data and insights that would be less accessible without the interaction found in a group’ (Morgan 1997: 2).\n\nWilkinson (1998) identifies three key features of focus group methods:\n\nproviding access to participants’ own language, concepts and concerns;\n\n\n“The relatively free flow of discussion and debate between members of a focus group offers an excellent opportunity for hearing ‘the language and vernacular used by respondents’”\nUseful for gaining insight into participants’ conceptual worlds on their own terms\n“Sometimes, the participants even offer the researcher a ‘translation’ of unfamiliar terms or concepts”\n“Focus group interactions reveal not only shared ways of talking, but also shared experiences, and shared ways of making sense of these experiences. The researcher is offered an insight into the commonly held assumptions, concepts and meanings that constitute and inform participants’ talk about their experiences.”\n\n\nencouraging the production of more fully articulated accounts;\n\n\n“In focus groups people typically disclose personal details, reveal discrediting information, express strong views and opinions. They elaborate their views in response to encouragement, or defend them in the face of challenge from other group members: focus groups which ‘take off’ may even, like those run by Robin Jarrett (1993: 194) have ‘the feel of rap sessions with friends’.”\n“Even when bcus group participants are not acquainted in advance, the interactive nature of the group means that participants ask questions of, disagree with, and challenge each other, thus serving ‘to elicit the elaboration of responses’ (Merton 1987: 555).”\n“Other ethical issues in focus group research stem from group dynamics, insofar as participants can collaborate or collude effectively to intimidate and/or silence a particular member, or to create a silence around a particular topic or issue, for example. In such cases, it falls to the group moderator to decide whether/how to intervene, and it can be difficult to balance such conflicting goals as ensuring the articulation of accounts, supporting individuals, and challenging offensive statements.”\n\n\nand offering an opportunity to observe the process of collective sense-making.\n\n\nFocus groups also offer an opportunity for researchers to see exactly how views are constructed, expressed, defended and (sometimes) modified during the course of conversations with others, 1.e. to observe the process of collective sense-making in action. This is likely to be of particular interest to researchers working within a social constructionist framework, who do not view beliefs, ideas, opinions and understandings as generated by individuals in splendid isolation, but rather as built in interaction with others, in specific social contexts: as Radley and Billig (1996: 223) say, ‘thinking is a socially shared activity’. In a focus group, people are confronted with the need to make collective sense of their individual experiences and beliefs (Morgan and Spanish 1984: 259).\n\nWilkinson (1998) addresses opportunities and challenges in analyzing focus group data using content analysis and ethnographic techniques.\nWith regards to content analysis:\n\nthe main advantages of content analysis are to allow for a relatively systematic treatment of the data and to enable its presentation in summary form. … the researcher has first to decide on the unit of analysis: this could be the whole group, the group dynamics, the individual participants, or the participants’ utterances\n\n\nMorgan (1997) proposes three distinct ways of coding focus group data: noting whether each group discussion contains a given code; noting whether each participant mentions a given code; and noting all mentions of a given code (i.e. across groups or participants). Once the data have been coded in one (or more) of these ways, the question of whether to quantify them is a further issue. Morgan (1993) argues the value of simple ‘descriptive counts’ of codes (stopping short of using inferential statistical tests, whose assumptions are unlikely to be met in focus groups)\n\nAnd with regards to ethnographic analysis:\n\nIts main advantage is to permit a detailed interpretative account of the everyday social processes of communication, talk and action occurring within the focus group. The key issue in ethnographic analysis is how to select the material to present (whether this is framed up as ‘themes’, ‘discourses’, or simply as illustrative quotations), without violating the ‘spirit’ of the group, and without losing sight of the specific context within which the material was generated. … A particular challenge is how to preserve the interactive nature of focus group data: a surprising limitation of published focus group research is the rarity with which group interactions are analysed and reported ( c.f. Carey and Smith 1994, Kitzinger 1994a). Extracts from focus group data are most commonly presented as if they were one-to-one interview data, often with no indication that more than one person is present; still more rarely does interaction per se constitute the analytic focus.\n\n\nKitzinger (1994) elaborates on the interactive nature of focus groups, which she identifies as the methods’ core feature.\n\nFocus groups are group discussions organised to explore a specific set of issues sucb as people’s views and experiences … The group is ‘focused’ in the sense that it involves some kind of collective activity - such as viewing a film, examining a single health education message or simply debating a particular set of questions.\n\n\nEven when group work is explicitly included as part of the research it is often simply employed as a convenient way to illustrate a theory generated by other methods or as a cost-effective technique for interviewing several people at once. Reading some such reports it is hard to believe that there was ever more than one person in the room at the same time. This criticism even applies to many studies which explicitly identify their methodology as ‘focus group discussion’ — in spite of the fact that the distinguishing feature of focus groups is supposed to be the use of interaction as part of the research data.\n\n\nIt would be naive, however, to assume that group data is by definition ‘natural’ in the sense that it would have occurred without the group having been convened for this purpose. It is important to note that although, at times, the focus groups may approximate to participant observation the focus groups are artifidally set up situations. Rather than assuming the group session unproblematically and inevitably reflects ‘everyday interactions’ (although sometimes it will) the group sbould be used to encourage people to engage with one another, verbally formulate their ideas and draw out the cognitive structures which previously have been unarticulated.\n\nOn the active role of the facilitator:\n\nSessions were conducted in a relaxed fashion with minimal intervention from the facilitator - at least at first. This allowed the facilitator to ‘find her feet’ and permitted the research participants to set the priorities. However, the researcber was never passive. Trying to maximise interaction between participants could lead to a more interventionist style: urging debate to continue beyond the stage it might otberwise have ended, challenging people’s taken for granted reality and encouraging them to discuss the inconsistencies both between participants and within their own thinking.\n\nOn the role of the presentation or activity:\n\nSuch exercises not only provided invaluable data from each group but allow for some cross-compadsons between groups. Each discussion session has its own dynamic and direction — when it comes to analysis it is extremely useful to have a common external reference point such as that provided by the card game or the use of vignettes (Kban and Manderson 1992).\n\nOn complementary interactions:\n\nThe excbange between the research participants not only allows the researcher to understand which advertisement they are talking about but to gather data on their shared perception of that image.\n\n\nBrainstorming and loose word association was a frequent feature of the research sessions.\n\n\npeople’s knowledge and attitudes are not entirely encapsulated in reasoned responses to direct questions. Everyday forms of communication such as anecdotes, jokes or loose word association may tell us as much, if not more, about what people ‘know’. In this sense focus groups ‘reach the parts that other methods cannot reach’ - revealing dimensions of understanding that often remain untapped by the more conventional one-to-one interview or questionnaire.\n\n\nIn addition to the advantages discussed above fo exampie, discussing whether they had the ‘right’ to know if another child in the piay group bad had virus asserted that ‘you think of your own first’. It was this phrase, and these sort of sentiments, which seemed to capture their consent and resulted in nods of agreement round the group and assertions that ‘that’s right’ and ‘of course’. Indeed, it was often the strength of the collective reaction that highlighted the specific context within which the research participants experienced AIDS information.\n\nOn argumentative interactions:\n\nthe group process however, is not only about consensus and the articulation of group norms and experiences. Differences between individuals within the group are equally important and, in any case, rarely disappear from view. Regardless of how they are selected, the research participants in any one group are never entirely homogenous. Participants do not just agree with each other — they also misunderstand one another, question one another, try to persuade each other of the justice of their own point of view and sometimes they vehemently disagree.\n\n\nSuch unexpected dissent led them to clarify why they thpught as they did, often identifying aspects of their personal experience which bad altered their opinions or specific occasions which had made them re-think their point of view. Had the data been collected by interviews the researcber might have been faced with ‘arm chair’ theorizing about the causes of such difference but in a focus group these can be explored ‘in situ’ with the help of the research participants.\n\n\nClose attention to the ways in which research participants tell stories to one another aiso prevents the researcber from assuming that she knows ‘the meaning’ of any particular anecdote or account.\n\n\nParker and Tritter (2006) discuss “key issues relating to the complexity and necessity of considering sampling issues within the context of focus group research and the implications this has for the collection and analysis of resultant data.”\nIdentifies some common logistical impetus for focus groups, relating to acquisition of funding:\n\nIncreasing pressure from research funding organizations to adopt multiple-method research strategies and the fact that focus groups generate far more data than a range of other methods in relation to face-to-face contact between researchers and participants, has added to [the method’s popularity].\n\nCalls out the conflation between focus groups and group interviews:\n\nA similarly pervasive trend in and around academic discussion of qualitative research methods is that focus groups are sometimes seen as synonymous with group interviews and it is this issue which constitutes our second point of contention. … In keeping with the views of a number of other writers in this field, we are of the opinion that there is a fundamental difference between these two research techniques and that the critical point of distinction surrounds the role of the researcher and her/ his relationship to the researched (Smithson, 2000). In group interviews the researcher adopts an ‘investigative’ role: asking questions, controlling the dynamics of group discussion, often engaging in dialogue with specific participants. This is premised on the mechanics of one-to-one, qualitative, in-depth interviews being replicated on a broader (collective) scale. A relatively straightforward scenario ensues: the researcher asks questions, the respondents relay their ‘answers’ back to the researcher. In focus groups the dynamics are different. Here, the researcher plays the role of ‘facilitator’ or ‘moderator’; that is, facilitator/moderator of group discussion between participants, not between her/himself and the participants. Hence, where focus groups are concerned, the researcher takes a peripheral, rather than a centre-stage role for the simple reason that it is the inter-relational dynamics of the participants that are important, not the relationship between researcher and researched (see Kitzinger, 1994a; Johnson, 1996).\n\n\nPart of the problem of achieving this kind of interactional synergy in data collection is that, despite their collective interests, participants may not always be keen to engage with each other, or alternatively, may know each other so well that interaction is based on patterns of social relations that have little to do with the research intent of the focus group. The need to consider the impact on interaction of the constitution of the focus group requires that close attention be paid to methods (and outcomes) of recruit ment. … issues of sampling and selection are likely to prove crucial in relation to the form and quality of interaction in a focus group and therefore the kinds of data one gathers and the extent to which participants share their opinions, attitudes and life experiences.\n\nA large part of what follows is an extremely honest account of the challenges involved in recruiting participants to sit for focus groups. The pressure on students to recruit their friends impacted their views on what views were represented and not represented. Moreover, they draw attention to the role of timing and convenience in recruitment strategy, and representativeness of expressed viewpoints.\nI found it hard to take notes on this since it comes off as a continuous stream of conciousness. However it should still be read from top to bottom to get a sense of practical challenges that are not commonly addressed, save for among veterans and in private circles.\n\nFrom Morgan and Hoffman (2018: 251), who define the key strengths and benefits of focus groups:\n\nThe strength of focus groups in this regard is the variety of different perspectives and experiences that participants reveal during theirinterac tive discussion. This is especially important in the twin processes of sharing and comparing, which create dynamics that are not available in individual interviews. This means that focus groups are especially useful for investigating the extent of both consensus and diversity among the participants, as they engage in sharing and comparing among themselves with the moderator in a facilitating role. By comparison, individual interviews provide a degree of depth and detail on each participant that is not available in focus groups.\n\nFrom Morgan and Hoffman (2018: 251-252) on complementary aspects of focus groups and individual interviews:\n\nOften, they are best seen as complementary rather than competing methods. For example, individual key inform ant interviews can be a good starting point for planning a future set of focus groups. Alternatively, individual interviews can be a useful follow-up to focus groups, giving more opportunities to hear from participants whose thoughts and experiences are worth pursuing further. Finally, either individual interviews or focus groups can be used as ‘member checks’, where one method is used to get feedback after the researcher has done preliminary analyses on the data from another method.\n\nFrom Morgan and Hoffman (2018: 252) on combining focus groups with quantitative research methods:\n\nOne common role for focus groups in mixed methods is to provide preliminary inputs to the development of either survey instruments or program interventions. In this case, the success of the quantitative portion of the project depends on having materials that work well for the participants, and focus groups can provide participants’ voices during this development phase. Focus groups can also be equally use ful for following-up on surveys and experimental studies. In this case, the typical goal is to extend what was learned with the quantitative data by gaining a better sense of how and why those quantitative results came about.\n\nFrom Morgan and Hoffman (2018: 252) on comparing data deriving from focus groups and individual interviews:\n\nOne trap to avoid in this regard is the assumption that individual interviews repre sent a kind of ‘gold standard’ where the focus group introduces an element of bias due to the influences of the group on the individual (see Morrison 1998 for an example of this argument). This assumes that each person has one ‘true’ set of attitudes that will be revealed only in the presence of a researcher in a one-to-one interview, rather than in a group setting with peers. Instead of arguing about whether one of these data collection formats is better than the other, it is more useful to treat them as different contexts –– which may well produce different kinds of data.\n\nFrom Morgan and Hoffman (2018: 255) on group size:\n\nSize is a crucial consideration in decisions about group composition. Typically, focus groups range in size from 5 to 10 people. Smaller sizes are particularly appropriate for sensitive topics and/or situations where the participants have a high level of engage ment with the topic.\n\nFrom Morgan and Hoffman (2018: 255) on dyadic interviews:\n\nDyadic interviews are similar to focus groups in that they seek to accomplish the ‘sharing and comparing’ dimension in inter action, but they limit the dynamic to a conversation between two people, rather than the complexity that can arise when multiple participants engage in a lively discussion. Similar to standard focus groups, the mod erator is there primarily to help the respondents establish rapport and produce rich data. Dyadic interviews are especially well suited to interviewing spouses, and, are thus frequently used in the family studies literature.\n\nFrom Morgan and Hoffman (2018: 255-256) on open-ended or restricted styles:\n\nOne alternative is to use less-structured interviews which create a ‘bubbling up’ of a wide range of potentially unanticipated responses. This approach necessarily uses fewer questions, with each lasting around 15 to 20 minutes. These interviews work best when the goal is to hear the participants’ wide-ranging thoughts about the topic, with less emphasis on the specific types of questions that the researcher feels are important about a particular topic. A disadvantage is that the participants may take the interview in a direction that is not necessarily productive for the overall project. Alternatively, more structured interviews –– with more targeted questions –– reduce this problem, but at the cost of restricting the participants’ frame of reference.\n\nFrom Morgan and Hoffman (2018: 256) on the “funnel” approach:\n\na third option is a ‘funnel’ approach to interviewing, that includes both open-ended and highly targeted questions. Funnels work systematically from less-structured, open-ended questions to more structured, targeted questions. After introductions, the moderator begins the focus group with a broad question that is intended to engage the participants, by asking about the topic from their point of view. Subsequent questions successively narrow in on the research questions the researcher has in mind. For example, a funnel-oriented series of questions may begin with a question like ‘What do you think are the most pressing issues around gun safety in the United States?’ This could be followed by, ‘Of those you have mentioned, which do you think should receive the highest priority from our policymakers?’ and then more targeted questions such as ‘If you were going to contact your legislators about your concerns, what kinds of things would you say?’\n\nFrom Morgan and Hoffman (2018: 256) on the “inverted funnel” approach:\n\nAnother alternative is the ‘inverted funnel approach’, where the questions begin with narrower topics and then broaden to the more open-ended. This approach can be helpful in cases where the participants themselves may not have an immediately available set of thoughts about the topic. This approach often begins by asking about examples of concrete experiences, and then moves to more abstract issues. For example, if you were interested in how the culture of a particular neighborhood was affected by gentrification over a specific period of time, it might be helpful to begin with a very targeted question of the appropriate\n\nFrom Morgan and Hoffman (2018: 256) on establishing rapport among participants:\n\n… there are two aspects of rapport in focus groups: between the moderator and the participants, and among the participants themselves. To foster both kinds of rapport, the initial introduction and first question(s) can help set a tone that is conducive to the goal of getting the participants to share and compare their thoughts and experiences. In focus groups, this often means a trade-off between asking questions that will get the participants talking with each other, versus concentrating on the things that are most directly important to the research. The key point is that the interaction among the participants is the source of the data, so the interview questions need to begin by generating discussions, which may mean delaying the most relevant questions until a good rapport has been established.\n\nFrom Morgan and Hoffman (2018: 257) on the division of labour in focus groups, from a logistical perspective:\n\nA common approach would be for the main moderator to manage the questions/discussion while an assistant observes, takes notes, and is available to help the moderator with any unanticipated needs. An especially important role for the assistant moderator is to ensure the recording equipment is functioning properly throughout the entire interview. This approach can also be advantageous to the research process if the two moderators debrief together afterward to co-create field notes as both moderators may have important –– but different –– observations.\n\nFrom Morgan and Hoffman (2018: 257) on dual moderators:\n\nAdditionally, a ‘dual moderator’ approach can be taken wherein both interviewers facilitate the group discussion. In the latter approach, it is important that the modera tors each have a clear understanding of their respective roles so that the overall experience is enhanced. One common division of labor for this strategy involves one moderator who is more familiar with technical aspects of the topics and another who is more familiar with the group dynamics of facilitation. In addition, working as either an assistant moderator or half of a dual moderating team can be a useful technique for training new moderators.\n\nFrom Morgan and Hoffman (2018: 258) on summary-based reporting:\n\nThe goal in Summary-Based Reporting is to determine which topics were most important to the participants through a descriptive account of the primary topics in the interviews. A simple standard for judging importance is whether a topic arose in nearly every focus group, as well as the extent to which it engaged the participants when it did arise. What matters is not just the frequency with which a topic is mentioned but also the level of interest and significance the participants attached to the topic. This requires a degree of judgment on the part of the analyst, but participants are usually clear about indicating which topics they find particularly important.\n\n\nTo examine these summaries, it is often helpful to create a group-by-question grid where each group is a row and each question is a column. The cells in this grid contain a summary of what a specific group said in response to a particular question. The most effective strategy for using this grid is to make comparisons across what each group said in response to each question. In essence, this moves from column to column, comparing what the different groups said in response to question number one, then question number two, and so on. The goal is to create an overall summary of what the full set of groups said about each question.\n\nFrom Morgan and Hoffman (2018: 259) on content analysis:\n\nVarious forms of qualitative Content Analysis can be applied to focus groups regardless of whether the analysis is driven by counting or by more qualitative approaches. The analytic system can be derived deductively, inductively, or alterna tively through a combination of the two;\n\nFrom Morgan and Hoffman (2018: 259) on thematic analysis:\n\nThe most widely cited version of Thematic Analysis was developed by Braun and Clarke (2006, 2012), who proposed a six-step process: (1) immersion in the data through repeated reading of the transcripts; (2) systematic coding of the data; (3) development of preliminary themes; (4) revision of those themes; (5) selection of a final set of themes; (6) organization of the final written product around those themes. This approach can also be applied in both a more deductive format, where codes are based on pre-existing theory, or a more inductive fashion, where codes are derived from the interviews themselves.\n\nMorgan and Hoffman (2018: 259-261) also address the potential benefits and limitations of online focus groups."
  },
  {
    "objectID": "notes/methodology-notes.html#coding",
    "href": "notes/methodology-notes.html#coding",
    "title": "Methodology notes",
    "section": "Coding",
    "text": "Coding\nThese notes are largely derived from my reading of Saldaña (2016), provides a practical overview of what coding entails and specific methods and techniques.\nCoding as component of knowledge construction:\n\nCoding is an intermediate step, “the”critical link” between data collection and their explanation or meaning” (from Charmaz 2001; as quoted in Saldaña 2016: 4).\n“coding is usually a mixture of data [summation] and data complication … breaking the data apart in analytically relevant ways in order to ead toward further questions about the data” (from Coffey and Atkinson 1996: 29-31; as quoted and edited in Saldaña 2016: 9).\n\nThis relates to the paired notions of decodng when we reflect on a passage to decipher its core meaning, and encoding when we determine its appropriate code and label it (Saldaña 2016: 5).\n\nCoding “generates the bones of your analysis. … [I]ntegration will assemble those bones into a working skeleton” (from Charmaz 2014: 113; as quoted in Saldaña 2016: 9).\nTo codify is to arrange things in a systematic order, to make something part of a system or classification, to categorize\n\nWhat I sometimes refer to as arranging the code tree\nWhat Saldaña (2016) refers to as categories, I tend to refer to as stubs\n\nCategories are arranged into themes or concepts, which in turn lead to assertions or theories\n\n\nPre-coding techniques\nSaldaña (2016) identifies several techniques for formatting the data to make them easier to code, but also to imbue meaning in the text.\n\nData layout\n\nSeparation between lines or paragraphs may hold significant meaning\nPutting interviewer words in square brackets or capital letters\n\nSemantic markup\n\nBold, italics, underline, highlight\nMeant to identify “codable moments” worthy of attention (Boyatzis 1998; as referenced in Saldaña 2016: 20)\nRelates to Saldaña’s (2016: 22) prompt: “what strikes you?”\n\nPreliminary jottings\n\nTri-column exercise with the text on the left, first impression or preliminary code in the middle, and code on the right, after Liamputtong and Ezzy (2005): 270-273.\n\nAsking questions back to the interviewer, or participating in an imagined dialogue\n\nI imagine this might be useful in situations where the time to hold an interview is quite limited and I have to work with limited responses that don’t touch on everything I want to cover\nThe form of questions maintains my tentativity, my unwillingness to commit or assume their responses, and opens the door for their own responses in rebuttal\n\n\nFollowing Emerson et al. (2011: 177), Saldaña (2016: 22) identifies a few key questions to keep in mind while coding:\n\nWhat are people doing? What are they trying to accomplish?\nHow, exactly, do they do this? What specific means and/or strategies do they use?\nHow do members talk about, characterize, and understand what is going on?\nWhat assumptions are they making?\nWhat do I see going on here?\nWhat did I learn from these notes?\nWhy did I include them?\nHow is what is going on here similar to, or different from, other incidents or events recorded elsewhere in the fieldnotes?\nWhat is the broader import or significance of this incident or event? What is it a case of?"
  },
  {
    "objectID": "notes/methodology-notes.html#memos",
    "href": "notes/methodology-notes.html#memos",
    "title": "Methodology notes",
    "section": "Memos",
    "text": "Memos\nCharmaz (2014: 162) dedicates Chapter 7 to memo-writing, which she frames as “the pivotal intermediate step between data collection and writing drafts of papers.” She locates the power of memo-writing as the prompt to analyze the data and codes early in the research process, which requires the researcher to pause and reflect.\nCharmaz (2014: 162):\n\nMemos catch your thoughts, capture the comparisons and connections you make, and crystallize questions and directions for you to pursue.\n\nCharmaz (2014: 162):\n\nMemo-writing creates an interactive space for conversing with yourself about your data, codes, ideas, and hunches. Questions arise. New ideas occur to you during the act of writing. Your standpoints and assumptions can become visible. You will make discoveries about your data, emerging categories, the developing frame of your analysis — and perhaps about yourself.\n\nCharmaz (2014: 164):\n\nMemo-writing encourages you to stop, focus, take your codes and data apart, compare them, and define links between them. Stop and catch meanings and actions. Get them down on paper and into your computer files.\n\nCharmaz (2014: 164):\n\nMemos are your path to theory constriction. They chronicle what you grappled with and learned along the way.\n\nCharmaz (2014: 165-168) distinguishes between memo-writing and journaling. The former is meant to be more analytical, whereas the latter is more of an account of a direct experience, including significant memories or recollections of moments that stood out (and reflection on why they stood out).\nCharmaz (2014: 171) indicates that “[n]o single mechanical procedure defines a useful memo. Do what is possible with the material you have.” She then lists a few possible approaches to memo-writing:\n\nDefine each code or category by its analytic properties\nSpell out and detail processes subsumed by the codes or categories\nMake comparisons between data and data, data and codes, codes and codes, codes and categories, categories and categories\nBring raw data into the memo\nProvide sufficient empirical evidence to support your definitions of the category and analytic claims about it\nOffer conjectures to check in the field setting(s)\nSort and order codes and categories\nIdentify gaps in the analysis\nInterrogate a code or category by asking questions of it.\n\nCharmaz (2014: 171) draws special attention on bringing data into the memo as a way to more concretely “ground” the abstract analysis in the data and lay the foundation for making claims about them:\n\nIncluding verbatim material from different sources permits you to make precise comparisons right in the memo. These comparisons enable you to define patterns in the empirical world. Thus, memo-writing moves your work beyond individual cases.\n\nThrough a detailed example over the prior several pages, Charmaz (2014: 178) reflects on how memos may “[hint] at how sensitizing concepts, long left silent, may murmur during coding and analysus”. She recalls how writing a memo encouraged her to look back at ideas presented in pivotal texts that she had read earlier in her career, and thereby committed her to a new strand of thought.\nCharmaz (2014: 180) describes how she developed memos from in vivo codes that recurred throughout across the cases. She asked how the saying was applied in different contexts, its overlapping and varied meaning.\nCharmaz (2014: 183-?) encourages adoption of various writing strategies. She notes that “memo-writing requires us to tolerate ambiguity”, which is inherent in the “discovery phase” of research, in which she considers memo-writing to be a part. She encourages adotion of clustering and freewriting techniques to help get the ball rolling (she refers to these as “pre-writing techniques”).\nSaldaña (2016) dedicates Chapter 2 to “writing analytic memos”. Saldaña (2016: 44) notes that codes are nothing more than labels until they are analyzed, and remarks that memo-writing is a stage in the process of getting beyond the data. He refers to Stake (1995: 19), who mused that “Good research is not about good methods as much as it is about good thinking”, and in keeping with Charmaz’s (2014) account of memo-writing, memos are tools for doing good thinking.\nSaldaña (2016: 44-45) channels Charmaz in saying that all memos are analytic memos.8\nI identify with his discomfort in writing according to a pre-defined category of memos, and his preference to categorizing them after writing the memo. He also suggests writing a title or brief description to help with sorting.\nHowever, like Charmaz, Saldaña does differentiate between memos and “field notes”, which are synonymous with Charmaz’s journal entries. According to Saldaña (2016: 45), Field notes are the researcher’s personal and subjective responses to social actions encountered during data collection.\nSaldaña (2016: 53-54) reports on O’Connor’s (2007: 8) conceptualization of contemplation of qualitative data as refraction.\n\nThis perspective acknowledes the mirrored reality and the researcher’s lens as dimpled and broken, obscured in places, operating as a concave or at other times a convex lens. As such, it throws unexpected and distorted images back. It does not imitate what looks into the mirror but deliberately highlights some things and obscures others. It is deliciously … unpredictable in terms of what might be revealed and what might remain hidden.\n\nOther analogies include that by Stern (2007: 119): If data are the building blocks of the developing theory, memos are the mortar”, and by Birks and Mills (2015: 40) who consider memos as the “lubricant” of the analytic machine, and “a series of snapshots that chronicle your study experience”.\nSee Montgomery and Bailey (2007) and McGrath (2021) for more on the distinction between memos and field notes, including detailed examples of these kinds of writing in action."
  },
  {
    "objectID": "notes/methodology-notes.html#preliminary-analyses",
    "href": "notes/methodology-notes.html#preliminary-analyses",
    "title": "Methodology notes",
    "section": "Preliminary analyses",
    "text": "Preliminary analyses\nSaldaña (2016) dedicates Chapter 6 to “post-coding and pre-writing transitions”. He frames these as a series of strategies to help crystallize the analytical work and springboard into written documents and reports.\n\nTop-10 lists\nSaldaña (2016: 274-275) suggests coming up with a list of the ten top quotes or passages as a potentially useful “focusing strategy”. Identify the passages (no longer than half a page each) that strike me as the most vivid and/or representational of my study. He suggests reflecting on the content of these items and arranging them in various orders to discover different ways of structuring or outlining the write-up of the research story. He provides some examples of orders to consider:\n\nchronologically\nhierarchically\ntelescopically\nepisodically\nnarratively\nfrom the expository to the dramatic\nfrom the mundane to the insightful\nfrom the smallest detail to the bigger picture\n\n\n\nThe study’s “trinity”\nSaldaña (2016: 275-276) suggests identifying the three (and only three) major codes, categories, themes and/or concepts generated thus far that strike me or stand out in my study. He suggests identifying which one is dominant, how does this status relate to or impact the other codes or concepts, and generally trace the relationships between these ideas. He suggests plotting them as a venn diagram to identify what aspects overlap across items, and to label those areas of overlap — although he does not mention this explicitly, I imagine these points of overlap represent the synthesis of new emergent ideas.\n\n\nCodeweaving\nSaldaña (2016: 276) addresses codeweaving as a viable strategy, but I don’t actually think of it as that useful for my purposes. Seems a but contrived and has lots of potential to be over-engineered.\n\n\nFrom coding to theorizing\nFor Saldaña (2016: 278), the stage at which he finds theories emerging in his mind is when he starts coming up with categories of categories. At this point, a level of abstraction occurs that transcends the particulars of a study, enabling generalizable transfer to other comparable contexts.\nSaldaña (2016: 278-279) identifies a few structures through which these categories of categories might emerge:\n\nSuperordindate and Subordinate Arrangements: Arrange categories as an outline, which suggests discrete linearity and classification. Supercategories and subcategories are “ranked” with numbers or capital letters.\nTaxonomy: Categories and their subcategories are grouped but without any inferred hierarchy; each category seems to have equal weight.\nHierarchy: Categories are ordered from most to least in some manner, i.e. frequency, importance, impact, etc.\nOverlap: Some categories share particular features with others while retaining their unique properties.\nSequential Order: The action suggested by categories progresses in a linear manner.\nConcurrency: Two or more categories operate simultaneously to influence and affect a third.\nDomino Effects: Categories cascade forward in multiple pathways.\nNetworks: Categories interact and interplay in complex pathways to suggest interrelationship.\n\nThe arrows connecting categories are meaningful in their own right. Saldaña (2016: 280) references Urquhart (2013) who states that category relationships are necessary to develop assertions, propositions, hypotheses and theories. He suggests inserting words or phrases between categories that plausibly establishes their connections, as suggested by the data and analytical memos. Saldaña (2016: 280-281) lists several possible connectors:\n\naccelerates\ncontributes toward\ndepends on the types of\ndrives\nfacilitates\nharnesses\nincreases\nincreases the difficulty of\nis affected by\nis essential for\nis necessary for\nprovides\nreconciles\nreduces\nresults in\nresults in achieving\ntriggers\nvaries according to\nwill help to\n\nMoreover, Saldaña (2016: 281) suggests that if you end up with categories as nouns or noun phrases, it could be helpful to transform them into gerund phrases. This will help get a better sense of process and action between catagories.\n\n\nFindings “at a glance”\nFollowing Henwood and Pidgeon (2003), Saldaña (2016: 283) suggests creating a tri-column chart that outlines the findings and the sources of evidence and reasoning that underlie them. See the specific page for a good example."
  },
  {
    "objectID": "notes/methodology-notes.html#the-constant-comparative-method",
    "href": "notes/methodology-notes.html#the-constant-comparative-method",
    "title": "Methodology notes",
    "section": "The constant comparative method",
    "text": "The constant comparative method\nThe constant comparative method is based on action codes, similar to what Saldaña (2016) refers to as process codes. According to Charmaz (2000: 515):\n\nThe constant comparative method of grounded theory means (a) comparing different people (such as their views, situations, actions, accounts, and experiences), (b) comparing data from the same individuals with themselves at different points in time, (c) comparing incident with incident, (d) comparing data with categori, and (e) comparing categories with other categories.\n\nMy initial impression is that this is very well suited for Stake’s (2006) multicase study framework, specifically with regards to his notion of the case-quintain dilemma. It also seems very well suited for analysis of situational meaning-making, as per Suchman (1987), Lave and Wenger (1991), Knorr Cetina (2001) and symbolic interactionism at large."
  },
  {
    "objectID": "notes/methodology-notes.html#situational-analysis",
    "href": "notes/methodology-notes.html#situational-analysis",
    "title": "Methodology notes",
    "section": "Situational analysis",
    "text": "Situational analysis\nSituational analysis originates from Strauss’s social worlds/arenas/negotiations framework. From Clarke (2003: 554):\n\nBuilding on and extending Strauss’s work, situational analyses offer three main cartographic approaches:\n\nsituational maps that lay out the major human, nonhuman, discursive, and other elements in the research situation of concern and provoke analyses of relations among them;\nsocial worlds/arenas maps that lay out the collective actors, key nonhuman elements, and the arena(s) of commitment within which they are engaged in ongoing negotiations, or mesolevel interpretations of the situation; and\npositional maps that lay out the major positions taken, and not taken, in the data vis-à-vis particular discursive axes of variation and difference, con cern, and controversy surrounding complicated issues in the situation.\n\n\nClarke (2003: 560) identifies the main purpose of situational data as a way of “opening up” the data, figuing out where and how to enter:\n\nAlthough they may do so, a major and perhaps the major use for them is “opening up” the data –— interrogating them in fresh ways. As researchers, we constantly confront the problem of “where and how to enter.” Doing situational analyses offers new paths into the full array of data sources and lays out in various ways what you have to date. These approaches should be considered analytic exercises — constituting an ongoing research “work out” of sorts—well into the research trajectory. Their most important outcome is provoking the researcher to analyze more deeply.\n\nShe emphasizes that this is meant to stimulate thinking, and should always be paired with comprehensive memoing before, during and after situation mapping excercises.\nA key feature that I think is invaluable is the ability to uncover the sites of silence, or the things that I as a researcher suspect are there but are not readily visible in my evidence. Situational analysis is useful for drawing out the thousand pound gorillas in the room that no one wants to talk about, and is therefore important for identifying things to address during continual data collection, as is one of the (often ignored) central pillars of grounded theory:\n\nThe fourth and last caveat is perhaps the most radical. As trained scholars in our varied fields, usually with some theoretical background, we may also suspect that certain things may be going on that have not yet explicitly appeared in our data. As ethically accountable researchers, I believe we need to attempt to articulate what we see as the sites of silence in our data. What seems present but unarticulated? What thousand-pound gorillas are sitting around in our situations of concern that nobody has bothered to mention as yet (Zerubavel 2002)? Why not? How might we pursue these sites of silence and ask about the gorillas without putting words in the mouths of our participants? These are very important directions for theoretical sampling. That is, the usefulness of the approaches elucidated here consists partly in helping the researcher think systematically through the design of research, espe cially decisions regarding future data to collect.\n\nThe process is remarkably similar to the brainstorming excercise I did with Costis one time. Starts by articulating the actors involved, their roles and relationships, the things they do, the things they make.\n\nThe goal here is to lay out as best one can all the human and nonhuman elements in the situation of concern of the research broadly conceived. In the Meadian sense, the questions are: Who and what are in this situation? Who and what matters in this situation? What elements “make a difference” in this situation?\n\nAfter jotting these down on a canvas or whiteboard, arrange them into more concrete categories. In each category, refer to examples or specific instances. Concepts can occur in multiple categories. By arranging these concepts, grouping them thematically, spatially, and through relationships and associations with arrows or lines (presumably, with labels indicating the nature of these relationships), this provides the researcher with a viable way of unlocking new pathways to think through their data. I imagine this will be especially helpful when arranging or re-arranging the coding tree.\nClarke (2003: 569-570) also dedicates a few paragraphs to relational forms of analysis using this technique:\n\nRelations among the various elements are key. You might not think to ask about certain relations, but if you do what I think of as quick and dirty relational analyses with the situational map, they can be revealing. The procedure here is to take each element in turn and think about it in relation to each other element on the map. One does this by circling one element and mentally or literally drawing lines, one at a time, between it and every other ele ment on the map and specifying the nature of the relationship by describing the nature of that line.\n\n\nYou could highlight (in blue perhaps) that organization’s perspectives on all the other actors to see which actors are attended to and which are not, as well as the actual contents of the organization’s discourses on its “others.” Silences can thus be made to speak.\n\nClarke (2003: 569) On how to generate memos from situational analysis:\n\nEach map in turn should lead to a memo about the relations diagrammed. At early stages of analysis, such memos should be partial and tentative, full of questions to be answered about the nature and range of particular sets of social relations, rather than being answers in and of themselves.\n\nClarke (2003: 570) addresses the energy or mood required to do this kind of work, which is reminiscient of Yin’s (2014: 73-74) description of the “mental and emotional exhaustion at the end of each fieldwork day, due to the depletion of ‘analytic energy’ associated with being attention on your toes.” Interesting that the mention of “freshness” is brought up in the context of Glaser’s tabla rasa approach to grounded theory.\n\nAs a practical matter, doing the situational map and then the relational analyses it organizes can be tiring and/or anxiety producing and/or elating. Work on it until you feel stale and then take a break. This is not the same order of work as entering bibliographic citations. The fresher you are, the more you can usually see. Glaser (1978: 18-35) cautions against prematurely discussing emergent ideas –— that we might not necessarily benefit from talking about everything right away but rather from reflection — and memoing. I strongly agree, especially about early even if quick and dirty memoing. But we all must find our own ways of working best. For most, the work of this map occurs over time and through multiple efforts and multiple memos.\n\nClarke (2003) refers to Shim (2000) as an exemplary case of situational analysis in action."
  },
  {
    "objectID": "notes/methodology-notes.html#qda-software-and-tooling",
    "href": "notes/methodology-notes.html#qda-software-and-tooling",
    "title": "Methodology notes",
    "section": "QDA software and tooling",
    "text": "QDA software and tooling\nWeitzman (2000) provides an overview of software and qualitative research, including a minihistory up to the year 2000 when the chapter was published.\nDescribing the first programs specifically designed for analysis of qualitative data, Weitzman (2000: 804) writes:\n\nEarly programs like QUALOG and the first versions of NUDIST reflected the state of computing at that time. Researchers typically accomplished the coding of texts (tagging chunks of texts with labels — codes — that indicate the conceptual categories the researcher wants to sort them into) by typing in line numbers and code names at a command prompt, and there was little or no facility for memoing or other annotation or markup of text.9 In comparison with marking up text with coloured pencils, this felt awkward to many researchers. And computer support for the analysis of video or audio data was at best a fantasy.\n\nThis history if followed by a sober account of what software can and can not do in qualitative research, as well as affirmation and dismissed of hopes and fears. Very reminiscient of Huggett (2018)."
  },
  {
    "objectID": "notes/methodology-notes.html#writing-techniques",
    "href": "notes/methodology-notes.html#writing-techniques",
    "title": "Methodology notes",
    "section": "Writing techniques",
    "text": "Writing techniques\nThese are some writing strategies to get ideas flowing. Both Saldaña (2016) and Charmaz (2014) actually refer to these as “pre-writing” techniques that occur as a researcher is trying to extend their observations and their notes into broader forms. However, I tend to consider writing as a more open-ended aspect of research, whose borders are less defined, so I am referring to these as writing techniques in a general sense.\n\nAnalytical storylining\nFollowing Charmaz (2010: 201), Saldaña (2016: 287) suggests using processual words that suggest the unfolding of events through time that “reproduce the tempo and mood of the [researched] experience”. However, he cautions that this is not suitable for every situation, and to be aware of how this framing differs from positivist notions of cause and effect.\n\n\nOne thing at a time\nSaldaña (2016: 287) suggests documenting each category, theme or concept one at a time as a wat of disentangling very interrelated ideas. This helps maintain focused as a writer, and may help maintain a more focused reading experience too.\n\n\nWriting about methods\nSaldaña (2016: 284-286) addresses some common tendencies and challenges with regards to writing about qualitative coding practices:\n\nResearchers provide brief storied accounts of what happened “backstage” at computer terminals and during intensive work sessions. After a description of the participants and the particular data collected for a study, descriptions of coding and analysis procedures generally include: references to the literature that guided the analytic work; qualitative data organization and management strategies for the project; the particular coding and data analytic methods employed and their general outcomes; and the types of CAQDAS programs and functions used. Some authors may include accompanying tables or figures that illustrate the codes or major categories constructed. Collaborative projects usually explain team coding processes and procedures for reaching intercoder agreement or con-sensus. Some authors may also provide brief confessional anecdotes that highlight any analytic dilemmas they may have encountered.\n\nSaldaña (2016: 286) then suggests emphasizing for readers through introductory phrases and italicized text the major outcomes of the analysis to help provide guidance for readers to grasp the headlines of your research story. For example:\n\n“The three major categories constructed from transcript analysis are …”\n“The core category that led to the groundedtheory is ..,” “The key assertion of this study is …”\n“A theory I propose is …”\n\n\n\nClustering\nFrom Charmaz (2014: 184-187) Essentially comprises visually mapping the core and peripheral ideas, similarly to Saldaña’s “big three” technique and Clarke’s situational analysis. Charmaz (2014) encourages experimentation, free from commitment to a specific arrangement.\nCharmaz (2014: 185) provides a series of directions that may help effectively implement this technique:\n\nStart with the main topic or idea at the center\nWork quickly\nMove out from the nucleus into smaller subclusters\nKeep all related material in the same subcluster\nMake the connections clear between each idea, code, and/or category\nTry several different clusters on the same topic\nUse clustering to play with your material\n\nClustering enables you to define essentials. It allows for chaos and prompts you to create paths through it.\n\n\nFreewriting\nFrom Charmaz (2014: 186-188). Simply putting fingers to keeboard and writing for 8-10 minutes straight. Write to and for yourself. Permit yourself to write badly. Don’t attend to grammar, organization, logic, evidence or audience. Write as though you are talking."
  },
  {
    "objectID": "notes/methodology-notes.html#footnotes",
    "href": "notes/methodology-notes.html#footnotes",
    "title": "Methodology notes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe term refers to a medieval jousting target: see https://en.wikipedia.org/wiki/Quintain_(jousting)↩︎\nThough Yin (2014: 40-444) is dismissive of such use of the term “sample” since he sees case study research as only generalizable to similar situations, and not to a general population from which a sample is typically said to be drawn. I agree with this focus on concrete situations over Stake’s prioritization of theory-building as an end unto itself.↩︎\nCharmaz uses the term “giving voice” in this specific context. I’m not sure if this is meant to represent Strauss and Corbin’s attitude, and whether this is an accurate representation on their views, but in my mind this should be framed as elevating, amplifying or re-articulating respondents’ voices (and this is a tenet of constructivist grounded theory in general, which derives from Charmaz). My take diverges from the position that we “give voice” to respondents in that it acknowledges (1) that the voices are already there, (2) that respondents are in fact giving us their voices, and (3) that the researcher plays an active editorial role, transforming the respondents’ elicitations into a format that is more amenable to analysis.↩︎\nVery much in line with the pragmatist turn of the late ’90s and early ’00s, as also documented by Lucas (2019: 54-57) in the context of archaeological theory, vis-a-vis positivism, postmodernism, and settling on a middle ground between them.↩︎\nopen-ended questions, asked of all participants in the same order.↩︎\nThis seems reminiscient of procedures planned for WP1 and WP2 from my FWO grant application.↩︎\nThis is also very reminiscient of Nicolini (2009).↩︎\nContrast this with Richardson (2000), who identified several kinds of notes:\n\nObservation notes: Concrete and detailed, accurate renditions of things I see, hear, feel and taste, and so on. Remain close to the scene as experienced through the senses.\nMethodological notes: Messages to self about how to collect data — who to talk to, what to wear, when to get in touch.\nTheoretical notes: Hunches, hypotheses, poststructuralist connections, critiques of what I’m doing/thinking/seeing, keeping me from getting hooked on one view of reality.\nPersonal notes: Uncensored feeling statements about the research, the people I’m talking to, my doubts, anxieties, pleasures. These can also be great sources of hypotheses (for example, if I am feeling anxious, others I’m speaking with might feel the same way, which provides a string to tug on).\n\n↩︎\nThis caught my eye since its the same approach as that adopted by qc!↩︎"
  },
  {
    "objectID": "notes/situational-analysis.html",
    "href": "notes/situational-analysis.html",
    "title": "Situational analysis",
    "section": "",
    "text": "Some notes on situational analysis and related methods are useful for my work. Largely drawn from Adele E. Clarke, Friese, and Washburn (2016)."
  },
  {
    "objectID": "notes/situational-analysis.html#maps",
    "href": "notes/situational-analysis.html#maps",
    "title": "Situational analysis",
    "section": "Maps",
    "text": "Maps\nThe main strategies involved in SA are the three maps that researchers do across the full trajectory of the project, from the earliest design stages to the preparation of publications.\nAdele E. Clarke (2005: 30) explains why mapping is a useful method:\n\nWhy situational maps? Why not narratives? There are a number of reasons. Let me start with some advantageous properties of maps elucidated by David Turnbull (2000) and elaborated by me. Because maps are visual representations, they helpfully rupture (some/most of) our normal ways of working and may provoke us to see things afresh (e.g., Latour 1986, 1988b; Suchman 1987). Maps also work more easily as discursive devices for mak ing assemblages and connections — relational analyses. Maps are excellent “devices to materialize questions.” Maps are tools of control, appropriation, and ideological expression. Mapping is a fundamental cognitive process we can “just do it.” Mapping opens up knowledge spaces. Maps are great boundary objects — devices for handling multiplicity, heterogeneity, and messiness in ways that can travel. Maps work well as spatial and temporal narratives. Maps allow unmapping and remapping. In addition, maps are very much part of the Chicago tradition as devices for analyzing relationality (see Chapter 2). Most important here, one can move around on/in maps much more quickly and easily than in narrative text, excellent for analytic work. Last, at least since the postcolonial era began, maps have been widely understood as very political — and shifting — devices. Hopefully, this will provoke enhanced reflexivity. The limitations of maps remain, of course, what can be “seen” by a particular analyst in a particular time and place. No method overcomes the situatedness of its users. A method can, however, attempt to use situatedness to improve the quality of the research.\n\n\nSituational maps\n\nFrom Adele E. Clarke, Friese, and Washburn (2016: 13-14)\nLay out all the major human, non-human, discursive, historical, symbolic, cultural, political, and other elements in the research situation of concern.\nThis map should ideally be made earlier on in the design phase.\nLays out everything about which at least some data should be gathered.\nIt meant to gain a sense of possibly important relations about all the elements.\nAlso helps guide data collection and develop stronger funding proposals.1\nDownstream in the research, situational maps are used to provoke analysis of relations among different elements through relational mapping.\nThese maps capture and provoke discussion of the many and heterogeneous elements, their relationships to one another, and the messy complexities of the situation (Adele E. Clarke 2005: 83-103; D. L. Clarke 2014; see also: Lather 2007; Law 1999, 2004, 2007; Law and Mol 2006; Taylor 2005).\nClarke distinguishes between two kinds of situational maps:\n\nMessy version\nOrdered version\n\n\nFrom Adele E. Clarke (2005: xxxv):\n\nThe first maps are the situational maps that lay out the major human, nonhuman, discursive, historical, symbolic, cultural, political, and other elements in the research situation of concern and provoke analy sis of relations among them. These maps are intended to capture and discuss the messy complexities of the situation in their dense relations and permuta tions. They intentionally work against the usual simplifications so characteristic of scientific work (Star 1983, 1986) in particularly postmodern ways.\n\nFrom Fosket (2021): [: 272-273]:\n\nMapping the Situation: Who/What Matters to STAR Upon entering the field of STAR, one of the first things I realized was that this “site” itself consisted of multiple sites. It comprised many different elements complexly organized and webbed together to form what I ultimately conceived of as the “STAR trial arena.” Nonhuman actants (things of various kinds from furniture to technologies to discourses), social actors, body parts, research protocols, organi zations, and paperwork represent key elements in the constitution of the trial, and critical activists and passionate advocates are central. Additionally, the deeper I delved into the research, the more obvious and important the historical and political situatedness of STAR became. By requiring the researcher to map out all of the “analytically pertinent human and nonhuman, material and symbolic/dis cursive elements of a particular situation as framed by those in it and by the analyst” (Adele E. Clarke 2005: 87), situational maps draw out complexities and reveal which anticipated and unanticipated elements of the situation matter.\n\nAdele E. Clarke, Friese, and Washburn (2016: 101):\n\nBecause they are intended to be done and redone multiple times across the life of a research project, there is no one “right” map. If you put something on it that turns out not to be important, you can delete it later or just ignore it. But if it was there in the first place, or got there during the research, at least you integrated it into the research design and sought some data about it systematically and have some sense of its relative importance.\n\n\nRelational mapping\nFramed as an extension of (messy) situational maps. From Adele E. Clarke, Friese, and Washburn (2016: 107):\n\nOnce you have your messy map, you can do relational analyses. This is the next phase of analytic work to be done with the messy map. … Then you take each element in turn and think about it in relation to each other element on the map. Literally center on one element and draw lines between it and the others and specify the nature of the relation ship by describing the nature of that line. One does this systematically, one at a time, from every element on the map to every other. Use as many maps as seem useful to diagram yourself through this analytic exercise. This to me is the major work one does with the situational map once it is constructed. This is one of those sites where being highly systematic in considering data can flip over into the exciting and creative moments of intellectual work. And sometimes there is no payoff.\n\nWith regards to sorting out and prioritizing the work at early stages, from Adele E. Clarke, Friese, and Washburn (2016: 108):\n\nRelational maps also help the analyst to decide which stories—which relations—to pursue. This is especially helpful in the early stages of research when we tend to feel a bit mystified about where to go and what to memo. A session should produce several relational analyses with the situational maps and several memos. One would return to elaborate on these memos several times as data are collected. They should also be useful guides for theoretical sampling.\n\n\n\n\nSocial worlds/aernas maps\n\nFrom Adele E. Clarke, Friese, and Washburn (2016: 14)\nLay out all the collective actors and the arena(s) of commitment within thich they are engaged in ongoing dicourses and negotiations.\nThese maps offer interpretations of the broader situation, taking up its social organizational, institutional and discursive dimensions (A. Strauss 1978).\nThey invoke distinctively poststructuralist assumptions:\n\nwe cannot assume directionalities of influence;\nboundaries are open and porous;\nnegotiations are fluid;\ndiscourses are multiple and potentially contradictory.\n\nNegotiations of many kinds, from coercion to bargaining are the “basic social processes” that construct and constantly destabilize social worlds’ relations and arenas maps.\nSymbolic interactionism tells is that things can always be otherwise — not only individually, but also collectively, organizationally, institutionally, and discursively.\nThese maps portray these poststructural possibilities.\nThe flipside of social worlds/arenas maps are discourse/arenas maps.\nSocial worlds are “universes of discourse”. routinely producing discourses about themselves, about other social worlds, and about issues of concern in the arena.\nThese discourses can be positionally mapped and analyzed.\n\nFrom Adele E. Clarke (2005: xxxv-xxxvi):\n\nSecond, the social worlds/arenas maps lay out all of the collective actors, key nonhuman elements, and the arena(s) of commitment within which they are engaged in ongoing discourse and negotiations. Such maps offer meso-level interpretations of the situation, explicitly taking up its social organizational, institutional, and discursive dimensions. They are distinctively postmodern irrtheir assumptions: We cannot assume directionalities of influence; bound aries are open and porous; negotiations are fluid; discourses are multiple and potentially contradictory. Negotiations of many kinds from coercion to bar gaining are the “basic social processes” that construct and constantly desta bilize the social worlds/arenas maps (A. L. Strauss and Maines 1993). Things could always be otherwise-not only individually but also collectively/organizationally/ institutionally/discursively-and these maps portray such postmodern possibilities.\n\n\n\nPositional maps\n\nFrom Adele E. Clarke, Friese, and Washburn (2016: 14-15)\nLay out the major positions taken and not taken in the data vis-a-vis particular axes of variation and difference, focus, and controversy found in the situation of concern.\nThe discursive data can include interviews, observations, website, documents, etc.\nMost significantly, discursive maps are not articulated with persons or groups but instead seek tp represent the full range of discursive positions on key issues in the broad situation of concern.\nThey allow multiple positions and contradictions to be articulated.\nDiscourses are therefore disarticulated from their sites of production, decentering them and making analytic complexities more visible.\n\nFrom Adele E. Clarke (2005: xxxvi):\n\nThird, positional maps lay out the major positions taken, and not taken, in the data vis-a-vis particular axes of variation and difference, focus, and controversy found in the situation of concern. Perhaps most significantly, positional maps are not articulated with persons or groups but rather seek to represent the full range of discursive positions on particular issues — fully allowing multiple positions and even contradictions within both individuals and collectivities to be articulated. Complexities are themselves heteroge neous, and we need improved means of representing them."
  },
  {
    "objectID": "notes/situational-analysis.html#affinities-with-other-frameworks",
    "href": "notes/situational-analysis.html#affinities-with-other-frameworks",
    "title": "Situational analysis",
    "section": "Affinities with other frameworks",
    "text": "Affinities with other frameworks\nSA does not operate in a vacuum, and has some very strong productive relationships with other key theoretical and methodological frameworks emerging or radically evolving around the same time of its “birth”. Here I outline these affinities, articulate distinctions and similarities between them, and perhaps also identify certain antagonistic or opposing perspectives.\n\nGrounded Theory\n\nSA both relies on and radically extends GT by pushing that method around the postmodern / postructuralist / interpretive turn and taking into account other theoretical and methodological developments since GT’s origins in 1967 (Adele E. Clarke, Friese, and Washburn 2016: 77).\nClarke framed SA in relation to GT as “the (re)turn to the social”, or the reconfiguration of relationality taking place across the social sciences and humanities, specifically regarding the location of individual agency in participation as members of collectives engaged in universes of discourses.\n\n\n\nActor-Network Theory\n\nSee Adele E. Clarke (1987) and Adela E. Clarke and Fujimura (1992) for examples of SA work that draws from ANT (which was initially developed by Latour and Woolgar (1986)).\n\n\n\nRhyzome Theory\n\nSituational maps are, in some way, representations of Deleuze and Guittari’s (2007) concept of rhyzomes, a plant-based metaphor for multiple emergent shoots emerging from a plane with widespread horizontal, entwined networks of underground roots.\nRhyzomes contrast and resist tree metaphors, which are vertical, have localized roots, and an original source like acorns or seeds.\nIn contrast, rhyzomes invoke multiplicities of connections, relationality, linkages across people and non-human objects and beings, nomadic propagation, and growth.\nThey are complex, contingent, and ultimately indeterminate — messy!\n\n\n\nPoststructualist Feminism\n\nKey affinities include Haraway (1991) and Lather (2007).\nClarke argues that GT has always been implicitly feminist (due to its pragmatic roots), and SA is one of a few attempts to draw that out more explicitly.\nSA shares with postructural feminism a criticsl lens through which one might identify biased outlooks in the research process.\nSA’s emphasis on elucidating complexities and diversities of the elements and positions under examination also belie its usefulness as a tool for critical, feminist and anti-racist research (Adele E. Clarke, Friese, and Washburn 2016: 20-21).\nSA’s consideration of non-human entities also lends itself to feminist critique and a social justice orientation in that it encourages identification of things and connections that exist and that could exist but do not among various actors (Adele E. Clarke, Friese, and Washburn 2016: 21).\n\nThis draws on what Foucault (1975) called the “conditions of possibility”.\n\nMoreover, SA is inherently reflexive and strongly encourages researchers to acknowledge their positionality in relation to the things they are examining.\nIt also encourages the researcher to situate themselves as the learner in critical inquiry, which can shift the tenor of the research in valuable ways.\n\n\n\nParticipatory Action Research\n\nGenat (2009) found it valuable to make social worlds/arenas maps of PAR projects themselves. carefully laying out and analyzing the worlds that should be involved, studied and analyzed.\nIn this sense, SA is used as a reflexive method that articulates the interacting system of social worlds of specific stakeholders, which carry different commitments to their collective research initiative.\nEnables researchers and their local research partners to foreground shared local understandings in order to both critique more dominant discourses originating elsewhere and to generate locally based statements of need on which policy positions to improve the local situation can be founded (Adele E. Clarke, Friese, and Washburn 2016: 17-18).\n\n\n\nConstructivism and Interpretivism\n\nInitiated by Blumer (1954); Blumer (1969), with roots in the pragmatic philosophy of George Herbert Mead2\nOne aspect of Blumer’s position was that theory and method are inextricably entwined and nonfungible, which today would be described as co-constitutive of Star’s (1989) concept of “theory/methods packages” (Adele E. Clarke, Friese, and Washburn 2016: 26-27).\nSee also Goffman’s dismissal of scientistic claims of positivism in sociology by asserting that “a sort of sympathetic magic seems to be involved, the assumption being that if you go through the motions attributable to science then science will result. But it hasn’t.” (quoted in Vidich and Lyman 1994 [: 40]).\n\nThis corresponds with similar dismissals of positivism made by Geertz (1973) and others.\n\nThe next 20 years (ending during the early 1990s) constituted that Denzin and Lincoln (1994: 9) called the era of blurred genres.\n\nThe first edition of the Handbook of Qualitative Research (Denzin and Lincoln 1994) signalled loud and clear that the renaissance of qualitative research had arrived, after years of poignant, but scattered critiques of positivist modes of inquiry.\n\n\n\n\nEpistemic Cultures\n\nThe concept of “epistemic cultures” (developed by Knorr Cetina (1999)) asserts that through their actual practices of working, different scientific disciplines, specialties, and approaches generate distinctive cultures based on their epistemic assumptions, theories, and the practices used in doing such research.\nThis concept is part of the “turn to practice” occurring throughout the social sciences and humanities.3.\nEpistemic cultures are one kind of social world.\n\n\n\nSocial Interactionism\nAdele E. Clarke (2021: 228) described a shift in Strauss’ later work (while she was his student), which emphasized the conditional and situational aspects of action:\n\nHe worked assiduously on framing and articulating ways to do grounded theory research that included specifying the structural conditions—seeking to literally make them vis ible in doing grounded theory analysis. Strauss’s provisional solution, largely pur sued with Juliet Corbin, was the conditional matrix, a strategy for supplementing grounded theory’s usual processual emphasis with serious attention to more struc tural contextual “conditions.” Strauss’s pragmatist interactionist sociology was based most of all in understanding action as a situated activity. Thus, for Strauss, the conditional matrix was a situating device — a heuristic means of enabling grounded theory researchers to envision the contexts and conditions under which the action is occurring.\n\nAdele E. Clarke (2021: 228-229) on Strauss’ visual representation of “levels” of situational context:\n\nHere the concentric circles represent the more structural conditions within which the focus of analysis dwells. Structural conditions are portrayed as context, arrayed around the central action focus from local to global (from near the center/core to faraway places on the periphery). Action is in the center of the diagram —– the grounded theory social process. … In later versions of the conditional matrix (e.g., A. Strauss 1988: 184), these conditions spiral around the central focus of analysis, implying that such conditions may be closer in or more peripheral, adding a sense of fluidity and improving the diagram.\n\nShe (2021: 230) goes on with a critique of this approach, which she then builds on:\n\nThere are two problems here. First, the concentric circles seem to predetermine relations between particular concerns and the phenomenon under study. Second, the matrix gestures too abstractly toward the possible salience of structural elements of situations rather than insisting on concrete and detailed empirical specification of structural elements and their clear explication as requisite for thorough analysis. That is, the relations among the important elements need to be empirically specified, studied, and interpreted. In sum, to me, the conditional matrices ultimately did not do the conceptual analytic work Strauss and Corbin wanted to do in grounded theory method. My critiques ultimately provoked me to develop new strategies to better focus empirically and analytically on the broader situation.\n\nClarke’s (2021: 231) describes the benefits of her alternative, the situational matrix:\n\nHere the conditions of the situation are in the situation. There is no such thing as “context,” and no concentric or spiraling circles. The conditional elements of the situation need to be specified in the analysis of the situation itself as they are constitutive of it, not merely surrounding it or framing it or contributing to it. They are it. Regardless of whether some actors might construe them as local or global, internal or external, close-in or far away, or whatever, the fundamental question is: “How do these conditions appear –— make themselves felt as consequential — as integral parts of the empirical situation under examination?” At least some answers to that question can be found through doing situational mapping and analyses.\n\n\n\nSocial Worlds/Arenas\nFrom Adele E. Clarke (2005: xxix) on how Strauss’ social worlds/arenas theory anticipated this emphasis on situations:\n\nSome years ago, Katovich and Reese (1993:400-405) interestingly argued that Strauss’s negotiated order and related work recuperatively pulled the social around the postmodern turn through its methodological (grounded theoretical) recognition of the partial, tenuous, shifting, and unstable nature of the, empirical world and its constructedness. I strongly agree and would argue that Strauss also particularly furthered this “postmodernization of the social” through his conceptualizations of social worlds and arenas as modes of understanding the deeply situated yet always also fluid organizational ele ments of negotiations and discourses. He foreshadowed what later came to be known as postmodern assumptions: the instability of situations; the char acteristic changing, porous boundaries of both social worlds and arenas; social worlds seen as mutually constitutive/coproduced in the negotiations taking place in arenas; negotiations as central social processes hailing that “things can always be otherwise”; and so on. Negotiations also signal micropolitics of power and the powers of discourses-decentering the subject and power in its more fluid and discursive forms (e.g., Foucault 1979, 1980) — as well as “the usual” meso/macro structural elements.\n\nFrom Adele E. Clarke, Friese, and Washburn (2016: 89):\n\nSocial worlds theory assumes multiple collective actors—social worlds—in all kinds of negotiations in a broad and often contentious substantive arena. Arenas are focused on matters about which all the involved social worlds and actors care enough to be (1) committed to act, and (2) to produce discourses about arena concerns. Thus, arenas are sites of action and discourse. They are discursive sites in often complicated ways. Particular social worlds are constructed in other world’s discourses as well as producing their own. But arenas usually endure for some time, and longstanding arenas are typically characterized by multiple, complex, and layered discourses that interpolate and combine old(er) and new(er) elements in on-going, contingent, and inflected practices. Further, because perspectives and commitments differ, arenas are usually sites of contestation and controversy. As such, they are especially good for analyzing heterogeneous perspectives or positions and for analyzing power in action (a lesson from technoscience studies) (e.g., Nelkin, 1995). Arenas are also especially amenable conceptual frames through which to work at a more meso/ organizational level, analyzing collective actors (social worlds), their work, and discourses in those arenas.\n\nAdele E. Clarke (2005: 46) on what constitutes social worlds:\n\nSocial worlds are universes of discourse (Mead 1938/1972:518) and principal affiliative mechanisms through which people organize social life. Insofar as it meaningfully exists, society as a whole, then, can be conceptualized as consisting of layered mosaics of social worlds and arenas. Strauss argued (1978:122) that each social world has at least one primary activity, particular sites, a technology (inherited or innovative means of carrying out the social world’s activities), and, once under way, more formal organizations typically evolve to further one aspect or another of the world’s activities. Hughes (1971) offered the more informal notion of a going con cern in which certain assumptions about what activities are important and what will be done can be taken for granted. People typically participate in a number of social worlds/going concerns simultaneously, and such par ticipation usually remains highly fluid. Entrepreneurs, deeply committed and active individuals (Becker 1963), cluster around the core of the world and mobilize those around them (Hughes 1971:54). Shibutani (1986:109) viewed social worlds as identity- and meaning-making segments in mass society, drawing on distinctive aspects of mass culture, with individuals capable of participation in only a limited number of such worlds. There can also be implicated actors in a social world and/or arena, actors silenced or only discursively present — constructed by others for their own purposes (Clarke & Montini 1993). This concept provides a means of analyzing the situatedness of less powerful actors and the consequences of others’ actions for them and raises issues of discursive constructions of actors and of nonhuman actants. I will therefore discuss it at some length. There are at least two kinds of implicated actors. First are those implicated actors who are physically present but are generally silenced/ignored/ invisibled by those in power in the social world or arena. Second are those implicated actors not physically present in a given social world but solely dis cursively constructed; they are conceived, represented, and perhaps targeted by the work of those others; hence they are discursively present. Neither category of implicated actors is actively involved in the actual negotiations of self-representation in the social world or arena, nor are their thoughts or opinions or identities explored or sought out by other actors through any openly empirical mode of inquiry (such as asking them questions). They are neither invited by those in greater power to participate nor to represent If themselves on their own terms. physically present, their perceptions are largely ignored and/or silenced. The difference between the two types turns on the issue of their physical presence.\n\nAdele E. Clarke (2005: 47) on the presence of non-human actors in social worlds and their role in human experiences and conceptions:\n\nThere can, of course, also be implicated actants-implicated nonhuman actors in situations of concern. Like humans, implicated actants can be physically and/or discursively present in the situation of inquiry. That is, human actors (individually and/or collectively as social worlds) routinely discursively construct nonhuman actants from those human actors’ own perspectives. The analytic question here is: Who is discursively constructing what, and how and why are they doing so?\n\nI think this is the main message, from Adele E. Clarke (2005: 48)\n\nThe concept of implicated actors and actants can be particularly useful in the explicit analysis of power in social worlds and arenas. Such analyses are both complicated and enhanced by the fact that there are generally multiple discursive constructions circulating of both the human and nonhuman actors in any given situation. Analyzing power involves analyzing: Whose con structions of whom/what exist? Which are taken as “the real” constructions or the ones that “matter” in the situation by the various participants? Which are contested? Whose are ignored? By whom? Through understanding the discursive constructions of implicated actors and actants, analysts can grasp a lot about the social worlds and the arena in which they are active and some of the consequences of those actions for the less powerful.\n\nFrom Adele E. Clarke (2005: 55):\n\nStrauss was similarly concerned with the invariably social/organizational ways in which what he frames as identities (rather than subjectivities) are produced and transformed throughout life through relations in social worlds in which people participate. … Also traceable back to Mead, both Blumer and Strauss understood individuals as constituted through such collectivities, and collectivities as constituted through interaction with other collectivities. … For Foucault, both individuals and collectivities are constituted through discourses and disciplining. For Strauss, both individuals and collectivities are produced through their participation in social worlds and arenas, including their discourses. While Foucault’s language of disciplining and the constitution of subjectivity(ies) is more insistent and decenters “the knowing subject” much more thoroughly, these productions are accomplished through routine practices. Later in his career, when issues of agency concerned him more, Foucault (1988: 11) (emphasis added) stated: I would say that if now I am interested, in fact, in the way in which the subject constitutes himself in an active fashion, by the practices of self, these practices are nevertheless not something that the individual invents by himself. They are patterns that he finds in his culture and which are proposed, suggested and imposed on him by his culture, his society and his social group. This, to me, is a key point of articulation with Strauss-and with interac tionism more broadly.\n\n\n\nDiscourse Studies\nAdele E. Clarke, Friese, and Washburn (2016: 90) summary of Foucault’s notion of discourse:\n\nFoucault (1972) began with the concept of “the order of discourse,” asserting that ways of framing and representing linguistic conventions of meanings and habits of usage together constitute specific discursive fields or terrains. Conceptually, discourses are analytic modes of ordering the chaos of the world. His con cept of “discursive practices” described ways of being in the world that could, when historicized, be understood to produce distinctive “discursive forma tions” –— dominant discourses that bind together social injunctions about particular practices (Dreyfus & Rabinow, 1983, p. 59). Dominant discourses are reinforced through extant institutional systems of law, media, medicine, education, and so on —– often operating in conjunction. A discourse is effected in disciplining practices, which produce subjects/subjectivities through surveillance, examination, and various technologies of the self — ways of produc ing ourselves as properly disciplined subjects (e.g., Foucault, 1973, 1975, 1978, 1988).\n\n\n\nNon-Human Actors\nAdele E. Clarke, Friese, and Washburn (2016: 92) on the importance of accounting for non-human actors:\n\n“Nonhuman actants” are not only present as nodes in the actor network in this approach, but also have agency. In science and technology studies, such conceptions exploded dualistic notions of a technical core and social superstructure—the separability of humans and machines. Instead, the social and technical together become a “seamless web,” co-constructed and mutually embedded (Bijker, Pinch, & Hughes, 1987; Latour, 1987). Woolgar (1991) captured this vividly in research on “how computers configure their users,” featuring the agency of the nonhuman in making us do things differ ently. With laptops or cell phones in place, we become “cyborgs” —– cybernetic organisms (Haraway, [1985] 1991a). … “Seeing” the agency of the nonhuman elements present in the situation disrupts the taken-for-granted, creating Meadian (e.g., [1927] 1964) moments of conceptual rupture through which we can see the world afresh. For example, “Magazines exist to sell readers to advertisers” ruptures the taken for- granted and offers a different perspective. The agency of magazines per se in the distribution of advertising discourses, normally invisible or at least not the lead point, is here rendered explicit and primary.\n\n\n\nInversion\nFrom Adele E. Clarke (2005: xxxvi-xxxvii):\n\nBowker and Star (2000: 10) discuss “infrastructural inversion” wherein the infrastructure of something is (unusually) revealed and even featured. … Situational maps and analyses do a kind of “social inversion” in making the usually invisible and inchoate social fea tures of a situation more visible: all the key elements in the situation and their interrelations; the social worlds and arenas in which the phenomena of interest are embedded; the discursive positions taken and not taken by actors (human and nonhuman) on key issues; and the discourses themselves as constitutive of the situation. This is the postmodernization of a grounded theory grounded in symbolic interactionism and Foucaultian analytics.\n\n\n\nBoundary objects\nFrom Adele E. Clarke (2005: 50-51):\n\nStar and Griesemer (1989) developed the concept of boundary objects for things that exist at junctures where varied social worlds meet in an arena of mutual concern. Boundary objects can be treaties among countries, software programs for users in different settings, even concepts themselves. Here the basic social process is “translating the object” to address the multiple specific needs or demands placed upon it by the different worlds involved. Boundary objects are often very important to many/most of the worlds involved and hence can be sites of intense controversy and competition for the power to define them. For example, in Star and Griesemer’s (1989) study of a regional zoology museum founded at the turn of the 20th century, the museum’s spec imens were boundary objects. There were collections of multiple specimens of each species and subspecies, which, for the zoologists to find them useful, had to be very, very carefully tagged as to date and where collected, and very, verr carefully preserved and taxidermied. Aerial temperature, humidity, rainfall, and the precise habitat information on specimens were all important. The mammal and bird specimens were usually killed, gathered, and sent to the museum by amateur collectors and “mercenaries” (paid collectors) of varied backgrounds. Also involved were university administrators, a patron (herself an amateur collector), curators, research scientists, clerical staff, members of scientific clubs, and taxidermists. All had particular concerns about the spec imens that needed to be addressed and mutually articulated for the museum’s collections to “work” well for all those involved. Thus the study of boundary objects can be an important pathway into often complicated situations, allowing the analyst to study the different participants through their distinc tiye relations with and discourses about the specific boundary object in question. This can help frame the broader situation of inquiry as well. Boundary objects can be human or nonhuman.\n\n\n\nReform movements\nFrom Adele E. Clarke (2005: 51) on movements and struggles within social worlds:\n\nBased on Bucher’s (1962, 1988; Bucher & Stelling 1977; Bucher & ,Strauss 1961) insights, interactionists have examined fluidity and change within social worlds and arenas by extending social movements analysis to include studies of reform movements of various kinds undertaken by segments or subworlds within professions, disciplines, and other work organi zations (Strauss et al. 1964, 1985/1997). Such reform movements can cut across whole arenas, such as rationalizing and standardizing hospital quality assurance in the late 20th century United States (Wiener 2000a, 2000b). Fujimura (1988, 1996), who studied the molecularization of biology, called such larger-scale processes “bandwagons.” In many arenas, reform move ments have centered around processes of homogenization, standardization, formal classifications-things that would organize and articulate the work of the social worlds in that arena in parallel ways (e.g., Bowker & Star 1999; Clarke & Casper 1996; Timmermans & Berg 2003)."
  },
  {
    "objectID": "notes/situational-analysis.html#data-sources",
    "href": "notes/situational-analysis.html#data-sources",
    "title": "Situational analysis",
    "section": "Data Sources",
    "text": "Data Sources\nFrom Adele E. Clarke (2021: 225):\n\nThe data for a situational analysis research project can be produced through in depth interviews and/or ethnographic observations, as is usual in grounded theory. However, situational analysis also strongly urges focusing on or also including extant discourse materials found in the situation under study as data—narrative, visual, and/or historical materials. These may include all kinds of documents, websites, imagery, material cultural objects, technological apparatuses, scientific or other specialized literatures, social media, and so forth. In SA, whatever discursive materials exist in the situation of inquiry are viewed as constitutive of that situation—integral parts of it—and therefore worthy of analysis.\n\nFrom Adele E. Clarke (2005: xxii-xxiii):\n\nSituational analysis supplements traditional or basic grounded theory with alternative approaches to both data gathering and analysis/interpretation. In addition to producing and analyzing interview and ethnographic data, situational analysis promotes the analysis of extant narrative, visual, and his torical discourse materials. It enhances our capacities to do incisive studies of differences of perspective, of highly complex situations of action and position ality, of the heterogeneous discourses in which we are all constantly awash, and of the situated knowledges of life itself thereby produced. What I am ulti mately grappling toward are approaches that can simultaneously address voice and discourse, texts and the consequential materialities and symbolisms of the nonhuman, the dynamics of historical change, and, last but far from least, power in both its more solid and fluid forms. The outcomes of situational mappings should be “thick analyses” (Fosket 2002:40), paralleling Geertz’s (1973) “thick descriptions.” Thick analyses take explicitly into account the full array of elements in the situation and explicate their interrelations.\n\n\nTheoretical Sampling\nFrom Adele E. Clarke (2005: xxxi-xxxii): &gt; Unique to this approach has been, first, its requiring that analysis begin as soon as there are data. &gt; Coding begins immediately, and theorizing based on that coding does as well, however provisionally (Glaser 1978). &gt; Second, “sampling” is driven not necessarily (or not only) by attempts to be “representative” of some social body or population or its heterogeneities but especially and explicitly by theoretical concerns that have emerged in the provisional analysis to date. &gt; Such “theoretical sampling” focuses on finding new data sources (persons or things-and not theories) that can best explic itly address specific theoretically interesting facets of the emergent analysis. &gt; Theoretical sampling has been integral to grounded theory from the outset, remains a fundamental strength of this analytic approach, and is crucial for situational analysis. &gt; In fact, “The true legacy of Glaser and Strauss is a collective awareness of the heuristic value of developmental research designs [through theoretical sampling] and exploratory data analytic strategies, not a ‘system’ for conducting and analyzing research” (Atkinson, Coffey, & Delamont 2003:162-163).\n\n\nSensitizing Concepts\nAccording to Adele E. Clarke (2005: 28) (emphasis original):\n\n… a key aspect of the alternative approach developed here is focused on grounded theor_izing_ through the development of sensitizing concepts and integrated analytics.\n\nShe goes on to quote Blumer (1969, :147–148):\n\n[T]he concepts of our discipline are fundamentally sensitizing instruments. Hence, I call them “sensitizing concepts” and put them in contrast with definitive concepts … A definitive concept refers precisely to what is common to a class of objects, and by the aid of a clear definition in terms of attributes or fixed bench marks … A sensitizing concept lacks such specification … Instead, it gives the user a general sense of reference and guidance in approaching empirical instances. Whereas definitive concepts provide prescriptions of what to see, sensitizing concepts merely suggest directions along which to look.\n\nThe bold part of this passage, which Clarke also emphasized, denotes recognition of the researcher’s intervention."
  },
  {
    "objectID": "notes/situational-analysis.html#situations",
    "href": "notes/situational-analysis.html#situations",
    "title": "Situational analysis",
    "section": "Situations",
    "text": "Situations\nFrom Fosket (2021): [: 269]:\n\nWith her conceptualization of situational analyses, Clarke moved these theories further (2003, 2005). Here social worlds/arenas theory expands to include as consequential elements everything within a given situation. That is, it is not just the social worlds and their human and nonhuman elements that situate and shape knowledge and practices, but histories, discourses, symbols, institutions, material things, and anything else conceived of as present in the situation.\n\nFrom Fosket (2021): [: 270]: &gt; Within this framework, an understanding of the work of scientific knowl edge production requires an understanding of everything in the situation: the workplaces and their organizations, scientists and other workers, theories, mod els, research materials, instruments, technologies, skills and techniques, sponsor ship and its organization, regulatory groups, audiences, consumers, and so on. Each of the relevant elements is not merely contextual (i.e., background) but conditional. Each element is an integral aspect of the situation itself, constitutive of the practices and contingencies of the research work that constitutes the very construction of knowledge. Even those elements that are not physically present in the situation are part of the situation in a very real sense.\nAdele E. Clarke (2005: 21-23) takes to heart four notions of “situations” while devising her methodology:\n\nFrom Thomas and Thomas (1928), who argued that situations defined as real are real in their consequences. Perspective dominates the interpretatiobs upon which actions are based. This is a very relational and ecological perspective.\nFrom Mills (1940), whose perspective is deeply pragmatist, in that he emphasized circumstances as important factors that influence action.\nFrom Haraway (1991), whose situated knowledges empasizes the embodied nature of all sensory experiences, and therefore of all knowledge.\nFrom Massumi (2002), who emphasized the situation of inquiry itself, and who draws a direct relationship with the researcher who enters a situation through its examination."
  },
  {
    "objectID": "notes/situational-analysis.html#footnotes",
    "href": "notes/situational-analysis.html#footnotes",
    "title": "Situational analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI could also present this during my “turn” at the MCHI meeting in April, and solicit the other members’ feedback.↩︎\nAs well as William James, Charles Sanders Pierce and John Dewey, but I think in highlighting Mead, Clarke intentionally situates symbolic interactism as a primarily sociological framework — which evolved as such at the University of Chicago in particular — rather than as a more general philosophical orientation that exhibits a more rationalist form of logic in its east coast counterparts (at Harvard and Johns Hopkins, specifically).↩︎\nAdele E. Clarke, Friese, and Washburn (2016) does not mention them specifically, but Giddens (1984) and Bourdieu (1983) come to mind as relevant to the practice turn, too.↩︎"
  },
  {
    "objectID": "notes/theory-building.html",
    "href": "notes/theory-building.html",
    "title": "Theory-building",
    "section": "",
    "text": "Urquhart (2019) provides some great ways to think about theory-building, especially in the latter half of the chapter. Specifically, they compare two axes: theory scope, and conceptual level.\nSee also: Kelle (2019) and Flick (2019).\nMruck and Mey (2019: 480):\n\nThe challenging task during analysis is to be as aware as possible of the co-constructiveness of data and methods on the one hand, i.e., focusing on the researcher’s continuous impact on every single decision during the data analysis process. On the other hand, in traditional research, this stage is most important for preserving the voices of research participants in theory construction in any possible way. But each encounter with the Other, each single word heard during the interview or written in a text, has to pass the bodily, cognitive, and emotional filters of the analyst, and leads to specific embodied resonances – there is no way to not (re)act personally. Researchers should not ignore this in the hope that, if they pretend ‘long enough that it does not exist, it should just quietly go away’ (Devereux, 1967: xviii). Instead, the researcher’s anxieties and ‘warding-off manoeuvres’, quite as much as his/her ‘research strategy, perception of data, and decision making … can shed light’ (ibid.) on the topic under interest, and enrich data analysis and theory development in two different ways:\n\n\nAs researchers are not neutral observers but part of the field, their responses are responses to the performance and narrations of the Other, the interviewee, be it as insiders sharing parts of his/her lifeworld, or as outsiders reacting to him/her, for example, in a friendly or aversive manner. This may help to get in touch with issues not explicitly mentioned during the interview but which are important for analysis. For example, while we worked on a study on theatre performance in a research group we supervised, different group members continuously reported feelings of severe time pressure while reading interview texts and preparing the sessions. Even though until that point ‘time’ had not been mentioned explicitly by the interviewees, this observation led to taking into account the role of ‘limited time’ slipping in as a new actor alongside the existing ones, and as an important aspect of further theory development (Mruck & Mey, 1998).\nTrying to see the world as strictly as possible through the interviewee’s eyes, researchers need to ‘bracket’ one’s own experiences, involvement, etc. Without reflexive strategies, ‘perspective slurring’ is unavoidable. This means that different perceptions and interpretations (of interviewees, field members, the researcher, members of research groups) need to be taken seriously and subjected to constant comparison (Glaser, 1965). Trying to understand interviewees’ perspectives is especially difficult in cases, far away from interviewers’ everyday life. In another research group we supervised, group members were asked to work individually on an interview with a sexual offender to prepare a group session. In the beginning of the session, a group member started by describing in detail the slaughtering of an animal he had witnessed some time ago. Other group members contributed their own ‘bloody’ everyday experiences, partly giggling and expressing feelings of disgust in a rather pleasurable way. In this way, the group in a way re-enacted bodily (and in this way got a first glimpse of) the offender’s trivialization of his actions during the interview, and at the same time tried to get closer to what the members (not the interviewee!) felt to be the monstrosity of murders (Mey & Mruck, 1998: 298f).\n\n\n\n\n\n\n\nReferences\n\nFlick, Uwe. 2019. “From Intuition to Reflexive Construction: Research Design and Triangulation in Grounded Theory Research.” In The SAGE Handbook of Current Developments in Grounded Theory, edited by Antony Bryant and Kathy Charmaz, 125–44. SAGE Publications Ltd. https://doi.org/10.4135/9781526485656.\n\n\nKelle, Udo. 2019. “The Status of Theories and Models in Grounded Theory.” In The SAGE Handbook of Current Developments in Grounded Theory, edited by Antony Bryant and Kathy Charmaz, 68–88. SAGE Publications Ltd. https://doi.org/10.4135/9781526485656.\n\n\nMruck, Katja, and Günter Mey. 2019. “Grounded Theory Methodology and Self-Reflexivity in the Qualitative Research Process.” In The SAGE Handbook of Current Developments in Grounded Theory, edited by Antony Bryant and Kathy Charmaz, 470–96. SAGE Publications Ltd. https://doi.org/10.4135/9781526485656.\n\n\nUrquhart, Cathy. 2019. “Grounded Theory’s Best Kept Secret: The Ability to Build Theory.” In The SAGE Handbook of Current Developments in Grounded Theory, edited by Antony Bryant and Kathy Charmaz, 89–106. SAGE Publications Ltd. https://doi.org/10.4135/9781526485656."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJun 20, 2025\n\n\nWeek notes (2025-W25)\n\n\nweek notes\n\n\n\n\nJun 13, 2025\n\n\nWeek notes (2025-W24)\n\n\nweek notes\n\n\n\n\nJun 6, 2025\n\n\nWeek notes (2025-W23)\n\n\nweek notes\n\n\n\n\nMay 30, 2025\n\n\nWeek notes (2025-W22)\n\n\nweek notes\n\n\n\n\nMay 23, 2025\n\n\nWeek notes (2025-W21)\n\n\nweek notes\n\n\n\n\nMay 16, 2025\n\n\nWeek notes (2025-W20)\n\n\nweek notes\n\n\n\n\nMay 9, 2025\n\n\nWeek notes (2025-W19)\n\n\nweek notes\n\n\n\n\nMay 2, 2025\n\n\nWeek notes (2025-W18)\n\n\nweek notes\n\n\n\n\nApr 25, 2025\n\n\nWeek notes (2025-W17)\n\n\nweek notes\n\n\n\n\nApr 18, 2025\n\n\nWeek notes (2025-W16)\n\n\nweek notes\n\n\n\n\nApr 11, 2025\n\n\nWeek notes (2025-W15)\n\n\nweek notes\n\n\n\n\nApr 4, 2025\n\n\nTechnical specs for this website\n\n\nwebsite\n\n\n\n\nApr 4, 2025\n\n\nWeek notes (2025-W14)\n\n\nweek notes\n\n\n\n\nMar 28, 2025\n\n\nWeek notes (2025-W13)\n\n\nweek notes\n\n\n\n\nMar 21, 2025\n\n\nWeek notes (2025-W12)\n\n\nweek notes\n\n\n\n\nMar 14, 2025\n\n\nWeek notes (2025-W11)\n\n\nweek notes\n\n\n\n\nMar 7, 2025\n\n\nWeek notes (2025-W10)\n\n\nweek notes\n\n\n\n\nFeb 28, 2025\n\n\nWeek notes (2025-W09)\n\n\nweek notes\n\n\n\n\nFeb 21, 2025\n\n\nWeek notes (2025-W08)\n\n\nweek notes\n\n\n\n\nFeb 14, 2025\n\n\nWeek notes (2025-W07)\n\n\nweek notes\n\n\n\n\nFeb 7, 2025\n\n\nWeek notes (2025-W06)\n\n\nweek notes\n\n\n\n\nJan 31, 2025\n\n\nWeek notes (2025-W05)\n\n\nweek notes\n\n\n\n\nJan 25, 2025\n\n\nWeek notes (2025-W04)\n\n\nweek notes\n\n\n\n\nJan 24, 2025\n\n\nOn the role of AI in my research\n\n\nAI / LLM, Methods, QDA\n\n\n\n\nJan 18, 2025\n\n\nWeek notes (2025-W03)\n\n\nweek notes\n\n\n\n\nDec 9, 2024\n\n\nReflection on first team meeting\n\n\nmeeting notes, general thoughts\n\n\n\n\nDec 9, 2024\n\n\nHello World!\n\n\nintroduction, website\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "posts/2024-12-09-hello-world.html",
    "href": "posts/2024-12-09-hello-world.html",
    "title": "Hello World!",
    "section": "",
    "text": "Welcome to the website for my CITF Postdoc! This will serve as a hub for documenting and sharing my work. I decided to do this as a way of managing and sharing always-updated drafts of research protocols with my supervisor and team members, but it is also generally useful for keeping my thoughts organized. I will also use this blog section to write my thoughts as the project progresses."
  },
  {
    "objectID": "posts/tech-specs.html",
    "href": "posts/tech-specs.html",
    "title": "Technical specs for this website",
    "section": "",
    "text": "I’m using this website as a way to help organize and share key documents and resources. The research protocols are in flux at this stage in the project’s development, and this will make it easier to distribute up-to-date drafts with partners, while simultaneously enhancing transparency.\nThis post outlines the technical specifications for this website and outlines a roadmap for its further development. It will therefore be continually updated as the site evolves."
  },
  {
    "objectID": "posts/tech-specs.html#fundamentals",
    "href": "posts/tech-specs.html#fundamentals",
    "title": "Technical specs for this website",
    "section": "Fundamentals",
    "text": "Fundamentals\nThis website is based on Quarto, a platform for writing and publishing scientific and technical writing. I had used quarto before but without fully understanding it, and now I am starting to see its elegance.\nI had started off using Hugo, but there were too many limitations that Quarto was able to accomodate. You can find an older version of this post reflecting that setup here: #2346852.\nThe site is hosted on GitHub Pages. The repo is located at https://github.com/zackbatist/CITF-Postdoc."
  },
  {
    "objectID": "posts/tech-specs.html#generating-pdfs",
    "href": "posts/tech-specs.html#generating-pdfs",
    "title": "Technical specs for this website",
    "section": "Generating PDFs",
    "text": "Generating PDFs\nAs an avid user, one thing I really like about Quarto is the ability to generate PDFs alongside html versions served over the web. I started tinkering with includes but I need to review how Quarto passes info from YAML frontmatter. It is not at all straightforward and I will need to experiment a bit more with this to get the hang of it."
  },
  {
    "objectID": "posts/tech-specs.html#archiving-and-version-control",
    "href": "posts/tech-specs.html#archiving-and-version-control",
    "title": "Technical specs for this website",
    "section": "Archiving and Version Control",
    "text": "Archiving and Version Control\nEvery change is tracked using git. I would also like to archive each research protocol in Zenodo once they reach a point of stability. This would ensure that they ca be aassigned DOIs and detailed metadata, which will make them easier to reference.\nHowever, I do not want to rely on Zenodo’s GitHub integration for two reasons: (1) I want this to be as platform-agnostic as possible, and (2) that system relies on GitHub’s release system which operates on the level of the whole repository rather than specific files.\nI might be able to write a custom CI workflow to archive specific files to Zenodo using their API. But, I want to be able to toggle this option, rather than have it occur for every single detected change. Maybe I can accomplish this by pushing the changes that I want to archive to a dedicated branch that the CI workflow is configured to operate on. Or it might be easier to simply do this manually, since I’m not sure I will be using it that often anyway."
  },
  {
    "objectID": "posts/tech-specs.html#git-submodules",
    "href": "posts/tech-specs.html#git-submodules",
    "title": "Technical specs for this website",
    "section": "Git Submodules",
    "text": "Git Submodules\nSince I’m dealing with potentially sensitive information, including recordings, transcripts and notes deriving from interviews that participants may opt to keep confidential, I need to isolate those files so they remain private and secure. To accomplish this, I will set up private git repos on the MCHI-administered GitLab instance and integrate them into my main project repository (hosted on GitHub, and hence less secure) as submodules. I will then configure my quarto rendering options to ignore any subdirectories deemed sensitive enough to not share publicly (which may not include everything in the submodule). I intend to follow this guide written by Tania Rascia to pull this off.\nJust as a quick reminder of the procedure for committing and syncing changes, based on my testing:\n\nMake changes to files in the submodule.\nCommit and sync those changes to the remote submodule.\nObtain a record of those changes from the remote submodule:\n\ngit submodule update --remote\nThis can be verified using git status; changes should be indicated as (new commits).\n\nCommit the record of commits to the main repo’s remote."
  },
  {
    "objectID": "posts/tech-specs.html#qc",
    "href": "posts/tech-specs.html#qc",
    "title": "Technical specs for this website",
    "section": "qc",
    "text": "qc\nI’m using qc, a novel qualitative data analysis tool designed for computational thinking. It’s a command line tool that makes QDA compatible with textfile based social science workflows, which more-or-less aligns with my overall approach. qc also plays nice with my plan to isolate potentially sensitive information on a private git submodule, and is loosely compatible with my strategy for posting memos and research outputs publicly via this website.\nI’m also communicating with its maintainer, Chris Proctor, who is an education researcher and computer scientist based at SUNY Buffalo. This is the perfect combination for this kind of work since he specializes in instilling computational literacies, he has the technical chops to actually develop this system, and most of all: qualitative data analysis is the bread and butter of education research, so he is developing from an informed perspective. I really like what I see so far, and I’m eager to collect data of my own so that I can maximize the full potential that this system affords. I will write another post about my experiences once I have more to report.\n\nInstalling qc\nI had a bit of a hard time installing qc using the recommended method (using pipx install qualitative-coding) due to NumPy and spaCy dependency issues, so I had to install from source. First I created a virtual environment:\npython3.12 -m venv qc\nsource qc/bin/activate\nI then downloaded the source code from the GitHub repo and installed it within the virtual environment:\npip install qualitative-coding-main.zip\nThen cd into the directory and initialize a qc project:\ncd qc\nqc init\nI applied some custom settings because I’m using Codium instead of VSCode. These basically replace the editor that’s called up when I use the code command.\ncodebook: codebook.yaml\ncorpus_dir: corpus\ndatabase: qualitative_coding.sqlite3\nlog_file: qualitative_coding.log\nmemos_dir: memos\nqc_version: 1.7.3\nverbose: true\neditor: codium\neditors:\n  codium:\n    name: Codium\n    code_command: 'codium \"{corpus_file_path}\" \"{codes_file_path}\" --wait'\n    memo_command: 'codium \"{memo_file_path}\" --wait'\nI also set verbose: true to help troubleshoot any issues that might come up and reduce my dependence on Chris.\nRegarding the submodule, there is a conflict with how the qc directory had to be generated. I needed to create the qc directory using venv, but I also had to create it by pulling it down from the GitLab server. So I had to create the virtual environment, exit the environment, rename the directory to a placeholder name, pull down the fresh submodule, commit and push the new submodule to the parent git repository, and then copy over all the contents from the placeholder directory into the submodule (and then commit changes and push). Not really a major issue, but a minor incompatibility in the workflow that’s worth noting."
  },
  {
    "objectID": "posts/weeknotes-2025-W04.html",
    "href": "posts/weeknotes-2025-W04.html",
    "title": "Week notes (2025-W04)",
    "section": "",
    "text": "This week was a bit slower than last. I spent much of it finalizing the content for my IRB application, and the rest preparing for a meeting with a key stakeholder relating to my research.\nThe IRB application is more or less done, just waiting on David and the department head to sign off. It was a major opportunity to re-organize my research protocol and related documents. I shuffled some things over into various placeholder sections in my methodology notes, and pushed a revised research protocol to the website.\nYesterday I posted about on the role of AI in my research. It’s mainly meant to lay out my current state of thinking on AI. I’m not fixed to those ideas, and I think there is much more nuance than I do justice to in that post, but putting it on the page helped me consolidate and put aside some scrambled opinions.\nAfter playing around with qc on Sunday, I started to assemble some feedback. I may post a github issue later this week, once I’ve had a chance to consolidate and edit my thoughts.\nI participated in the weekly CITF logistics update, after which I met with Aklil to discuss the overall strategy for her project and strategize on how we might form the focus groups for that work. We’re gonna meet more regularly, just to share some updates on our respective projects which have a lot in common.\nOn Thursday I met with Isabel Fortier, with the intention of discussing data harmonozation initiatives that might serve as potential cases. It was a bit of a tough meeting. During the first 45 minutes I struggled to communicate the purpose of my work, but I think by the end we reached a greater understanding of what this work will entail and the unique perspective it will bring. One surprising outcome that I still need to think through is Isabel’s suggestion that I slow down a bit, immerse myself more in the world of data harmonization. While she is absolutely right that I’ve been rushing through this first month, I do feel pressure to get the project going and to start accumulating data — I felt a similar pressure when starting my PhD, too. So I need to put my eagerness aside so that the data are relevant and of good enough quality. Isabel offered to schedule regular meetings with me, and even to have me work in the Maelstrom office once a week, and I’m extremely grateful for her support! Plus, I’ll get to have lunch with my mom who works in the same hospital complex, one building over :)"
  },
  {
    "objectID": "posts/weeknotes-2025-W06.html",
    "href": "posts/weeknotes-2025-W06.html",
    "title": "Week notes (2025-W06)",
    "section": "",
    "text": "This week was a bit slower than usual. The highlight was my meeting with Isabel to go over details about harmonization procedures and to discuss projects that may serve as potential cases. I got a better sense of the community composition and the kinds of challenges that are commonly experienced, and Isabel was able to provide me with contacts to get in touch with once I’m ready to begin collecting data.\nI was less active with regards to my methodology notes this week, having spent much time re-organizing and consolidating them. I’m also reading about coding techniques and the foundational principles and debates surrounding grounded theory, but there is a lot of ground to cover and I’m taking it all in before I begin recording my thoughts in a systematic way.\nI did some non-postdoc service this week too. I started a peer-review that I had committed to, participated in the monthly SSLA meeting, and completed the first-pass screening of archaeology-related journals in the Directory of Open Access Journals to verify their inclusion in diamond.open-archaeo."
  },
  {
    "objectID": "posts/weeknotes-2025-W08.html",
    "href": "posts/weeknotes-2025-W08.html",
    "title": "Week notes (2025-W08)",
    "section": "",
    "text": "Some highlights from this week:\n\nI met with Isabel for our biweekly update. We talked about challenges involved in introducing my work to epidemiologists, and practiced some techniques for communicating what my project is about.\nI made some revisions to my ethics protocol in response to the feedback provided by the IRB. Currently trying to resolve a technical problem with the online portal that processes these applications.\nI submitted a peer-review.\nI followed up on some translation work for SNAP.\nI continued with my methodology notes."
  },
  {
    "objectID": "posts/weeknotes-2025-W10.html",
    "href": "posts/weeknotes-2025-W10.html",
    "title": "Week notes (2025-W10)",
    "section": "",
    "text": "Ethics protocol was approved!\nMet with Isabel, who provided me with some more background info to help plan my first interviews\nMet with collaborators for a paper on data governance / informed consent materials, and made significant progress outlining and re-framing the paper\nDeveloped case overview files and interview guides\nStarted reading more on situational analysis methods\nSettled on a directory structure and workflow for dealing with sensitive data\nReceived reviews from another paper deriving from my dissertation, which call for some revisions (deadline April 1)\nStarted checking out my options for re-submitting my other twice-rejected paper deriving from my dissertation to another journal\nMet with the SSLA-SIG, where I accidentally signed on to lead an initiative to document AI in archaeological practice\nWorked on side-gig translation work in my spare time"
  },
  {
    "objectID": "posts/weeknotes-2025-W12.html",
    "href": "posts/weeknotes-2025-W12.html",
    "title": "Week notes (2025-W12)",
    "section": "",
    "text": "Met with Isabel, discussed the practical implementation of harmonization procedures.\nWorked on interview guides\nWrote more notes on situational analysis\nMet with Alex and made progress regarding the ICF / data governance paper\nWorked on revisions for the papers from my dissertation during my spare time\n\nSee my preprint submitted to PCI-archaeo: https://zackbatist.info/locating-creative-agency/"
  },
  {
    "objectID": "posts/weeknotes-2025-W14.html",
    "href": "posts/weeknotes-2025-W14.html",
    "title": "Week notes (2025-W14)",
    "section": "",
    "text": "This was a really tough week. I did not sleep well at all. Got a lot done, though.\n\nContinued refining the guide for my first interview.\nWriting and editing new memos.\nDid an overview of graph databases with other members of the CITF team, to explore their potential utility for us.\nDid my presentation for the MCHI research group, where I basically talked about my research goals and methods.\nI started drafting a blog post about tech fetishism among academics.\nWorked a bit more on my revisions for the AAP paper (got a 2-week extension).\nConnected with other coool postdocs at the monthly casual meetup.\nParticipated in the monthly SSLA meeting.\nAttended a talk on pragmatic clinical trials, which I thought was really interesting.\nSet up the qualitative coding system."
  },
  {
    "objectID": "posts/weeknotes-2025-W16.html",
    "href": "posts/weeknotes-2025-W16.html",
    "title": "Week notes (2025-W16)",
    "section": "",
    "text": "Finished open coding the first interview\nUpdated coding protocol and case selection protocol\nDid some more reading notes\nSubmitted revisions for a paper deriving from my dissertation"
  },
  {
    "objectID": "posts/weeknotes-2025-W18.html",
    "href": "posts/weeknotes-2025-W18.html",
    "title": "Week notes (2025-W18)",
    "section": "",
    "text": "This week I was largely focused on revising the research protcol and submitting an ethics amendment for a slight pivot to the overall project. I also co-hosted a focus group among practicum and thesis students regarding accessing data from the CITF databank. Also made some significant progress on this ICF/data governance paper.\nIt’s been more relaxed now that the term is over, and some of the more lively personalities in the office are away for various reasons. So it’s been really quiet and calm in general.\nSome additional developments in the archaeology realm of my life too, but I do not want to conflate those with this work."
  },
  {
    "objectID": "posts/weeknotes-2025-W20.html",
    "href": "posts/weeknotes-2025-W20.html",
    "title": "Week notes (2025-W20)",
    "section": "",
    "text": "Ethics amendment under review\nDeveloped a couple situational maps\nDeveloped some interview guides\nMade significant progress on the data governance paper"
  },
  {
    "objectID": "posts/weeknotes-2025-W22.html",
    "href": "posts/weeknotes-2025-W22.html",
    "title": "Week notes (2025-W22)",
    "section": "",
    "text": "Still waiting on ethics review.\nBackground reading.\nMemo-writing / reflection."
  },
  {
    "objectID": "posts/weeknotes-2025-W24.html",
    "href": "posts/weeknotes-2025-W24.html",
    "title": "Week notes (2025-W24)",
    "section": "",
    "text": "Revised ethics amendment.\nWorked on the data governance paper.\nWorked on grant apps."
  },
  {
    "objectID": "qda-protocol.html",
    "href": "qda-protocol.html",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "",
    "text": "Note\n\n\n\nThis document is still a work in progress.",
    "crumbs": [
      "QDA Protocol"
    ]
  },
  {
    "objectID": "qda-protocol.html#open-coding",
    "href": "qda-protocol.html#open-coding",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "Open coding",
    "text": "Open coding\nOpen coding (also sometimes referred to as initial coding) constitutes a first pass through data. I go through the transcript line by line, remaining open to all theoretical directions and interpretations that arise. It is akin to a brainstorming session.\nEssentially serves asn opportunity to reflect deeply on the contents and nuances of the data and to begin taking ownership over them.\nI also engage in open-ended memo-writing pertaining to specific passages within a transcipt, similar to writing in-document memos using MaxQDA. These are stored in a single file associated with the interview transcript, with reference to specific line numbers and labelled subheadings when appropriate.\nThis also relates to holistic coding, which is similar to open/initial coding, but operates at a coarser grain. Holistic coding is useful for “chunking” the data into broader topics as a preliminary step for more detailed analysis later on. However, for the sake of simplicity, I not really make this distinction between open and holisitc coding methods in my coding processes.\n\nPrefix: O:\nNotes on open coding",
    "crumbs": [
      "QDA Protocol"
    ]
  },
  {
    "objectID": "qda-protocol.html#in-vivo-coding",
    "href": "qda-protocol.html#in-vivo-coding",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "In vivo coding",
    "text": "In vivo coding\nIn vivo coding draws out specific words or phrases directly from the text. In vivo codes are identified opportunistically, and are typically created during an initial or open coding phase.\nIn vivo codes are especially useful for drawing attention to the improvised collective adoption of certain terms or imagery.\n\nPrefix: IV:\nNotes on in vivo coding",
    "crumbs": [
      "QDA Protocol"
    ]
  },
  {
    "objectID": "qda-protocol.html#process-coding",
    "href": "qda-protocol.html#process-coding",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "Process coding",
    "text": "Process coding\nProcess coding entails framing expressions of action in terms of generalizable processes. This may help reveal common objectives, circumstances and tooling that contextualize these action or that give these actions meaning. It may also help to elucidate commonalities and differences across similar processes, especially when coding the same passages with multiple processes.\n\nPrefix: PRO:\nNotes on process coding",
    "crumbs": [
      "QDA Protocol"
    ]
  },
  {
    "objectID": "qda-protocol.html#values-coding",
    "href": "qda-protocol.html#values-coding",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "Values coding",
    "text": "Values coding\nEntails three kinds of codes:\n\nValues\n\nThe importance that respondents attribute to themselves, to other people, things or ideas.\nThe principles, moral codes, and situational norms that they live by.\n\nAttitudes\n\nThe way respondents think and feel about themselves, other people, things or ideas.\nThey are part of a relatively enduring system of evaluative, affective reactions.\n\nBeliefs\n\nPart of a system that includes respondents’ values and attitudes, plus their personal knowledge, experiences, opinions, prejudices, morals, and other perceptions of the social world.\nRTey can be considered as “rules for action”.\n\n\nSome key phrases that signpost values, attitudes and beliefs include:\n\nIt’s important that…\nI like…\nI love…\nI need…\nI think…\nI feel…\nI want…\n\n\n\nPrefix: VAL: for preliminary or general codings; VAL-V:, VAL-A: or VAL-B: when specifying a specific kind of value code.\nNotes on values coding",
    "crumbs": [
      "QDA Protocol"
    ]
  },
  {
    "objectID": "qda-protocol.html#versus-coding",
    "href": "qda-protocol.html#versus-coding",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "Versus coding",
    "text": "Versus coding\n\nPrefix: VER:\nNotes on versus coding",
    "crumbs": [
      "QDA Protocol"
    ]
  },
  {
    "objectID": "qda-protocol.html#domain-taxonomic-coding",
    "href": "qda-protocol.html#domain-taxonomic-coding",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "Domain / taxonomic coding",
    "text": "Domain / taxonomic coding\n\nPrefix: X:\nNotes on domain and taxonomic coding",
    "crumbs": [
      "QDA Protocol"
    ]
  },
  {
    "objectID": "qda-protocol.html#attribute-coding",
    "href": "qda-protocol.html#attribute-coding",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "Attribute coding",
    "text": "Attribute coding\nMaintain a separate document with poeple’s and projects’ names, affiliations, objectives, etc, as a way to keep track of them all. This may turn into a memo document in its own right.\n\nNotes on attribute coding",
    "crumbs": [
      "QDA Protocol"
    ]
  },
  {
    "objectID": "qda-protocol.html#kinds-of-memos",
    "href": "qda-protocol.html#kinds-of-memos",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "Kinds of memos",
    "text": "Kinds of memos\nI find memos hard to classify and distinguish. For instance, when I attempt to simply describe an interaction that I observed or participated in, which would typically be classified as descriptive field notes, I often find myself offering interpreations alongside my accounts of what happened. At the same time, there are some clear distinctions between these accounts of events, memos about situational maps, or general reflections about the strange epidemiological world I’m encountering. I therefore lump most of these into the category of “memo” (and stored under the data/memos/ directory), and append tags to the metadata that indicate the context in which the memo was created. Some common tags include:\n\nsituational map\nin-document memo\ndescriptive field notes\npost-interview reflections\nreflection on an encounter\n\nSee Mruck and Mey (2019: 483-485) for further details on the distinctions and crossovers between journalling and memo-writing.",
    "crumbs": [
      "QDA Protocol"
    ]
  },
  {
    "objectID": "qda-protocol.html#titles-and-descriptions",
    "href": "qda-protocol.html#titles-and-descriptions",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "Titles and descriptions",
    "text": "Titles and descriptions\nI find it kind of difficult to provide very brief titles for memos, but there is a lot of value in writing brief descriptive summaries. Those are probably better quality and more helpful than the titles themselves.\nI also struggle with the file names. Sometimes it is appropriate to use a very simple series of hyphenated words, especially if I plan on adding content or editing the document in the future. But in situations where I am reflecting on a speciifc encounter, I may prefer to use a datestamp, with or without a series of descriptive terms (usually borrowed from the title).\nFor memos that are linked to other documents, such as in-document memos associated with a collection of files collected during an interview (transcript, recordings, etc), then I put the memo in the folder where those materials are kept and five the memos corresponding names.",
    "crumbs": [
      "QDA Protocol"
    ]
  },
  {
    "objectID": "qda-protocol.html#analyzing-memos",
    "href": "qda-protocol.html#analyzing-memos",
    "title": "Qualitaive Data Analysis Protocol",
    "section": "Analyzing memos",
    "text": "Analyzing memos\nI may import memos into qc for their analysis as research materials in their own right. I prefer to analyze memos that are more descriptive and less interpretive, or that have a basis in what my informants say, rather than what I say.",
    "crumbs": [
      "QDA Protocol"
    ]
  }
]